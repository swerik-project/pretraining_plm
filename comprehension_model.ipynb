{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import comprehension_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"swerick_data_random_train.pkl\", \"test\": \"swerick_data_random_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kb = preprocessing.create_model_MLM(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exbert_tokenizer = AutoTokenizer.from_pretrained(\"exbert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "swerick_tokenizer= PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/home/laurinemeier/swerick/alvis_project/pretraining_from_scratch/tokenizer_swerick.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "small_valid_dataset = preprocessing.create_deterministic_eval_dataset(valid_dataset.select(range(10000)),data_collator)\n",
    "small_valid_dataloader=preprocessing.create_dataloader(small_valid_dataset,64,default_data_collator)\n",
    "                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Statsrådet\"\n",
    "token_id = tokenizer.convert_tokens_to_ids(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_token(token,example):\n",
    "    return token in example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filtered_dataset = valid_dataset.filter(lambda example : special_token(token_id,example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_filtered_dataloader=preprocessing.create_dataloader(valid_filtered_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exbert = AutoModelForMaskedLM.from_pretrained(\"exbert-finetuned-imdb/checkpoint-3995640\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "config = transformers.BertConfig.from_pretrained(\"alvis_project/pretraining_from_scratch/checkpoint-5258900\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"alvis_project/pretraining_from_scratch/checkpoint-5258900\",config=config,trust_remote_code=True)\n",
    "mosaicBert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "valid_sentence_filtered = valid_filtered_dataset.map(lambda example : preprocessing.get_context_with_mask(example,token_id,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bis(model,dataloader, tokenizer,token_id):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    layerwise_embeddings = [[] for _ in range(model.config.num_hidden_layers + 1)]\n",
    "    preds=[]\n",
    "    for batch in dataloader :\n",
    "        tokens={key : value.to(device) for key,value in batch.items()}\n",
    "        if token_id not in list(batch[\"labels\"][0]):\n",
    "            continue\n",
    "        index=list(batch[\"labels\"][0]).index(token_id)\n",
    "        outputs= model(input_ids=tokens[\"input_ids\"],attention_mask=tokens[\"attention_mask\"],labels=tokens[\"labels\"],output_hidden_states=True)\n",
    "        preds.append(torch.argmax(F.softmax(outputs.logits.squeeze(0)[index])))\n",
    "        hidden_states = outputs.hidden_states  # tuple of (layer+1) tensors, each of shape (batch_size, seq_len, hidden_size)\n",
    "        for i, hidden_state in enumerate(hidden_states):\n",
    "            masked_embeddings = hidden_state[:, index, :].detach().cpu().numpy()  # Extract [CLS] token\n",
    "            layerwise_embeddings[i].append(masked_embeddings)\n",
    "    return [np.vstack(layer) for layer in layerwise_embeddings],preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "inputs = valid_sentence_filtered[3]\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))\n",
    "token = {key: torch.tensor(value, dtype=torch.long).unsqueeze(0).to(device) for key,value in inputs.items()}\n",
    "outputs = model_hugging_face(input_ids=token[\"input_ids\"],attention_mask=token[\"attention_mask\"],labels=token[\"labels\"],output_hidden_states=True)\n",
    "outputs2=model_kb(input_ids=token[\"input_ids\"],attention_mask=token[\"attention_mask\"],labels=token[\"labels\"],output_hidden_states=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token[\"input_ids\"].squeeze())\n",
    "index=inputs[\"labels\"].index(token_id)\n",
    "hidden_states = outputs.hidden_states\n",
    "hidden_states_kb = outputs2.hidden_states\n",
    "last_hidden_state = hidden_states[-1].squeeze().detach().cpu().numpy()\n",
    "print(tokenizer.decode(torch.argmax(outputs.logits.squeeze()[index])))\n",
    "print(tokenizer.decode(torch.argmax(outputs2.logits.squeeze()[index])))\n",
    "def plot_pca_hidden_state(hidden_state,tokens,number):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_states = pca.fit_transform(hidden_state.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    # Préparer la figure pour la visualisation\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in [\"[CLS]\",\"[SEP]\"]:\n",
    "            color = 'green'  # Token spécial\n",
    "        elif token == '[MASK]':\n",
    "            color = 'purple'  # Token masqué\n",
    "        else:\n",
    "            color = 'cyan'  # Token de contexte\n",
    "        \n",
    "        plt.scatter(reduced_states[i, 0], reduced_states[i, 1], c=color, label=token)\n",
    "        plt.text(reduced_states[i, 0], reduced_states[i, 1], token, fontsize=9)\n",
    "\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.title(f\"PCA of hidden state for one sentence at layer {number} \")\n",
    "    plt.legend(handles=[\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='cyan', markersize=10, label='Token de Contexte'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Token Spécial'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label='Token Masqué')\n",
    "    ], loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for j in range(13):\n",
    "    print(\"hugging face\")\n",
    "    plot_pca_hidden_state(hidden_states[j],tokens,j)\n",
    "    print(\"kb\")\n",
    "    plot_pca_hidden_state(hidden_states_kb[j],tokens,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA hidden states\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "text = \"Herr [MASK] von Ehrenheim : Anledningen till den framställning \"\n",
    "inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "input_eb=exbert_tokenizer(text,return_tensors='pt').to(device)\n",
    "input_spa = swerick_tokenizer(text,return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model_kb(**inputs,output_hidden_states=True)\n",
    "outputs_hugging = model_hugging_face(**inputs,output_hidden_states=True)\n",
    "output_eb=model_exbert(**input_eb,output_hidden_states=True)\n",
    "output_spa=mosaicBert(**input_spa,output_hidden_states=True)\n",
    "print(output_spa)\n",
    "all_hidden_states = outputs.hidden_states\n",
    "all_hidden_states_cpt = outputs_hugging.hidden_states\n",
    "all_hidden_states_eb = output_eb.hidden_states\n",
    "all_hidden_states_spa = output_spa.hidden_states\n",
    "\n",
    "index=2\n",
    "token_hidden_states = [layer_hidden_states[0, index].detach().cpu().numpy() for layer_hidden_states in all_hidden_states]\n",
    "token_hidden_states_cpt = [layer_hidden_states[0, index].detach().cpu().numpy() for layer_hidden_states in all_hidden_states_cpt]\n",
    "token_hidden_states_eb = [layer_hidden_states[0, index].detach().cpu().numpy() for layer_hidden_states in all_hidden_states_eb]\n",
    "#token_hidden_states_spa = [layer_hidden_states[0, index].detach().cpu().numpy() for layer_hidden_states in all_hidden_states_spa]\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "token_hidden_states_2d = pca_2d.fit_transform(token_hidden_states)\n",
    "token_hidden_states_cpt_2d = pca_2d.fit_transform(token_hidden_states_cpt)\n",
    "token_hidden_states_eb_2d = pca_2d.fit_transform(token_hidden_states_eb)\n",
    "#token_hidden_states_spa_2d = pca_2d.fit_transform(token_hidden_states_spa)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "token_hidden_states_3d = pca_3d.fit_transform(token_hidden_states)\n",
    "token_hidden_states_cpt_3d = pca_3d.fit_transform(token_hidden_states_cpt)\n",
    "token_hidden_states_eb_3d = pca_3d.fit_transform(token_hidden_states_eb)\n",
    "#token_hidden_states_spa_3d = pca_3d.fit_transform(token_hidden_states_spa)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(len(token_hidden_states_2d)):\n",
    "    plt.scatter(token_hidden_states_2d[i, 0], token_hidden_states_2d[i, 1], label=f'Layer {i}')\n",
    "  \n",
    "plt.plot(token_hidden_states_2d[:, 0], token_hidden_states_2d[:, 1], linestyle='-', marker='o')\n",
    "plt.plot(token_hidden_states_cpt_2d[:, 0], token_hidden_states_cpt_2d[:, 1], linestyle='-', marker='o',color='red',label='cptBERT')\n",
    "plt.plot(token_hidden_states_eb_2d[:, 0], token_hidden_states_eb_2d[:, 1], linestyle='-', marker='o',color='green',label='sBERTex')\n",
    "#plt.plot(token_hidden_states_spa_2d[:, 0], token_hidden_states_spa_2d[:, 1], linestyle='-', marker='o',color='black',label='sparBERT')\n",
    "plt.title(f'2D Projection of Hidden States of Token {tokenizer.decode(inputs[\"input_ids\"][0][index])} through layers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation en 3D\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(len(token_hidden_states_3d)):\n",
    "    ax.scatter(token_hidden_states_3d[i, 0], token_hidden_states_3d[i, 1], token_hidden_states_3d[i, 2], label=f'Layer {i}')\n",
    "ax.plot(token_hidden_states_3d[:, 0], token_hidden_states_3d[:, 1], token_hidden_states_3d[:, 2], linestyle='-', marker='o')\n",
    "ax.plot(token_hidden_states_cpt_3d[:, 0], token_hidden_states_cpt_3d[:, 1], token_hidden_states_cpt_3d[:, 2], linestyle='-', marker='o',color='red',label='cptBERT')\n",
    "ax.plot(token_hidden_states_eb_3d[:, 0], token_hidden_states_eb_3d[:, 1], token_hidden_states_eb_3d[:, 2], linestyle='-', marker='o',color='green',label='sBERTex')\n",
    "#ax.plot(token_hidden_states_spa_3d[:, 0], token_hidden_states_spa_3d[:, 1], token_hidden_states_spa_3d[:, 2], linestyle='-', marker='o',color='black',label='sparBERT')\n",
    "ax.set_title(f'3D Projection of Hidden States of Token {tokenizer.decode(inputs[\"input_ids\"][0][index])} through layer')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(13):\n",
    "        combined_embeddings = np.concatenate([baseline_embeddings[layer], finetuned_embeddings[layer]])\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(combined_embeddings)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i, color in enumerate(['blue', 'red', 'orange', 'green']):\n",
    "            indices = [j for j, c in enumerate(colors) if c == color]\n",
    "            plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], c=color, label=color, alpha=0.5, edgecolors='w', s=50)\n",
    "\n",
    "        plt.title('t-SNE of Word Embeddings with Classification Results')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "num_words=500\n",
    "embedding_matrix_kb = model_kb.bert.embeddings.word_embeddings.weight.detach().cpu()\n",
    "embedding_matrix_cpt= model_hugging_face.bert.embeddings.word_embeddings.weight.detach().cpu()\n",
    "#words_of_interest = ['##nader', '##varo', '##sapparat', 'ned', 'begär', '##ier', '##vens', 'replik', '##skri', '##ads', 'regionerna', '##skar', '##ositionen', 'försäkra', '##vot', '##fri', '##näringen', '##oko', '##olin', 'fonden', '##äck', '##skill', '##kontakt', '##grip', '##raftt', 'Fru', '##mäl', '##vari', 'Statsrådet', 'Utskottet', '##£', '##ati', '##rift', 'Full', '##farlig', '##eminister', '##iter', 'Pro', '##ande', 'någonting', 'Hultsfred', '##ötet', '##igen', '##speriod', 'Kungl', '##pekt', 'Onsdagen', '##kning', '##£', 'Kungl', 'Flott', '##sättas', '##arbetar', '718', '##mäter', '##mt', 'fi', '##NE', 'godkännas', 'JOHANSSON', 'Onsdagen', '##taga', 'Måndagen', 'Nr', '##hai', 'talman', 'Riksdagen']\n",
    "#word_indices = [tokenizer.vocab[word] for word in words_of_interest if word in tokenizer.vocab]\n",
    "words_of_interest= list(tokenizer.vocab.keys())[:num_words]\n",
    "selected_embeddings_kb = embedding_matrix_kb[:num_words, :]\n",
    "selected_embeddings_cpt = embedding_matrix_cpt[:num_words, :]\n",
    "tsne = TSNE(n_components=2, random_state=42,perplexity=30)\n",
    "embedding_2d = tsne.fit_transform(selected_embeddings_kb)\n",
    "\n",
    "# Tracer les embeddings 2D\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, word in enumerate(words_of_interest):\n",
    "    x, y = embedding_2d[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, (x, y), fontsize=9)\n",
    "\n",
    "plt.title('2D t-SNE Representation of Word Embeddings')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "embedding_cpt = tsne.fit_transform(selected_embeddings_cpt)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, word in enumerate(words_of_interest):\n",
    "    x, y = embedding_cpt[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, (x, y), fontsize=9)\n",
    "\n",
    "plt.title('2D t-SNE Representation of Word Embeddings for cptBERT')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison of weights for different model\n",
    "\n",
    "\n",
    "weights_kb = comprehension_model.get_model_weights(model_kb)\n",
    "weights_finetuned = comprehension_model.get_model_weights(model_hugging_face)\n",
    "\n",
    "weight_diffs = {}\n",
    "for key in weights_kb.keys():\n",
    "    weight_diffs[key] = weights_finetuned[key] - weights_kb[key]\n",
    "    if (np.linalg.norm(weight_diffs[key])/weight_diffs[key].size) > 0.04 :\n",
    "        print(key)\n",
    "    #     print(np.linalg.norm(weight_diffs[key],2))\n",
    "    #     print(weight_diffs[key].size)\n",
    "    #     print( np.linalg.norm(weight_diffs[key])/weight_diffs[key].size)\n",
    "    \n",
    "\n",
    "weight_diffs[\"cls.predictions.decoder.bias\"] = model_hugging_face.cls.predictions.decoder.bias.detach().cpu().numpy() - model_kb.cls.predictions.decoder.bias.detach().cpu().numpy()\n",
    "weight_diffs[\"cls.predictions.decoder.weight\"] = model_hugging_face.cls.predictions.decoder.weight.detach().cpu().numpy() - model_kb.cls.predictions.decoder.weight.detach().cpu().numpy()\n",
    "norms = [(np.linalg.norm(weight_diffs[key])/weight_diffs[key].size) for key in weight_diffs.keys()]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(norms)), norms, tick_label=list(weight_diffs.keys()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frobenius Norm of Weight Differences')\n",
    "plt.title('Comparison of Weight Changes in BERT Layers')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face.state_dict()[\"bert.encoder.layer.0.attention.self.key.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages('weight_distribution_scratch.pdf') as pdf:\n",
    "    for name,param in mosaicBert.named_parameters():\n",
    "        layer_name = name\n",
    "        print(name)\n",
    "        split_name =name.split('.')\n",
    "        layer = split_name[3]\n",
    "        print(layer)\n",
    "        #comprehension_model.plot_weight_distributions(model_hugging_face, model_kb, layer_name)\n",
    "        if \"attention.self.Wqkv\"  in name and \"weight\" in name:\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.query.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][:768,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.key.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][768:1536,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.value.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][1536:,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        elif  \"attention.self.Wqkv\" in name and \"bias\" in name :\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.query.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][:768].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.key.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][768:1536].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.value.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][1536:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close() \n",
    "        elif \"mlp.gated_layers\"  in name :\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.intermediate.dense.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"mlp.wo\" in name :\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.output.dense.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"mlp.layernorm\"  in name :\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.output.LayerNorm.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"cls.predictions.decoder\" in name:\n",
    "            continue\n",
    "            \n",
    "        else :\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            weights2 = model_kb.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model_exbert.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_name = \"cls.predictions.transform.dense.weight\"\n",
    "\n",
    "weights2 = model_kb.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "plt.title(f\"Weight Distribution Comparison for {layer_name} for SparBERT\")\n",
    "plt.xlabel(\"Weight values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "print(hidden_states1[0].shape)\n",
    "i=8\n",
    "hidden_states1,logits1=get_embeddings_bis(model_kb,valid_sentence_filtered [i],tokenizer)\n",
    "hidden_states2,logits2=get_embeddings_bis(model_hugging_face,valid_sentence_filtered [i],tokenizer)\n",
    "#masked_positions =[idx for idx, token in enumerate(valid_filtered_dataset[i]['input_ids']) if token == tokenizer.mask_token_id]\n",
    "index = valid_sentence_filtered [i]['labels'].index(token_id)\n",
    "print(token_id)\n",
    "print(tokenizer.decode(valid_sentence_filtered[i]['input_ids']))\n",
    "print(tokenizer.decode(torch.argmax(F.softmax(logits1.squeeze()[index], dim=-1)).item()))\n",
    "print(tokenizer.decode(torch.argmax(F.softmax(logits2.squeeze()[index], dim=-1)).item()))\n",
    "for j in range(len(hidden_states1)) :\n",
    "    print(tokenizer.decode((valid_sentence_filtered[i]['labels'][index])))\n",
    "    print('hidden layer ',j)\n",
    "    plt.figure(figsize=(10,6))\n",
    "   #plt.hist(hidden_states1[j][0][index].detach().cpu().numpy(), bins=100, alpha=0.5, label='Baseline Model')\n",
    "    plt.hist(hidden_states1[j][0][0].detach().cpu().numpy(), bins=100, alpha=0.5, label='Baseline Model cls')\n",
    "    #plt.hist(hidden_states2[j][0][index].detach().cpu().numpy(), bins=100, alpha=0.5, label='Fine-tuned Model')\n",
    "    plt.hist(hidden_states2[j][0][0].detach().cpu().numpy(), bins=100, alpha=0.5, label='finetuned Model cls')\n",
    "    plt.xlabel('weight')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "checkpoint_directory = \"/home/laurinemeier/swerick/finetuning/finetuning_hugging_whitespace-finetuned-imdb\"\n",
    "checkpoint_files = os.listdir(checkpoint_directory)\n",
    "checkpoint_files.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)))\n",
    "selected_checkpoints = [checkpoint_files[i] for i in range(0, len(checkpoint_files), 10)]\n",
    "weight1 = model_kb.state_dict()[\"bert.embeddings.word_embeddings.weight\"].flatten().cpu().numpy()\n",
    "print(\"std kb\", np.std(weight1))\n",
    "for name in selected_checkpoints :\n",
    "    print(name)\n",
    "    model_hugging =AutoModelForMaskedLM.from_pretrained(checkpoint_directory + '/'+name)\n",
    "    weights2 = model_hugging.state_dict()[\"bert.embeddings.word_embeddings.weight\"].flatten().cpu().numpy()\n",
    "    print(np.std(weights2))\n",
    "    model_hugging.to(device)\n",
    "    comprehension_model.plot_weight_distributions(model_hugging, model_kb, \"bert.embeddings.word_embeddings.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states1 = comprehension_model.get_embeddings(model_kb, small_valid_dataloader, tokenizer)\n",
    "hidden_states2 = comprehension_model.get_embeddings(model_hugging_face, small_valid_dataloader, tokenizer)\n",
    "\n",
    "for i in range(len(hidden_states1)):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(hidden_states1[i].flatten(), bins=100, alpha=0.5, label='Baseline Model')\n",
    "        plt.hist(hidden_states2[i].flatten(), bins=100, alpha=0.5, label='Fine-tuned Model')\n",
    "        plt.title(f\"Hidden States Distribution Comparison for Layer {i}\")\n",
    "        plt.xlabel(\"Hidden States Values\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evolution of a specific layer through epochs\n",
    "checkpoint_directory = 'finetuning/finetuning_hugging_whitespace-finetuned-imdb'\n",
    "comprehension_model.evolution_specific_layer_weight(chekpoint_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study of Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(mean_similarities,x_label='Layer Number',y_label='Average Cosine Similarity',title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(mean_similarities)), mean_similarities, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "mean_similarities_hidden_states,mean_similarities_attention,diff_tot = comprehension_model.extract_and_compare_activations(model_kb, model_hugging_face, valid_filtered_dataloader,token_id)\n",
    "print(\"Layer-wise cosine similarities:\", mean_similarities_hidden_states)\n",
    "\n",
    "\n",
    "plot_results(mean_similarities_hidden_states)\n",
    "plot_results(mean_similarities_attention,'Attention Layer Number',title='Average Attention Layer-wise Cosine Similarity between Attention values Across Validation Dataset')\n",
    "plot_results(diff_tot,'Layer Number',title='Average Norm difference between hidden states  Across Validation Dataset for token {words}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def plot_results(similarities, x_label='Layer Number', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(similarities)), similarities, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def compare_ffn_contributions(model_pre, model_post, dataloader):\n",
    "    similarities_pre = []\n",
    "   #similarities_post = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = {k: batch[k].to(device) for k in batch.keys()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre_output = model_pre(**batch, output_hidden_states=True)\n",
    "            pre_activations = pre_output.hidden_states\n",
    "            post_output = model_post(**batch, output_hidden_states=True)\n",
    "            post_activations = post_output.hidden_states\n",
    "    \n",
    "        pre_contribution = [(pre_activations[layer+1] -pre_activations[layer]) for layer in range(len(pre_activations)-1)]\n",
    "        post_contribution = [(post_activations[layer+1] - post_activations[layer]) for layer in range(len(post_activations)-1)]\n",
    "\n",
    "        pre_activation = [cosine_similarity(pre_contribution[i],post_contribution[i]) for i in range (len(pre_contribution))]\n",
    "       # post_activation = [cosine_similairity(post_contribution[i] for i in range (len(post_contribution)))]\n",
    "        similarities_pre.append(pre_activation)\n",
    "       # similarities_post.append(post_contribution)\n",
    "        \n",
    "        del pre_activations\n",
    "        del post_activations\n",
    "        del pre_output\n",
    "        del post_output\n",
    "    \n",
    "    similarities_pre = np.mean(np.array(similarities_pre), axis=0)\n",
    "   # similarities_post = np.mean(np.array(similarities_post), axis=0)\n",
    "    \n",
    "    return similarities_pre\n",
    "# Example usage\n",
    "mean_similarity_pr = compare_ffn_contributions(model_kb, model_hugging_face, small_valid_dataloader)\n",
    "\n",
    "plot_results(mean_similarity_pr, x_label='Layer n+1 - Layer n', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity for differences consecutive layers of KB Bert Model and cptBERT')\n",
    "#plot_results(mean_similarity_post, label='Layer n+1 - Layer n', x_label='Layer n+1 - Layer n', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity for Finetuned Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# see if final layers is similar\n",
    "\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "\n",
    "def interpolate_to_length(tensor, target_length):\n",
    "    # Interpolation linéaire pour redimensionner le tenseur à la longueur cible\n",
    "    current_length = tensor.shape[1]\n",
    "    if current_length == target_length:\n",
    "        return tensor\n",
    "    # Créer un tenseur avec la longueur cible en utilisant l'interpolation linéaire\n",
    "    interpolated_tensor = F.interpolate(tensor.transpose(1, 2), size=target_length, mode='linear', align_corners=False).transpose(1, 2)\n",
    "    return interpolated_tensor\n",
    "\n",
    "def plot_results(similarities, x_label='Layer Number', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(similarities)), similarities, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "model_hugging_face.to(device)\n",
    "model_exbert.to(device)\n",
    "#mosaicBert.to(device)  \n",
    "model_kb.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cosine_cpt_final=[]\n",
    "cosine_eb_final=[]\n",
    "cosine_spa_final=[]\n",
    "hidden_states=[]\n",
    "cosine_embedding=[]\n",
    "diff_final_kb=[]\n",
    "diff_final_eb=[]\n",
    "embedding_cpt = model_hugging_face.bert.embeddings\n",
    "embedding_kb=model_kb.bert.embeddings\n",
    "\n",
    "# random_vector = torch.randn_like(hidden_states[0]).to(device)\n",
    "torch.manual_seed(33)\n",
    "for layer in range(12):\n",
    "\n",
    "    cosine_cpt=[]\n",
    "    cosine_eb=[]\n",
    "    cosine_spa=[]\n",
    "    diff_kb=[]\n",
    "    diff_eb=[]\n",
    "    last_layer_kb = model_kb.bert.encoder.layer[layer]\n",
    "    last_layer_cpt = model_hugging_face.bert.encoder.layer[layer]\n",
    "    last_layer_eb=model_exbert.bert.encoder.layer[layer]\n",
    "       \n",
    "    for batch in small_valid_dataloader :\n",
    "        batch={key:value.to(device) for key,value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output=model_kb(**batch,output_hidden_states=True)\n",
    "        if layer ==0:\n",
    "            embedding = embedding_cpt(batch[\"input_ids\"])\n",
    "            \n",
    "        hidden_states=output.hidden_states[layer].to(device)\n",
    "\n",
    "      \n",
    "        # if layer ==0:\n",
    "        #     embedding = embedding_cpt(random_embedding)  \n",
    "        #     embeddingkb= embedding_kb(random_embedding)\n",
    "        hs_kb=last_layer_kb(hidden_states)\n",
    "        hs_cpt=last_layer_cpt(hidden_states)\n",
    "        hs_eb=last_layer_eb(hidden_states)\n",
    "        \n",
    "        \n",
    "                \n",
    "        # if layer==0:\n",
    "        #     cosine_embedding.append(cosine_similarity(embeddingkb,embedding))\n",
    "            \n",
    "        cosine_cpt.append(cosine_similarity(hs_kb[0],hs_cpt[0]))\n",
    "        cosine_eb.append(cosine_similarity(hs_kb[0],hs_eb[0]))\n",
    "        cosine_spa.append(cosine_similarity(hs_eb[0],hs_cpt[0]))\n",
    "        diff_kb.append(cosine_similarity(hs_kb[0]-hidden_states,hs_cpt[0]-hidden_states))\n",
    "        diff_eb.append(cosine_similarity(hs_kb[0]-hidden_states,hs_eb[0]-hidden_states))\n",
    "        # if layer==0:\n",
    "        #     cosine_cpt_final.append(np.mean(cosine_embedding))\n",
    "    cosine_cpt_final.append(np.mean(cosine_cpt))\n",
    "    cosine_eb_final.append(np.mean(cosine_eb))\n",
    "    cosine_spa_final.append(np.mean(cosine_spa,axis=0))\n",
    "    diff_final_kb.append(np.mean(diff_kb))\n",
    "    diff_final_eb.append(np.mean(diff_eb))\n",
    "    \n",
    "    print(np.mean(cosine_cpt))\n",
    "    print(np.mean(cosine_eb))   \n",
    "            \n",
    "\n",
    "plot_results(cosine_cpt_final,x_label='Layer',title='Avg cosine similarity between KB bert and cptBERT without propagation')\n",
    "plot_results(cosine_eb_final,x_label='Layer',title='Avg cosine similarity between KB bert and sBERTex without propagation')\n",
    "plot_results(cosine_spa_final,x_label='Layer',title='Avg cosine similarity between cptBERT and sBERTex without propagation')\n",
    "plot_results(diff_final_kb,x_label='Layer',title='Avg cosine similarity between differences of consecutives layers cptBERT and KB bert without propagation')\n",
    "plot_results(diff_final_eb,x_label='Layer',title='Avg cosine similarity between differences of consecutives layers sBERTex and KB bert without propagation')\n",
    "# # print(\"cosine similarity spa\",np.mean(cosine_spa,axis=0))\n",
    "    \n",
    "# sentence = \"Herr [MASK] von Ehrenheim : Äfven\"\n",
    "# input_kb=tokenizer(sentence,return_tensors='pt').to(device)\n",
    "# input_eb=exbert_tokenizer(sentence,return_tensors='pt').to(device)\n",
    "# model_kb.to(device)\n",
    "# model_exbert.to(device)\n",
    "\n",
    "\n",
    "# output1=model_kb(**input_kb,output_hidden_states=True)\n",
    "# output2=model_exbert(**input_eb,output_hidden_states=True)\n",
    "# pre_activations = output1.hidden_states\n",
    "# post_activation=output2.hidden_states\n",
    "\n",
    "# target_length=len(pre_activations[0][0])\n",
    "# print(target_length)\n",
    "\n",
    "# pre_contribution = [interpolate_to_length((pre_activations[layer+1] -pre_activations[layer]),target_length) for layer in range(len(pre_activations)-1)]\n",
    "# post_contribution = [interpolate_to_length((post_activation[layer+1] - post_activation[layer]),target_length) for layer in range(len(post_activation)-1)]\n",
    "\n",
    "# cosine_activation = [cosine_similarity(pre_contribution[i],post_contribution[i]) for i in range (len(pre_contribution))]\n",
    "\n",
    "\n",
    "# plot_results(cosine_activation,x_label=\"Layer\",y_label=\"Average Cosine Similairyt difference\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "text=\"Herr [MASK] von Ehrenheim : Anledningen till den framställning\"\n",
    "input_kb=tokenizer(text,return_tensors='pt').to(device)\n",
    "output=model_kb(**input_kb,output_hidden_states=True)\n",
    "\n",
    "hidden_states=output.hidden_states[-1].to(device)\n",
    "head_layer_kb=model_kb.cls\n",
    "head_layer_cpt=model_hugging_face.cls\n",
    "head_layer_eb=model_exbert.cls\n",
    "head_layer_spa=mosaicBert.cls\n",
    "output_kb = head_layer_kb(hidden_states)\n",
    "output_cpt = head_layer_cpt(hidden_states)\n",
    "output_eb = head_layer_eb(hidden_states)\n",
    "output_spa=head_layer_spa(hidden_states)\n",
    "print(output_kb.shape)\n",
    "\n",
    "softmax_probs_kb = F.softmax(output_kb.squeeze()[2], dim=-1)\n",
    "sorted_probs_kb, sorted_indices_kb = torch.sort(softmax_probs_kb, descending=True)\n",
    "sorted_tokens_kb = [tokenizer.decode([idx]) for idx in sorted_indices_kb[:10]]\n",
    "print(sorted_tokens_kb)\n",
    "\n",
    "softmax_probs_kb = F.softmax(output.logits.squeeze()[2], dim=-1)\n",
    "sorted_probs_kb, sorted_indices_kb = torch.sort(softmax_probs_kb, descending=True)\n",
    "sorted_tokens_kb = [tokenizer.decode([idx]) for idx in sorted_indices_kb[:10]]\n",
    "print(sorted_tokens_kb)\n",
    "\n",
    "softmax_probs_cpt = F.softmax(output_cpt.squeeze()[2], dim=-1)\n",
    "sorted_probs_cpt, sorted_indices_cpt = torch.sort(softmax_probs_cpt, descending=True)\n",
    "sorted_tokens_cpt= [tokenizer.decode([idx]) for idx in sorted_indices_cpt[:10]]\n",
    "print(sorted_tokens_cpt)\n",
    "\n",
    "softmax_probs_eb = F.softmax(output_eb.squeeze()[2], dim=-1)\n",
    "sorted_probs_eb, sorted_indices_eb = torch.sort(softmax_probs_eb, descending=True)\n",
    "sorted_tokens_eb = [exbert_tokenizer.decode([idx]) for idx in sorted_indices_eb[:10]]\n",
    "print(sorted_tokens_eb)\n",
    "\n",
    "\n",
    "softmax_probs_spa = F.softmax(output_spa.squeeze()[2], dim=-1)\n",
    "sorted_probs_spa, sorted_indices_spa = torch.sort(softmax_probs_spa, descending=True)\n",
    "sorted_tokens_spa = [swerick_tokenizer.decode([idx]) for idx in sorted_indices_spa[:10]]\n",
    "print(sorted_tokens_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between layers\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_compare_feed_forward_weights(model_pre, model_post, dataloader):\n",
    "    similarities_attention = {}\n",
    "    similarities_query = {}\n",
    "    similarities_key = {}\n",
    "    similarities_value = {}\n",
    "    for (name_base, param_base), (name_fine, param_fine) in zip(model_kb.named_parameters(), model_hugging_face.named_parameters()):\n",
    "        if \"cls.predictions.transform.dense.weight\" in name_base :\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_attention[name_base]=sim\n",
    "            print(f\"{name_base} - Cosine Similarity: {sim}\")\n",
    "        if  \"cls.predictions.transform.dense.bias\" in name_base:\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_query[name_base]=sim\n",
    "        if  \"attention.self.key.bias\" in name_base:\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_key[name_base]=sim\n",
    "        if  \"attention.self.value.bias\" in name_base:\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_value[name_base]=sim\n",
    "            print(f\"{name_base} - Cosine Similarity: {sim}\")\n",
    "\n",
    "    return similarities_attention,similarities_query,similarities_key,similarities_value\n",
    "       \n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(similarities,label,x_label='Layer Number',y_label='Average Cosine Similarity',title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(similarities)), similarities, marker='o', linestyle='-', color='b',)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "mean_similarities_attention,mean_similarities_query,mean_similarities_key,mean_similarities_value = extract_and_compare_feed_forward_weights(model_kb, model_hugging_face, valid_dataloader)\n",
    "similarity_attention=[mean_similarities_attention[i] for i in mean_similarities_attention.keys()]\n",
    "similarity_query=[mean_similarities_query[i] for i in mean_similarities_query.keys()]\n",
    "similarity_key=[mean_similarities_key[i] for i in mean_similarities_key.keys()]\n",
    "similarity_value=[mean_similarities_value[i] for i in mean_similarities_value.keys()]\n",
    "\n",
    "\n",
    "\n",
    "plot_results(similarity_attention,label = mean_similarities_attention.keys(),title='layer wise cosine similarity between weights for attention.output.dense')\n",
    "plot_results(similarity_query,label = mean_similarities_query.keys(),title='layer wise cosine similarity between weights for attention.self.query')\n",
    "plot_results(similarity_key,label = mean_similarities_key.keys(),title='layer wise cosine similarity between weights for attention.self.key')\n",
    "plot_results(similarity_value,label = mean_similarities_value.keys(),title='layer wise cosine similarity between weights for attention.self.value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text,model):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,output_hidden_states=True)\n",
    "        \n",
    "    embeddings = outputs.hidden_states\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = [get_embeddings(phrase,model_kb) for phrase in date_dataset[\"train\"][\"content\"][:5]]\n",
    "print(len(embeddings_train))\n",
    "print(len(embeddings_train[0]))\n",
    "embeddings_test = [get_embeddings(phrase,model_kb) for phrase in date_dataset[\"test\"][\"content\"][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge probing : predicting noun  \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "        \n",
    "def extract_and_classify(dataset, model,length):\n",
    "    layer_accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for layer_index in range(model.config.num_hidden_layers + 1):  # Include the embedding layer\n",
    "            embeddings_train = [get_embeddings(phrase,model_kb) for phrase in dataset[\"train\"][\"content\"][:length]]\n",
    "            train_embeddings =[sentence[layer_index] for sentence in embeddings_train] \n",
    "            train_labels = date_dataset[\"train\"][\"reform_label\"][:length]\n",
    "            print(train_embeddings)\n",
    "            print(len(train_labels))\n",
    "            embeddings_test = [get_embeddings(phrase,model_kb) for phrase in dataset[\"test\"][\"content\"][:length]]\n",
    "            test_embeddings =[sentence[layer_index] for sentence in embeddings_test] \n",
    "            testlabels = date_dataset[\"test\"][\"reform_label\"][:length]\n",
    "    \n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(train_embeddings, train_labels)\n",
    "            y_pred = clf.predict(test_embeddings)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(test_labels, y_pred)\n",
    "            layer_accuracies.append((layer_index, accuracy))\n",
    "    \n",
    "    return layer_accuracies\n",
    "\n",
    "\n",
    "accuracies = extract_and_classify(date_dataset, model_kb,5)\n",
    "\n",
    "# Output the accuracies for each layer\n",
    "for layer, acc in accuracies:\n",
    "    print(f\"Layer {layer}: Accuracy {acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings =[sentence[1] for sentence in embeddings_train] \n",
    "len(train_embeddings[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = date_dataset[\"train\"][\"content\"][0]\n",
    "input = tokenizer(input,return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "input=data_collator([input])\n",
    "collated_inputs = {key: value.squeeze(1) for key, value in input.items()}\n",
    "output =model_kb(collated_inputs[\"input_ids\"],attention_mask=collated_inputs[\"attention_mask\"],labels =collated_inputs[\"labels\"],output_hidden_states=True)\n",
    "hidden_states = output.hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(examples,model):\n",
    "  # take a batch of images\n",
    "  images = examples['content']\n",
    "  images = tokenizer(images,return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "  input=data_collator([images])\n",
    "  collated_inputs = {key: value.squeeze(1) for key, value in input.items()}\n",
    "  with torch.no_grad():\n",
    "    output =model(collated_inputs[\"input_ids\"],attention_mask=collated_inputs[\"attention_mask\"],labels =collated_inputs[\"labels\"],output_hidden_states=True)\n",
    "  hidden_states = output.hidden_states\n",
    "  # add features of each layer\n",
    "  for i in range(len(hidden_states)):\n",
    "      features = torch.mean(hidden_states[i], dim=1)\n",
    "      examples[f'features_{i}'] = features.cpu().detach().numpy()\n",
    "  \n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_train_bis=Dataset.from_dict(date_dataset[\"train\"][100:150]).map(lambda example :extract_features(example,model_hugging_face), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_test_bis = Dataset.from_dict(date_dataset[\"test\"][:100]).map(lambda example :extract_features(example,model_hugging_face), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_test['features_4']==encoded_dataset_test_bis['features_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "def scores_linear_prob(train_dataset,test_dataset):\n",
    "    train_dataset = train_dataset\n",
    "    test_dataset = test_dataset\n",
    "\n",
    "    scores = dict()\n",
    "    for i in range(model_kb.config.num_hidden_layers + 1):\n",
    "        train_features = torch.Tensor(train_dataset[f'features_{i}']).squeeze(1)\n",
    "        test_features = torch.Tensor(test_dataset[f'features_{i}']).squeeze(1)\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(train_features, train_dataset['reform_label'])\n",
    "        # compute accuracy on training + test set\n",
    "        #training_score = lr_clf.score(train_features, train_dataset['reform_label'])\n",
    "        #test_score = lr_clf.score(test_features, test_dataset['reform_label'])\n",
    "        #scores[f'features_{i}'] = (training_score, test_score)\n",
    "\n",
    "        train_preds = lr_clf.predict(train_features)\n",
    "        test_preds = lr_clf.predict(test_features)\n",
    "        training_f1 = f1_score(train_dataset['reform_label'], train_preds, average='macro')\n",
    "        test_f1 = f1_score(test_dataset['reform_label'], test_preds, average='macro')\n",
    "        \n",
    "        scores[f'features_{i}'] = (training_f1, test_f1)\n",
    "        \n",
    "    return scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
