{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import comprehension_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kb = preprocessing.create_model_MLM(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "small_valid_dataset = preprocessing.create_deterministic_eval_dataset(valid_dataset.select(range(10000)),data_collator)\n",
    "small_valid_dataloader=preprocessing.create_dataloader(small_valid_dataset,64,default_data_collator)\n",
    "                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Statsrådet\"\n",
    "token_id = tokenizer.convert_tokens_to_ids(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_token(token,example):\n",
    "    return token in example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filtered_dataset = valid_dataset.filter(lambda example : special_token(token_id,example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_filtered_dataloader=preprocessing.create_dataloader(valid_filtered_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exbert = AutoModelForMaskedLM.from_pretrained(\"exbert-finetuned-imdb/checkpoint-1271340\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "config = transformers.BertConfig.from_pretrained(\"pretraining_from_scratch/checkpoint-3944175\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"pretraining_from_scratch/checkpoint-3944175\",config=config,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "valid_sentence_filtered = valid_filtered_dataset.map(lambda example : preprocessing.get_context_with_mask(example,token_id,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bis(model,dataloader, tokenizer,token_id):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    layerwise_embeddings = [[] for _ in range(model.config.num_hidden_layers + 1)]\n",
    "    preds=[]\n",
    "    for batch in dataloader :\n",
    "        tokens={key : value.to(device) for key,value in batch.items()}\n",
    "        if token_id not in list(batch[\"labels\"][0]):\n",
    "            continue\n",
    "        index=list(batch[\"labels\"][0]).index(token_id)\n",
    "        outputs= model(input_ids=tokens[\"input_ids\"],attention_mask=tokens[\"attention_mask\"],labels=tokens[\"labels\"],output_hidden_states=True)\n",
    "        preds.append(torch.argmax(F.softmax(outputs.logits.squeeze(0)[index])))\n",
    "        hidden_states = outputs.hidden_states  # tuple of (layer+1) tensors, each of shape (batch_size, seq_len, hidden_size)\n",
    "        for i, hidden_state in enumerate(hidden_states):\n",
    "            masked_embeddings = hidden_state[:, index, :].detach().cpu().numpy()  # Extract [CLS] token\n",
    "            layerwise_embeddings[i].append(masked_embeddings)\n",
    "    return [np.vstack(layer) for layer in layerwise_embeddings],preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "inputs = valid_sentence_filtered[3]\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))\n",
    "token = {key: torch.tensor(value, dtype=torch.long).unsqueeze(0).to(device) for key,value in inputs.items()}\n",
    "outputs = model_hugging_face(input_ids=token[\"input_ids\"],attention_mask=token[\"attention_mask\"],labels=token[\"labels\"],output_hidden_states=True)\n",
    "outputs2=model_kb(input_ids=token[\"input_ids\"],attention_mask=token[\"attention_mask\"],labels=token[\"labels\"],output_hidden_states=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token[\"input_ids\"].squeeze())\n",
    "index=inputs[\"labels\"].index(token_id)\n",
    "hidden_states = outputs.hidden_states\n",
    "hidden_states_kb = outputs2.hidden_states\n",
    "last_hidden_state = hidden_states[-1].squeeze().detach().cpu().numpy()\n",
    "print(tokenizer.decode(torch.argmax(outputs.logits.squeeze()[index])))\n",
    "print(tokenizer.decode(torch.argmax(outputs2.logits.squeeze()[index])))\n",
    "def plot_pca_hidden_state(hidden_state,tokens,number):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_states = pca.fit_transform(hidden_state.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    # Préparer la figure pour la visualisation\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in [\"[CLS]\",\"[SEP]\"]:\n",
    "            color = 'green'  # Token spécial\n",
    "        elif token == '[MASK]':\n",
    "            color = 'purple'  # Token masqué\n",
    "        else:\n",
    "            color = 'cyan'  # Token de contexte\n",
    "        \n",
    "        plt.scatter(reduced_states[i, 0], reduced_states[i, 1], c=color, label=token)\n",
    "        plt.text(reduced_states[i, 0], reduced_states[i, 1], token, fontsize=9)\n",
    "\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.title(f\"PCA of hidden state for one sentence at layer {number} \")\n",
    "    plt.legend(handles=[\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='cyan', markersize=10, label='Token de Contexte'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Token Spécial'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label='Token Masqué')\n",
    "    ], loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for j in range(13):\n",
    "    print(\"hugging face\")\n",
    "    plot_pca_hidden_state(hidden_states[j],tokens,j)\n",
    "    print(\"kb\")\n",
    "    plot_pca_hidden_state(hidden_states_kb[j],tokens,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(13):\n",
    "        combined_embeddings = np.concatenate([baseline_embeddings[layer], finetuned_embeddings[layer]])\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(combined_embeddings)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i, color in enumerate(['blue', 'red', 'orange', 'green']):\n",
    "            indices = [j for j, c in enumerate(colors) if c == color]\n",
    "            plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], c=color, label=color, alpha=0.5, edgecolors='w', s=50)\n",
    "\n",
    "        plt.title('t-SNE of Word Embeddings with Classification Results')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison of weights for different model\n",
    "\n",
    "\n",
    "weights_kb = comprehension_model.get_model_weights(model_kb)\n",
    "weights_finetuned = comprehension_model.get_model_weights(model_hugging_face)\n",
    "\n",
    "weight_diffs = {}\n",
    "for key in weights_kb.keys():\n",
    "    weight_diffs[key] = weights_finetuned[key] - weights_kb[key]\n",
    "    if (np.linalg.norm(weight_diffs[key])/weight_diffs[key].size) > 0.04 :\n",
    "        print(key)\n",
    "    #     print(np.linalg.norm(weight_diffs[key],2))\n",
    "    #     print(weight_diffs[key].size)\n",
    "    #     print( np.linalg.norm(weight_diffs[key])/weight_diffs[key].size)\n",
    "    \n",
    "\n",
    "weight_diffs[\"cls.predictions.decoder.bias\"] = model_hugging_face.cls.predictions.decoder.bias.detach().cpu().numpy() - model_kb.cls.predictions.decoder.bias.detach().cpu().numpy()\n",
    "weight_diffs[\"cls.predictions.decoder.weight\"] = model_hugging_face.cls.predictions.decoder.weight.detach().cpu().numpy() - model_kb.cls.predictions.decoder.weight.detach().cpu().numpy()\n",
    "norms = [(np.linalg.norm(weight_diffs[key])/weight_diffs[key].size) for key in weight_diffs.keys()]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(norms)), norms, tick_label=list(weight_diffs.keys()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frobenius Norm of Weight Differences')\n",
    "plt.title('Comparison of Weight Changes in BERT Layers')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face.state_dict()[\"bert.encoder.layer.0.attention.self.key.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages('weight_distribution_scratch.pdf') as pdf:\n",
    "    for name,param in mosaicBert.named_parameters():\n",
    "        layer_name = name\n",
    "        print(name)\n",
    "        split_name =name.split('.')\n",
    "        layer = split_name[3]\n",
    "        print(layer)\n",
    "        #comprehension_model.plot_weight_distributions(model_hugging_face, model_kb, layer_name)\n",
    "        if \"attention.self.Wqkv\"  in name and \"weight\" in name:\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.query.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][:768,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.key.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][768:1536,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.value.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][1536:,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        elif  \"attention.self.Wqkv\" in name and \"bias\" in name :\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.query.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][:768].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.key.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][768:1536].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.value.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][1536:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close() \n",
    "        elif \"mlp.gated_layers\"  in name :\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.intermediate.dense.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"mlp.wo\" in name :\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.output.dense.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"mlp.layernorm\"  in name :\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.output.LayerNorm.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"cls.predictions.decoder\" in name:\n",
    "            continue\n",
    "            \n",
    "        else :\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            weights2 = model_kb.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "print(hidden_states1[0].shape)\n",
    "i=8\n",
    "hidden_states1,logits1=get_embeddings_bis(model_kb,valid_sentence_filtered [i],tokenizer)\n",
    "hidden_states2,logits2=get_embeddings_bis(model_hugging_face,valid_sentence_filtered [i],tokenizer)\n",
    "#masked_positions =[idx for idx, token in enumerate(valid_filtered_dataset[i]['input_ids']) if token == tokenizer.mask_token_id]\n",
    "index = valid_sentence_filtered [i]['labels'].index(token_id)\n",
    "print(token_id)\n",
    "print(tokenizer.decode(valid_sentence_filtered[i]['input_ids']))\n",
    "print(tokenizer.decode(torch.argmax(F.softmax(logits1.squeeze()[index], dim=-1)).item()))\n",
    "print(tokenizer.decode(torch.argmax(F.softmax(logits2.squeeze()[index], dim=-1)).item()))\n",
    "for j in range(len(hidden_states1)) :\n",
    "    print(tokenizer.decode((valid_sentence_filtered[i]['labels'][index])))\n",
    "    print('hidden layer ',j)\n",
    "    plt.figure(figsize=(10,6))\n",
    "   #plt.hist(hidden_states1[j][0][index].detach().cpu().numpy(), bins=100, alpha=0.5, label='Baseline Model')\n",
    "    plt.hist(hidden_states1[j][0][0].detach().cpu().numpy(), bins=100, alpha=0.5, label='Baseline Model cls')\n",
    "    #plt.hist(hidden_states2[j][0][index].detach().cpu().numpy(), bins=100, alpha=0.5, label='Fine-tuned Model')\n",
    "    plt.hist(hidden_states2[j][0][0].detach().cpu().numpy(), bins=100, alpha=0.5, label='finetuned Model cls')\n",
    "    plt.xlabel('weight')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "checkpoint_directory = \"/home/laurinemeier/swerick/finetuning/finetuning_hugging_whitespace-finetuned-imdb\"\n",
    "checkpoint_files = os.listdir(checkpoint_directory)\n",
    "checkpoint_files.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)))\n",
    "selected_checkpoints = [checkpoint_files[i] for i in range(0, len(checkpoint_files), 10)]\n",
    "weight1 = model_kb.state_dict()[\"bert.embeddings.word_embeddings.weight\"].flatten().cpu().numpy()\n",
    "print(\"std kb\", np.std(weight1))\n",
    "for name in selected_checkpoints :\n",
    "    print(name)\n",
    "    model_hugging =AutoModelForMaskedLM.from_pretrained(checkpoint_directory + '/'+name)\n",
    "    weights2 = model_hugging.state_dict()[\"bert.embeddings.word_embeddings.weight\"].flatten().cpu().numpy()\n",
    "    print(np.std(weights2))\n",
    "    model_hugging.to(device)\n",
    "    comprehension_model.plot_weight_distributions(model_hugging, model_kb, \"bert.embeddings.word_embeddings.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states1 = comprehension_model.get_embeddings(model_kb, small_valid_dataloader, tokenizer)\n",
    "hidden_states2 = comprehension_model.get_embeddings(model_hugging_face, small_valid_dataloader, tokenizer)\n",
    "\n",
    "for i in range(len(hidden_states1)):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(hidden_states1[i].flatten(), bins=100, alpha=0.5, label='Baseline Model')\n",
    "        plt.hist(hidden_states2[i].flatten(), bins=100, alpha=0.5, label='Fine-tuned Model')\n",
    "        plt.title(f\"Hidden States Distribution Comparison for Layer {i}\")\n",
    "        plt.xlabel(\"Hidden States Values\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evolution of a specific layer through epochs\n",
    "checkpoint_directory = 'finetuning/finetuning_hugging_whitespace-finetuned-imdb'\n",
    "comprehension_model.evolution_specific_layer_weight(chekpoint_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study of Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(mean_similarities,x_label='Layer Number',y_label='Average Cosine Similarity',title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(mean_similarities)), mean_similarities, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "mean_similarities_hidden_states,mean_similarities_attention,diff_tot = comprehension_model.extract_and_compare_activations(model_kb, model_hugging_face, valid_filtered_dataloader,token_id)\n",
    "print(\"Layer-wise cosine similarities:\", mean_similarities_hidden_states)\n",
    "\n",
    "\n",
    "plot_results(mean_similarities_hidden_states)\n",
    "plot_results(mean_similarities_attention,'Attention Layer Number',title='Average Attention Layer-wise Cosine Similarity between Attention values Across Validation Dataset')\n",
    "plot_results(diff_tot,'Layer Number',title='Average Norm difference between hidden states  Across Validation Dataset for token {words}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def plot_results(similarities, label, x_label='Layer Number', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(similarities)), similarities, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def compare_ffn_contributions(model_pre, model_post, dataloader):\n",
    "    similarities_pre = []\n",
    "    similarities_post = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = {k: batch[k].to(device) for k in batch.keys()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre_output = model_pre(**batch, output_hidden_states=True)\n",
    "            pre_activations = pre_output.hidden_states\n",
    "            post_output = model_post(**batch, output_hidden_states=True)\n",
    "            post_activations = post_output.hidden_states\n",
    "    \n",
    "        pre_contribution = [cosine_similarity(pre_activations[layer+1].cpu(), pre_activations[layer].cpu()) for layer in range(len(pre_activations)-1)]\n",
    "        post_contribution = [cosine_similarity(post_activations[layer+1].cpu(), post_activations[layer].cpu()) for layer in range(len(post_activations)-1)]\n",
    "\n",
    "        similarities_pre.append(pre_contribution)\n",
    "        similarities_post.append(post_contribution)\n",
    "        \n",
    "        del pre_activations\n",
    "        del post_activations\n",
    "    \n",
    "    similarities_pre = np.mean(np.array(similarities_pre), axis=0)\n",
    "    similarities_post = np.mean(np.array(similarities_post), axis=0)\n",
    "    \n",
    "    return similarities_pre, similarities_post\n",
    "\n",
    "# Example usage\n",
    "mean_similarity_pr, mean_similarity_post = compare_ffn_contributions(model_kb, model_hugging_face, small_valid_dataloader)\n",
    "\n",
    "plot_results(mean_similarity_pr, label='Layer n+1 - Layer n', x_label='Layer n+1 - Layer n', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity for Pre Model')\n",
    "plot_results(mean_similarity_post, label='Layer n+1 - Layer n', x_label='Layer n+1 - Layer n', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity for Post Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between layers\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_compare_feed_forward_weights(model_pre, model_post, dataloader):\n",
    "    similarities_attention = {}\n",
    "    similarities_query = {}\n",
    "    similarities_key = {}\n",
    "    similarities_value = {}\n",
    "    for (name_base, param_base), (name_fine, param_fine) in zip(model_kb.named_parameters(), model_hugging_face.named_parameters()):\n",
    "        if \"cls.predictions.transform.dense.weight\" in name_base :\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_attention[name_base]=sim\n",
    "            print(f\"{name_base} - Cosine Similarity: {sim}\")\n",
    "        if  \"cls.predictions.transform.dense.bias\" in name_base:\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_query[name_base]=sim\n",
    "        if  \"attention.self.key.bias\" in name_base:\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_key[name_base]=sim\n",
    "        if  \"attention.self.value.bias\" in name_base:\n",
    "            sim = cosine_similarity(param_base, param_fine)\n",
    "            similarities_value[name_base]=sim\n",
    "            print(f\"{name_base} - Cosine Similarity: {sim}\")\n",
    "\n",
    "    return similarities_attention,similarities_query,similarities_key,similarities_value\n",
    "       \n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(similarities,label,x_label='Layer Number',y_label='Average Cosine Similarity',title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(similarities)), similarities, marker='o', linestyle='-', color='b',)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "mean_similarities_attention,mean_similarities_query,mean_similarities_key,mean_similarities_value = extract_and_compare_feed_forward_weights(model_kb, model_hugging_face, valid_dataloader)\n",
    "similarity_attention=[mean_similarities_attention[i] for i in mean_similarities_attention.keys()]\n",
    "similarity_query=[mean_similarities_query[i] for i in mean_similarities_query.keys()]\n",
    "similarity_key=[mean_similarities_key[i] for i in mean_similarities_key.keys()]\n",
    "similarity_value=[mean_similarities_value[i] for i in mean_similarities_value.keys()]\n",
    "\n",
    "\n",
    "\n",
    "plot_results(similarity_attention,label = mean_similarities_attention.keys(),title='layer wise cosine similarity between weights for attention.output.dense')\n",
    "plot_results(similarity_query,label = mean_similarities_query.keys(),title='layer wise cosine similarity between weights for attention.self.query')\n",
    "plot_results(similarity_key,label = mean_similarities_key.keys(),title='layer wise cosine similarity between weights for attention.self.key')\n",
    "plot_results(similarity_value,label = mean_similarities_value.keys(),title='layer wise cosine similarity between weights for attention.self.value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text,model):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,output_hidden_states=True)\n",
    "        \n",
    "    embeddings = outputs.hidden_states\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = [get_embeddings(phrase,model_kb) for phrase in date_dataset[\"train\"][\"content\"][:5]]\n",
    "print(len(embeddings_train))\n",
    "print(len(embeddings_train[0]))\n",
    "embeddings_test = [get_embeddings(phrase,model_kb) for phrase in date_dataset[\"test\"][\"content\"][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge probing : predicting noun  \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "        \n",
    "def extract_and_classify(dataset, model,length):\n",
    "    layer_accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for layer_index in range(model.config.num_hidden_layers + 1):  # Include the embedding layer\n",
    "            embeddings_train = [get_embeddings(phrase,model_kb) for phrase in dataset[\"train\"][\"content\"][:length]]\n",
    "            train_embeddings =[sentence[layer_index] for sentence in embeddings_train] \n",
    "            train_labels = date_dataset[\"train\"][\"reform_label\"][:length]\n",
    "            print(train_embeddings)\n",
    "            print(len(train_labels))\n",
    "            embeddings_test = [get_embeddings(phrase,model_kb) for phrase in dataset[\"test\"][\"content\"][:length]]\n",
    "            test_embeddings =[sentence[layer_index] for sentence in embeddings_test] \n",
    "            testlabels = date_dataset[\"test\"][\"reform_label\"][:length]\n",
    "    \n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(train_embeddings, train_labels)\n",
    "            y_pred = clf.predict(test_embeddings)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(test_labels, y_pred)\n",
    "            layer_accuracies.append((layer_index, accuracy))\n",
    "    \n",
    "    return layer_accuracies\n",
    "\n",
    "\n",
    "accuracies = extract_and_classify(date_dataset, model_kb,5)\n",
    "\n",
    "# Output the accuracies for each layer\n",
    "for layer, acc in accuracies:\n",
    "    print(f\"Layer {layer}: Accuracy {acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings =[sentence[1] for sentence in embeddings_train] \n",
    "len(train_embeddings[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = date_dataset[\"train\"][\"content\"][0]\n",
    "input = tokenizer(input,return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "input=data_collator([input])\n",
    "collated_inputs = {key: value.squeeze(1) for key, value in input.items()}\n",
    "output =model_kb(collated_inputs[\"input_ids\"],attention_mask=collated_inputs[\"attention_mask\"],labels =collated_inputs[\"labels\"],output_hidden_states=True)\n",
    "hidden_states = output.hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(examples,model):\n",
    "  # take a batch of images\n",
    "  images = examples['content']\n",
    "  images = tokenizer(images,return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "  input=data_collator([images])\n",
    "  collated_inputs = {key: value.squeeze(1) for key, value in input.items()}\n",
    "  with torch.no_grad():\n",
    "    output =model(collated_inputs[\"input_ids\"],attention_mask=collated_inputs[\"attention_mask\"],labels =collated_inputs[\"labels\"],output_hidden_states=True)\n",
    "  hidden_states = output.hidden_states\n",
    "  # add features of each layer\n",
    "  for i in range(len(hidden_states)):\n",
    "      features = torch.mean(hidden_states[i], dim=1)\n",
    "      examples[f'features_{i}'] = features.cpu().detach().numpy()\n",
    "  \n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_train_bis=Dataset.from_dict(date_dataset[\"train\"][100:150]).map(lambda example :extract_features(example,model_hugging_face), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_test_bis = Dataset.from_dict(date_dataset[\"test\"][:100]).map(lambda example :extract_features(example,model_hugging_face), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_test['features_4']==encoded_dataset_test_bis['features_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "def scores_linear_prob(train_dataset,test_dataset):\n",
    "    train_dataset = train_dataset\n",
    "    test_dataset = test_dataset\n",
    "\n",
    "    scores = dict()\n",
    "    for i in range(model_kb.config.num_hidden_layers + 1):\n",
    "        train_features = torch.Tensor(train_dataset[f'features_{i}']).squeeze(1)\n",
    "        test_features = torch.Tensor(test_dataset[f'features_{i}']).squeeze(1)\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(train_features, train_dataset['reform_label'])\n",
    "        # compute accuracy on training + test set\n",
    "        #training_score = lr_clf.score(train_features, train_dataset['reform_label'])\n",
    "        #test_score = lr_clf.score(test_features, test_dataset['reform_label'])\n",
    "        #scores[f'features_{i}'] = (training_score, test_score)\n",
    "\n",
    "        train_preds = lr_clf.predict(train_features)\n",
    "        test_preds = lr_clf.predict(test_features)\n",
    "        training_f1 = f1_score(train_dataset['reform_label'], train_preds, average='macro')\n",
    "        test_f1 = f1_score(test_dataset['reform_label'], test_preds, average='macro')\n",
    "        \n",
    "        scores[f'features_{i}'] = (training_f1, test_f1)\n",
    "        \n",
    "    return scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
