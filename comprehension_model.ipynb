{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import comprehension_model\n",
    "from datasets import Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_token(token,example):\n",
    "    return token in example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_mask_with_token_id(example, token_id, tokenizer):\n",
    "    masked_positions = [idx for idx, token in enumerate(example['input_ids']) if token == tokenizer.mask_token_id]\n",
    "    for i in range(len(masked_positions)):\n",
    "        if example['labels'][masked_positions[i]] == token_id:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_context_with_mask(example, token_id, tokenizer):\n",
    "    masked_positions = [idx for idx, token in enumerate(example['input_ids']) if token == tokenizer.mask_token_id]\n",
    "    for i in range(len(masked_positions)):\n",
    "        if example['labels'][masked_positions[i]] == token_id:\n",
    "            start_position = max(0, masked_positions[i-1]) if i > 0 else 0\n",
    "            end_position = min(len(example[\"input_ids\"]), masked_positions[i+1]) if i < len(masked_positions) - 1 else len(example[\"input_ids\"])\n",
    "            context = example[\"input_ids\"][start_position + 1:end_position]\n",
    "            context_attention_mask = example['attention_mask'][start_position + 1:end_position]\n",
    "            context_labels = example['labels'][start_position + 1:end_position]\n",
    "            print(tokenizer.decode(context))\n",
    "            return {\n",
    "                'input_ids': context,\n",
    "                'attention_mask': context_attention_mask,\n",
    "                'labels': context_labels\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bis(model,dataloader, tokenizer,token_id):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    layerwise_embeddings = [[] for _ in range(model.config.num_hidden_layers + 1)]\n",
    "    preds=[]\n",
    "    for batch in dataloader :\n",
    "        tokens={key : value.to(device) for key,value in batch.items()}\n",
    "        if token_id not in list(batch[\"labels\"][0]):\n",
    "            continue\n",
    "        index=list(batch[\"labels\"][0]).index(token_id)\n",
    "        outputs= model(input_ids=tokens[\"input_ids\"],attention_mask=tokens[\"attention_mask\"],labels=tokens[\"labels\"],output_hidden_states=True)\n",
    "        preds.append(torch.argmax(F.softmax(outputs.logits.squeeze(0)[index])))\n",
    "        hidden_states = outputs.hidden_states  # tuple of (layer+1) tensors, each of shape (batch_size, seq_len, hidden_size)\n",
    "        for i, hidden_state in enumerate(hidden_states):\n",
    "            masked_embeddings = hidden_state[:, index, :].detach().cpu().numpy()  # Extract [CLS] token\n",
    "            layerwise_embeddings[i].append(masked_embeddings)\n",
    "    return [np.vstack(layer) for layer in layerwise_embeddings],preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text,model):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,output_hidden_states=True)\n",
    "        \n",
    "    embeddings = outputs.hidden_states\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_model(trajectoire1,trajectoire2):\n",
    "    similarities_euclidean = []\n",
    "    similarities_cosinus=[]\n",
    "    for layer in range(12):\n",
    "        # Euclidean Distance\n",
    "        euclidean_dist = euclidean(trajectoire1[layer], trajectoire2[layer])\n",
    "        similarities_euclidean.append(euclidean_dist)\n",
    "        # Cosine Similarity\n",
    "        cos_sim = cosine_similarity(torch.tensor(trajectoire1[layer]), torch.tensor(trajectoire2[layer]))\n",
    "        similarities_cosinus.append(cos_sim)\n",
    "    print(np.mean(similarities_euclidean))\n",
    "    print(np.mean(similarities_cosinus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(model):\n",
    "    parameters = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        parameters[name] = param.detach().cpu().numpy().flatten()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_results(similarities, x_label='Layer Number', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity between hidden_states Across Validation Dataset'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(similarities)), similarities, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ffn_contributions_consecutive_layers(model_pre, model_post, dataloader):\n",
    "    similarities_pre = []\n",
    "\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = {k: batch[k].to(device) for k in batch.keys()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre_output = model_pre(**batch, output_hidden_states=True)\n",
    "            pre_activations = pre_output.hidden_states\n",
    "            post_output = model_post(**batch, output_hidden_states=True)\n",
    "            post_activations = post_output.hidden_states\n",
    "    \n",
    "        pre_contribution = [(pre_activations[layer+1] -pre_activations[layer]) for layer in range(len(pre_activations)-1)]\n",
    "        post_contribution = [(post_activations[layer+1] - post_activations[layer]) for layer in range(len(post_activations)-1)]\n",
    "\n",
    "        pre_activation = [cosine_similarity(pre_contribution[i],post_contribution[i]) for i in range (len(pre_contribution))]\n",
    "\n",
    "        similarities_pre.append(pre_activation)\n",
    "   \n",
    "        \n",
    "        del pre_activations\n",
    "        del post_activations\n",
    "        del pre_output\n",
    "        del post_output\n",
    "    \n",
    "    similarities_pre = np.mean(np.array(similarities_pre), axis=0)\n",
    "\n",
    "    \n",
    "    return similarities_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_linear_prob(train_dataset,test_dataset):\n",
    "    train_dataset = train_dataset\n",
    "    test_dataset = test_dataset\n",
    "\n",
    "    scores = dict()\n",
    "    for i in range(model_kb.config.num_hidden_layers + 1):\n",
    "        train_features = torch.Tensor(train_dataset[f'features_{i}']).squeeze(1)\n",
    "        test_features = torch.Tensor(test_dataset[f'features_{i}']).squeeze(1)\n",
    "        lr_clf = LogisticRegression(max_iter=1000)\n",
    "        lr_clf.fit(train_features, train_dataset['tag'])\n",
    "\n",
    "        train_preds = lr_clf.predict(train_features)\n",
    "        test_preds = lr_clf.predict(test_features)\n",
    "        training_f1 = f1_score(train_dataset['tag'], train_preds, average='macro')\n",
    "        test_f1 = f1_score(test_dataset['tag'], test_preds, average='macro')\n",
    "        \n",
    "        scores[f'features_{i}'] = (training_f1, test_f1)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(examples,model,tokenizer):\n",
    "  # take a batch of images\n",
    "  images = examples['content']\n",
    "  images = tokenizer(images,return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "  with torch.no_grad():\n",
    "    output =model(**images,output_hidden_states=True)\n",
    "  hidden_states = output.hidden_states\n",
    "  # add features of each layer\n",
    "  for i in range(len(hidden_states)):\n",
    "      features = torch.mean(hidden_states[i], dim=1)\n",
    "      #features = hidden_states[i].cpu().numpy().flatten() \n",
    "      examples[f'features_{i}'] = features.cpu().detach().numpy()\n",
    "      #examples[f'features_{i}'] = features\n",
    "  \n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"swerick_data_random_train.pkl\", \"test\": \"swerick_data_random_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kb = preprocessing.create_model_MLM(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)\n",
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exbert_tokenizer = AutoTokenizer.from_pretrained(\"exbert_tokenizer\")\n",
    "model_exbert = AutoModelForMaskedLM.from_pretrained(\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swerick_tokenizer=  AutoTokenizer.from_pretrained(\"swerick_tokenizer\")\n",
    "config = transformers.BertConfig.from_pretrained(\"pretraining_scratch/checkpoint-5258900\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"pretraining_scratch/checkpoint-5258900\",config=config,trust_remote_code=True)\n",
    "mosaicBert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "    \n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "small_valid_dataset = preprocessing.create_deterministic_eval_dataset(valid_dataset.select(range(10000)),data_collator)\n",
    "small_valid_dataloader=preprocessing.create_dataloader(small_valid_dataset,64,default_data_collator)\n",
    "                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered Valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Statsrådet\"\n",
    "token_id = tokenizer.convert_tokens_to_ids(word)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)\n",
    "valid_filtered_dataset = valid_dataset.filter(lambda example : special_token(token_id,example))\n",
    "valid_filtered_dataloader=preprocessing.create_dataloader(valid_filtered_dataset,64,default_data_collator)\n",
    "\n",
    "filtered_dataset = valid_filtered_dataset.filter(lambda example: has_mask_with_token_id(example, token_id, tokenizer))\n",
    "valid_sentence_filtered = filtered_dataset.map(lambda example: get_context_with_mask(example, token_id, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison of hidden states between different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity\n",
    "\n",
    "\n",
    "similarities = {'kb_cpt': [], 'kb_eb': [], 'cpt_eb': []}\n",
    "trajectories = {'kb': [], 'cpt': [], 'eb': []}\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "\n",
    "for i in range(len(valid_sentence_filtered )): \n",
    "    text = tokenizer.decode(valid_sentence_filtered[i][\"input_ids\"])\n",
    "    if '[MASK]' in text :\n",
    "        inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "        input_eb = exbert_tokenizer(text, return_tensors='pt').to(device)\n",
    "        input_spa = swerick_tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "        outputs = model_kb(**inputs, output_hidden_states=True)\n",
    "        outputs_hugging = model_hugging_face(**inputs, output_hidden_states=True)\n",
    "        output_eb = model_exbert(**input_eb, output_hidden_states=True)\n",
    "        output_spa = mosaicBert(**input_spa, output_hidden_states=True)\n",
    "\n",
    "        index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        index_eb = torch.where(input_eb[\"input_ids\"] == exbert_tokenizer.mask_token_id)[1]\n",
    "        index_spa = torch.where(input_spa[\"input_ids\"] == swerick_tokenizer.mask_token_id)[1]\n",
    "\n",
    "        all_hidden_states = outputs.hidden_states\n",
    "        all_hidden_states_cpt = outputs_hugging.hidden_states\n",
    "        all_hidden_states_eb = output_eb.hidden_states\n",
    "        all_hidden_states_spa = output_spa.hidden_states\n",
    "\n",
    "        \n",
    "        token_hidden_states = [layer_hidden_states[0, index.item()].detach().cpu().numpy() for layer_hidden_states in all_hidden_states]\n",
    "        token_hidden_states_cpt = [layer_hidden_states[0, index.item()].detach().cpu().numpy() for layer_hidden_states in all_hidden_states_cpt]\n",
    "        token_hidden_states_eb = [layer_hidden_states[0, index_eb.item()].detach().cpu().numpy() for layer_hidden_states in all_hidden_states_eb]\n",
    "\n",
    "        \n",
    "        kb_contribution = [(all_hidden_states[layer+1][0, index] -all_hidden_states[layer][0, index]) for layer in range(len(all_hidden_states)-1)]\n",
    "        cpt_contribution = [(all_hidden_states_cpt[layer+1][0, index] - all_hidden_states_cpt[layer][0, index]) for layer in range(len(all_hidden_states_cpt)-1)]\n",
    "        eb_contribution = [(all_hidden_states_eb[layer+1][0, index_eb] - all_hidden_states_eb[layer][0, index_eb]) for layer in range(len(all_hidden_states_eb)-1)]\n",
    "    \n",
    "        sim_kb_cpt = [cosine_similarity(kb_contribution[i],cpt_contribution[i]) for i in range (len(kb_contribution))]\n",
    "        sim_kb_eb= [cosine_similarity(kb_contribution[i],eb_contribution[i]) for i in range (len(kb_contribution))]\n",
    "        sim_cpt_eb = [cosine_similarity(eb_contribution[i],cpt_contribution[i]) for i in range (len(kb_contribution))]\n",
    "       \n",
    "        similarities['kb_cpt'].append(sim_kb_cpt)\n",
    "        similarities['kb_eb'].append(sim_kb_eb)\n",
    "        similarities['cpt_eb'].append(sim_cpt_eb)\n",
    "                \n",
    "        trajectories['kb'].append(token_hidden_states)\n",
    "        trajectories['cpt'].append(token_hidden_states_cpt)\n",
    "        trajectories['eb'].append(token_hidden_states_eb)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "# Moyenniser les similarités\n",
    "\n",
    "mean_sim_kb_cpt = np.mean(np.array(similarities['kb_cpt']),axis=0)\n",
    "mean_sim_kb_eb = np.mean(np.array(similarities['kb_eb']),axis=0)\n",
    "mean_sim_cpt_eb = np.mean(np.array(similarities['cpt_eb']),axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(mean_sim_kb_cpt)), mean_sim_kb_cpt, marker='o', linestyle='-', color='b',label='KB-cpt')\n",
    "plt.plot(range(len(mean_sim_kb_cpt)), mean_sim_kb_eb, marker='o', linestyle='-', color='red',label='KB-eb')\n",
    "plt.plot(range(len(mean_sim_kb_cpt)), mean_sim_cpt_eb, marker='o', linestyle='-', color='green',label='cpt_eb')\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Cosine similarity')\n",
    "plt.title(title)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Calculer les trajectoires moyennes pour la projection\n",
    "mean_trajectory_kb = np.mean(trajectories['kb'], axis=0)\n",
    "mean_trajectory_cpt = np.mean(trajectories['cpt'], axis=0)\n",
    "mean_trajectory_eb = np.mean(trajectories['eb'], axis=0)\n",
    "\n",
    "# PCA sur les trajectoires moyennes\n",
    "pca_2d = PCA(n_components=2)\n",
    "mean_trajectory_kb_2d = pca_2d.fit_transform(mean_trajectory_kb)\n",
    "mean_trajectory_cpt_2d = pca_2d.fit_transform(mean_trajectory_cpt)\n",
    "mean_trajectory_eb_2d = pca_2d.fit_transform(mean_trajectory_eb)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "mean_trajectory_kb_3d = pca_3d.fit_transform(mean_trajectory_kb)\n",
    "mean_trajectory_cpt_3d = pca_3d.fit_transform(mean_trajectory_cpt)\n",
    "mean_trajectory_eb_3d = pca_3d.fit_transform(mean_trajectory_eb)\n",
    "\n",
    "# Visualisation 2D des trajectoires moyennes\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(mean_trajectory_kb_2d[:, 0], mean_trajectory_kb_2d[:, 1], linestyle='-', marker='o', label='KB')\n",
    "plt.plot(mean_trajectory_cpt_2d[:, 0], mean_trajectory_cpt_2d[:, 1], linestyle='-', marker='o', color='red', label='cptBERT')\n",
    "plt.plot(mean_trajectory_eb_2d[:, 0], mean_trajectory_eb_2d[:, 1], linestyle='-', marker='o', color='green', label='sBERTex')\n",
    "plt.scatter(mean_trajectory_kb_2d[-1, 0], mean_trajectory_kb_2d[-1, 1], color='blue', s=100, label='KBBERT Last Layer')\n",
    "plt.scatter(mean_trajectory_cpt_2d[-1, 0], mean_trajectory_cpt_2d[-1, 1], color='darkred', s=100, label='cpt Last Layer')\n",
    "plt.scatter(mean_trajectory_eb_2d[-1, 0], mean_trajectory_eb_2d[-1, 1], color='darkgreen', s=100, label='sBERTex Last Layer')\n",
    "    \n",
    "plt.title('2D Projection of Mean Trajectories through Layers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation 3D des trajectoires moyennes\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(mean_trajectory_kb_3d[:, 0], mean_trajectory_kb_3d[:, 1], mean_trajectory_kb_3d[:, 2], linestyle='-', marker='o', label='KB')\n",
    "ax.plot(mean_trajectory_cpt_3d[:, 0], mean_trajectory_cpt_3d[:, 1], mean_trajectory_cpt_3d[:, 2], linestyle='-', marker='o', color='red', label='cptBERT')\n",
    "ax.plot(mean_trajectory_eb_3d[:, 0], mean_trajectory_eb_3d[:, 1], mean_trajectory_eb_3d[:, 2], linestyle='-', marker='o', color='green', label='sBERTex')\n",
    "# # Ajouter des points distinctifs pour la dernière couche\n",
    "ax.scatter(mean_trajectory_kb_3d[-1, 0], mean_trajectory_kb_3d[-1, 1], mean_trajectory_kb_3d[-1, 2], color='blue', s=100, label='KB Last Layer')\n",
    "ax.scatter(mean_trajectory_cpt_3d[-1, 0],mean_trajectory_cpt_3d[-1, 1], mean_trajectory_cpt_3d[-1, 2], color='darkred', s=100, label='cptBERT Last Layer')\n",
    "ax.scatter(mean_trajectory_eb_3d[-1, 0], mean_trajectory_eb_3d[-1, 1], mean_trajectory_eb_3d[-1, 2], color='darkgreen', s=100, label='sBERTex Last Layer')\n",
    "    \n",
    "ax.set_title('3D Projection of Mean Trajectories through Layers')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of trajectory\n",
    "\n",
    "\n",
    "sim_model(mean_trajectory_cpt,mean_trajectory_kb)\n",
    "print('KB-EB')\n",
    "sim_model(mean_trajectory_eb,mean_trajectory_kb)\n",
    "print('EB_dapt')\n",
    "sim_model(mean_trajectory_cpt,mean_trajectory_eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_similarities_hidden_states,mean_similarities_attention,diff_tot = comprehension_model.extract_and_compare_activations(model_kb, model_hugging_face, valid_filtered_dataloader,token_id)\n",
    "print(\"Layer-wise cosine similarities:\", mean_similarities_hidden_states)\n",
    "\n",
    "\n",
    "plot_results(mean_similarities_hidden_states)\n",
    "plot_results(mean_similarities_attention,'Attention Layer Number',title='Average Attention Layer-wise Cosine Similarity between Attention values Across Validation Dataset')\n",
    "plot_results(diff_tot,'Layer Number',title='Average Norm difference between hidden states  Across Validation Dataset for token {words}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference consecutive lauyers\n",
    "\n",
    "mean_similarity_pr = compare_ffn_contributions_consecutive_layers(model_kb, model_hugging_face, small_valid_dataloader)\n",
    "\n",
    "plot_results(mean_similarity_pr, x_label='Layer n+1 - Layer n', y_label='Average Cosine Similarity', title='Average Layer-wise Cosine Similarity for differences consecutive layers of KB Bert Model and cptBERT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight distribution study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison of weights for different model\n",
    "\n",
    "weights_kb = comprehension_model.get_model_weights(model_kb)\n",
    "weights_finetuned = comprehension_model.get_model_weights(model_hugging_face)\n",
    "\n",
    "weight_diffs = {}\n",
    "for key in weights_kb.keys():\n",
    "    weight_diffs[key] = weights_finetuned[key] - weights_kb[key]\n",
    "    if (np.linalg.norm(weight_diffs[key])/weight_diffs[key].size) > 0.04 :\n",
    "        print(key)\n",
    "    #     print(np.linalg.norm(weight_diffs[key],2))\n",
    "    #     print(weight_diffs[key].size)\n",
    "    #     print( np.linalg.norm(weight_diffs[key])/weight_diffs[key].size)\n",
    "    \n",
    "\n",
    "weight_diffs[\"cls.predictions.decoder.bias\"] = model_hugging_face.cls.predictions.decoder.bias.detach().cpu().numpy() - model_kb.cls.predictions.decoder.bias.detach().cpu().numpy()\n",
    "weight_diffs[\"cls.predictions.decoder.weight\"] = model_hugging_face.cls.predictions.decoder.weight.detach().cpu().numpy() - model_kb.cls.predictions.decoder.weight.detach().cpu().numpy()\n",
    "norms = [(np.linalg.norm(weight_diffs[key])/weight_diffs[key].size) for key in weight_diffs.keys()]\n",
    "embedding_indices = [list(range(0, 5))]\n",
    "encoder_layers= list(range(5 ,5 +16*(11)))  # Adjust based on actual sublayers\n",
    "head_layer_indices = list(range(181, len(norms)))\n",
    "\n",
    "xticks_positions = [0, len(encoder_layers)//2 + len(embedding_indices), len(norms)-1]\n",
    "xticks_labels = ['Embedding Layers', 'Encoder Layers', 'Head Layers']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(norms)), norms)\n",
    "plt.axvline(x=5.5, color='grey', linestyle='--')\n",
    "plt.axvline(x=len(embedding_indices) + len(encoder_layers) - 0.5, color='grey', linestyle='--')\n",
    "plt.xticks(xticks_positions, xticks_labels, rotation=0)\n",
    "plt.ylabel('Frobenius Norm of Weight Differences')\n",
    "plt.xlabel('Sublayers')\n",
    "plt.title('Comparison of Weight Changes in BERT Layers')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comaparaison weight change\n",
    "\n",
    "\n",
    "with PdfPages('weight_distribution_scratch.pdf') as pdf:\n",
    "    for name,param in mosaicBert.named_parameters():\n",
    "        layer_name = name\n",
    "        print(name)\n",
    "        split_name =name.split('.')\n",
    "        layer = split_name[3]\n",
    "        print(layer)\n",
    "        #comprehension_model.plot_weight_distributions(model_hugging_face, model_kb, layer_name)\n",
    "        if \"attention.self.Wqkv\"  in name and \"weight\" in name:\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.query.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][:768,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.key.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][768:1536,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.value.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][1536:,:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        elif  \"attention.self.Wqkv\" in name and \"bias\" in name :\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.query.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][:768].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.key.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][768:1536].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.attention.self.value.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name][1536:].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close() \n",
    "        elif \"mlp.gated_layers\"  in name :\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.intermediate.dense.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"mlp.wo\" in name :\n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.output.dense.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"mlp.layernorm\"  in name :\n",
    "            \n",
    "            weights2 = model_kb.state_dict()[f\"bert.encoder.layer.{str(layer)}.output.LayerNorm.{split_name[-1]}\"].flatten().cpu().numpy()\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        elif \"cls.predictions.decoder\" in name:\n",
    "            continue\n",
    "            \n",
    "        else :\n",
    "            weights1 = mosaicBert.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            weights2 = model_kb.state_dict()[layer_name].flatten().cpu().numpy()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(weights1, bins=100, alpha=0.5, label='finetuned Model',density=True)\n",
    "            plt.hist(weights2, bins=100, alpha=0.5, label='Baseline Model',density=True)\n",
    "            plt.title(f\"Weight Distribution Comparison for {layer_name}\")\n",
    "            plt.xlabel(\"Weight values\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Wasserstein distance of weight distribution\n",
    "\n",
    "\n",
    "params_model_1 = extract_parameters(model_kb)\n",
    "params_model_2 = extract_parameters(model_exbert)\n",
    "\n",
    "\n",
    "distances = []\n",
    "param_names = []\n",
    "\n",
    "for param_name in params_model_1:\n",
    "        distance = wasserstein_distance(params_model_1[param_name], params_model_2[param_name])\n",
    "        distances.append(distance)\n",
    "        param_names.append(param_name)\n",
    "\n",
    "# Créer un graphique de la distance de Wasserstein pour chaque paramètre\n",
    "embedding_indices = [list(range(0, 5))]\n",
    "encoder_layers= list(range(5 ,5 +16*(11)))  # Adjust based on actual sublayers\n",
    "head_layer_indices = list(range(181, len(distances)))\n",
    "xticks_positions = [0, len(encoder_layers)//2 + len(embedding_indices), len(distances)-1]\n",
    "xticks_labels = ['Embedding Layers', 'Encoder Layers', 'Head Layers']\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(distances)), distances)\n",
    "plt.axvline(x=5.5, color='grey', linestyle='--')\n",
    "plt.axvline(x=len(embedding_indices) + len(encoder_layers) - 0.5, color='grey', linestyle='--')\n",
    "plt.xticks(xticks_positions, xticks_labels, rotation=0)\n",
    "plt.ylabel('Distance of Wassertstein')\n",
    "plt.xlabel('Sublayers')\n",
    "plt.title('Distance of Wasserstein between BERT Layers of sBERTex and KB BERT')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study of intrinseque effect of layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Encoder layer\n",
    "\n",
    "cosine_cpt_final=[]\n",
    "cosine_eb_final=[]\n",
    "cosine_spa_final=[]\n",
    "hidden_states=[]\n",
    "cosine_embedding=[]\n",
    "diff_final_kb=[]\n",
    "diff_final_eb=[]\n",
    "embedding_cpt = model_hugging_face.bert.embeddings\n",
    "embedding_kb=model_kb.bert.embeddings\n",
    "\n",
    "# random_vector = torch.randn_like(hidden_states[0]).to(device)\n",
    "torch.manual_seed(33)\n",
    "for layer in range(12):\n",
    "\n",
    "    cosine_cpt=[]\n",
    "    cosine_eb=[]\n",
    "    cosine_spa=[]\n",
    "    diff_kb=[]\n",
    "    diff_eb=[]\n",
    "    last_layer_kb = model_kb.bert.encoder.layer[layer]\n",
    "    last_layer_cpt = model_hugging_face.bert.encoder.layer[layer]\n",
    "    last_layer_eb=model_exbert.bert.encoder.layer[layer]\n",
    "       \n",
    "    for batch in small_valid_dataloader :\n",
    "        batch={key:value.to(device) for key,value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output=model_kb(**batch,output_hidden_states=True)\n",
    "        if layer ==0:\n",
    "            embedding = embedding_cpt(batch[\"input_ids\"])\n",
    "            \n",
    "        hidden_states=output.hidden_states[layer].to(device)\n",
    "\n",
    "    \n",
    "        hs_kb=last_layer_kb(hidden_states)\n",
    "        hs_cpt=last_layer_cpt(hidden_states)\n",
    "        hs_eb=last_layer_eb(hidden_states)\n",
    "        \n",
    "            \n",
    "        cosine_cpt.append(cosine_similarity(hs_kb[0],hs_cpt[0]))\n",
    "        cosine_eb.append(cosine_similarity(hs_kb[0],hs_eb[0]))\n",
    "        cosine_spa.append(cosine_similarity(hs_eb[0],hs_cpt[0]))\n",
    "        diff_kb.append(cosine_similarity(hs_kb[0]-hidden_states,hs_cpt[0]-hidden_states))\n",
    "        diff_eb.append(cosine_similarity(hs_kb[0]-hidden_states,hs_eb[0]-hidden_states))\n",
    "\n",
    "    cosine_cpt_final.append(np.mean(cosine_cpt))\n",
    "    cosine_eb_final.append(np.mean(cosine_eb))\n",
    "    cosine_spa_final.append(np.mean(cosine_spa,axis=0))\n",
    "    diff_final_kb.append(np.mean(diff_kb))\n",
    "    diff_final_eb.append(np.mean(diff_eb))\n",
    "    \n",
    "    print(np.mean(cosine_cpt))\n",
    "    print(np.mean(cosine_eb))   \n",
    "            \n",
    "\n",
    "plot_results(cosine_cpt_final,x_label='Layer',title='Avg cosine similarity between KB bert and cptBERT without propagation')\n",
    "plot_results(cosine_eb_final,x_label='Layer',title='Avg cosine similarity between KB bert and sBERTex without propagation')\n",
    "plot_results(cosine_spa_final,x_label='Layer',title='Avg cosine similarity between cptBERT and sBERTex without propagation')\n",
    "plot_results(diff_final_kb,x_label='Layer',title='Avg cosine similarity between differences of consecutives layers cptBERT and KB bert without propagation')\n",
    "plot_results(diff_final_eb,x_label='Layer',title='Avg cosine similarity between differences of consecutives layers sBERTex and KB bert without propagation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Head layers\n",
    "\n",
    "text=\"Herr [MASK] von Ehrenheim : Anledningen till den framställning\"\n",
    "input_kb=tokenizer(text,return_tensors='pt').to(device)\n",
    "output=model_kb(**input_kb,output_hidden_states=True)\n",
    "\n",
    "hidden_states=output.hidden_states[-1].to(device)\n",
    "head_layer_kb=model_kb.cls\n",
    "head_layer_cpt=model_hugging_face.cls\n",
    "head_layer_eb=model_exbert.cls\n",
    "head_layer_spa=mosaicBert.cls\n",
    "output_kb = head_layer_kb(hidden_states)\n",
    "output_cpt = head_layer_cpt(hidden_states)\n",
    "output_eb = head_layer_eb(hidden_states)\n",
    "output_spa=head_layer_spa(hidden_states)\n",
    "print(output_kb.shape)\n",
    "\n",
    "softmax_probs_kb = F.softmax(output_kb.squeeze()[2], dim=-1)\n",
    "sorted_probs_kb, sorted_indices_kb = torch.sort(softmax_probs_kb, descending=True)\n",
    "sorted_tokens_kb = [tokenizer.decode([idx]) for idx in sorted_indices_kb[:10]]\n",
    "print(sorted_tokens_kb)\n",
    "\n",
    "softmax_probs_kb = F.softmax(output.logits.squeeze()[2], dim=-1)\n",
    "sorted_probs_kb, sorted_indices_kb = torch.sort(softmax_probs_kb, descending=True)\n",
    "sorted_tokens_kb = [tokenizer.decode([idx]) for idx in sorted_indices_kb[:10]]\n",
    "print(sorted_tokens_kb)\n",
    "\n",
    "softmax_probs_cpt = F.softmax(output_cpt.squeeze()[2], dim=-1)\n",
    "sorted_probs_cpt, sorted_indices_cpt = torch.sort(softmax_probs_cpt, descending=True)\n",
    "sorted_tokens_cpt= [tokenizer.decode([idx]) for idx in sorted_indices_cpt[:10]]\n",
    "print(sorted_tokens_cpt)\n",
    "\n",
    "softmax_probs_eb = F.softmax(output_eb.squeeze()[2], dim=-1)\n",
    "sorted_probs_eb, sorted_indices_eb = torch.sort(softmax_probs_eb, descending=True)\n",
    "sorted_tokens_eb = [exbert_tokenizer.decode([idx]) for idx in sorted_indices_eb[:10]]\n",
    "print(sorted_tokens_eb)\n",
    "\n",
    "\n",
    "softmax_probs_spa = F.softmax(output_spa.squeeze()[2], dim=-1)\n",
    "sorted_probs_spa, sorted_indices_spa = torch.sort(softmax_probs_spa, descending=True)\n",
    "sorted_tokens_spa = [swerick_tokenizer.decode([idx]) for idx in sorted_indices_spa[:10]]\n",
    "print(sorted_tokens_spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for spelling reform\n",
    "data_files = {\"train\":\"swerick_subsetdata_party_train.csv\",\"test\": \"swerick_subsetdata_party_test.csv\"}\n",
    "date_dataset=load_dataset(\"csv\",data_files=data_files)\n",
    "date_dataset[\"train\"] = date_dataset[\"train\"].shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = default_data_collator\n",
    "encoded_dataset_train_cpt=Dataset.from_dict(date_dataset[\"train\"][:1000]).map(lambda example :extract_features(example,model_hugging_face,tokenizer), batched=False)\n",
    "encoded_dataset_train_kb=Dataset.from_dict(date_dataset[\"train\"][:1000]).map(lambda example :extract_features(example,model_kb,tokenizer), batched=False)\n",
    "encoded_dataset_train_eb=Dataset.from_dict(date_dataset[\"train\"][:1000]).map(lambda example :extract_features(example,model_exbert,exbert_tokenizer), batched=False)\n",
    "encoded_dataset_test_cpt = Dataset.from_dict(date_dataset[\"test\"][:1000]).map(lambda example :extract_features(example,model_hugging_face,tokenizer), batched=False)\n",
    "encoded_dataset_test_kb = Dataset.from_dict(date_dataset[\"test\"][:1000]).map(lambda example :extract_features(example,model_kb,tokenizer), batched=False)\n",
    "encoded_dataset_test_eb = Dataset.from_dict(date_dataset[\"test\"][:1000]).map(lambda example :extract_features(example,model_exbert,exbert_tokenizer), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores_cpt = scores_linear_prob(encoded_dataset_train_cpt,encoded_dataset_test_cpt)\n",
    "scores_kb = scores_linear_prob(encoded_dataset_train_kb,encoded_dataset_test_kb)\n",
    "scores_eb = scores_linear_prob(encoded_dataset_train_eb,encoded_dataset_test_eb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(scores_cpt.keys())\n",
    "\n",
    "test_scores_1 = [scores_cpt[layer][1] for layer in layers]\n",
    "test_scores_2 = [scores_kb[layer][1] for layer in layers]\n",
    "test_scores_3 = [scores_eb[layer][1] for layer in layers]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "\n",
    "plt.plot(layers, test_scores_1, label='cptBERT', marker='o')\n",
    "plt.plot(layers, test_scores_2, label='KB-BERT', marker='s')\n",
    "plt.plot(layers, test_scores_3, label='sBERTex', marker='s')\n",
    "\n",
    "\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Scores for Each Layer')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, bert_version, tokenizer):\n",
    "        self.data_path_train = 'swerick_subsetdata_party_train.csv'\n",
    "        self.data_path_test = 'swerick_subsetdata_party_test.csv'\n",
    "        self.data_path_valid = 'swerick_subsetdata_party_test.csv'\n",
    "        self.bert_version = bert_version\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 3\n",
    "        self.hidden_size = 768\n",
    "        self.learning_rate = 2e-5\n",
    "        self.optimizer = 'adam'\n",
    "        self.seed = 42\n",
    "    \n",
    "    \n",
    "from evaluation.probing_experiments import GridLocProbeExperiment\n",
    "config_kb = Config(\"KBLab/bert-base-swedish-cased\",\"KBLab/bert-base-swedish-cased\")\n",
    "config_cpt = Config(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\",\"KBLab/bert-base-swedish-cased\")\n",
    "experiment_kb = GridLocProbeExperiment(config_kb)\n",
    "experiment_cpt = GridLocProbeExperiment(config_cpt)\n",
    "config_eb = Config(\"exbert-finetuned-imdb/checkpoint-6054000\",\"exbert_tokenizer\")\n",
    "experiment_eb= GridLocProbeExperiment(config_eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_kb,diff_kb =experiment_kb.cumulative_probe()\n",
    "F1_cpt,diff_cpt =experiment_cpt.cumulative_probe()\n",
    "F1_eb,diff_eb =experiment_eb.cumulative_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_kb = torch.nn.functional.softmax(experiment_kb.model.weights.detach(), dim=0).cpu().numpy()\n",
    "weights_cpt = torch.nn.functional.softmax(experiment_cpt.model.weights.detach(), dim=0).cpu().numpy()\n",
    "weights_eb = torch.nn.functional.softmax(experiment_eb.model.weights.detach(), dim=0).cpu().numpy()\n",
    "\n",
    "num_layers = len(weights_kb)\n",
    "bar_width = 0.3\n",
    "positions_kb = np.arange(num_layers)\n",
    "positions_cpt = positions_kb + bar_width\n",
    "positions_eb = positions_cpt + bar_width\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars_kb = plt.plot(positions_kb, weights_kb, color='skyblue', label='KBBERT')\n",
    "bars_cpt = plt.plot(positions_cpt, weights_cpt,color='red', label='cptBERT')\n",
    "bars_eb = plt.plot(positions_eb, weights_eb, color='green', label='sBERTex')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Layer', fontsize=14)\n",
    "plt.ylabel('Weight', fontsize=14)\n",
    "plt.title('Weights of Each Layer in ScalarMix', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "layer_labels = ['input_embedding'] + [f'Layer {i}' for i in range(1, num_layers)]\n",
    "plt.xticks(positions_kb + bar_width, labels=layer_labels, fontsize=12,rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
