{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files={\"train\":\"swerick_data_random_train.pkl\",\"test\":\"swerick_data_random_test.pkl\",\"valid\":\"swerick_data_random_valid.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "swerick_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"tokenizer_swerick.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"swerick_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference de tokenizer \n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"KBLab/bert-base-swedish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace =lambda x :x.replace('##',\"\")\n",
    "swerick_voc=list(map(replace,wrapped_tokenizer.vocab.keys()))\n",
    "base_voc=list(map(replace,base_tokenizer.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretraing_tokenizer\n",
    "inter,f,s,jaccard,vocab_f= pretraing_tokenizer.get_vocab_sim(swerick_voc,base_voc)\n",
    "print(inter)\n",
    "print(f,s)\n",
    "print(\"similarity of Jaccard\",jaccard)\n",
    "print(\"New Vocab added in tokenizer of swerick\", vocab_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"from_scratc_dataset\",\"rb\") as f:\n",
    "    tokenized_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig as TransformersBertConfig\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional, cast\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "\n",
    "class BertConfig(TransformersBertConfig):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alibi_starting_size: int = 512,\n",
    "        attention_probs_dropout_prob: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Configuration class for MosaicBert.\n",
    "\n",
    "        Args:\n",
    "            alibi_starting_size (int): Use `alibi_starting_size` to determine how large of an alibi tensor to\n",
    "                create when initializing the model. You should be able to ignore this parameter in most cases.\n",
    "                Defaults to 512.\n",
    "            attention_probs_dropout_prob (float): By default, turn off attention dropout in Mosaic BERT\n",
    "                (otherwise, Flash Attention will be off by default). Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob, **kwargs)\n",
    "        self.alibi_starting_size = alibi_starting_size\n",
    "\n",
    "with open(\"examples/examples/benchmarks/bert/yamls/main/mosaic-bert-base-uncased.yaml\") as f:\n",
    "        yaml_cfg = om.load(f)\n",
    "cfg = cast(DictConfig, yaml_cfg)  \n",
    "print(cfg)  \n",
    "        \n",
    "\n",
    "pretrained_model_name = \"KBLab/bert-base-swedish-cased\"\n",
    "model_config=cfg.model.get('model_config', None)\n",
    "print(model_config)\n",
    "config = BertConfig.from_pretrained(\n",
    "        pretrained_model_name, **model_config)\n",
    "print(config)\n",
    "if config.vocab_size % 8 != 0:\n",
    "        config.vocab_size += 8 - (config.vocab_size % 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_layers as bert_layers_module\n",
    "from composer.metrics.nlp import (BinaryF1Score, LanguageCrossEntropy,\n",
    "                                  MaskedAccuracy)\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "model = bert_layers_module.BertForMaskedLM(config)\n",
    "metrics = [\n",
    "        LanguageCrossEntropy(ignore_index=-100),\n",
    "        MaskedAccuracy(ignore_index=-100)\n",
    "    ]\n",
    "model = HuggingFaceModel(model=model,\n",
    "                                tokenizer=wrapped_tokenizer,\n",
    "                                use_logits=True,\n",
    "                                metrics=metrics)\n",
    "\n",
    "    # Padding for divisibility by 8\n",
    "    # We have to do it again here because wrapping by HuggingFaceModel changes it\n",
    "if config.vocab_size % 8 != 0:\n",
    "    config.vocab_size += 8 - (config.vocab_size % 8)\n",
    "model.model.resize_token_embeddings(config.vocab_size)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dict =torch.load(\"From_scratch_train/ep0-ba16494-rank0.pt\")\n",
    "model.load_state_dict(state_dict[\"state\"][\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict[\"state\"][\"optimizers\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{n_params=:.4e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.train_loader.dataset.get('eos_token_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "mlm_probability = 0.30\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "        tokenizer=wrapped_tokenizer,\n",
    "        mlm=mlm_probability is not None,\n",
    "        mlm_probability=mlm_probability)\n",
    "\n",
    "eos_token_id = wrapped_tokenizer.sep_token_id\n",
    "bos_token_id = wrapped_tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"],collate_fn=collate_fn,batch_size=64,num_workers=4)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"],collate_fn=collate_fn,batch_size=64,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import algorithms\n",
    "from composer.optim import DecoupledAdamW\n",
    "from composer.optim.scheduler import (ConstantWithWarmupScheduler,\n",
    "                                      CosineAnnealingWithWarmupScheduler,\n",
    "                                      LinearWithWarmupScheduler)\n",
    "\n",
    "\n",
    "algorithms = [algorithms.LowPrecisionLayerNorm()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = DecoupledAdamW(model.parameters(),\n",
    "                            lr=cfg.optimizer.lr,\n",
    "                            betas=cfg.optimizer.betas,\n",
    "                            eps=cfg.optimizer.eps,\n",
    "                            weight_decay=cfg.optimizer.weight_decay)\n",
    "\n",
    "#optimizer.load_state_dict(state_dict[\"state\"][\"optimizers\"][\"DecoupledAdamW\"])\n",
    "scheduler = LinearWithWarmupScheduler(t_warmup=cfg.scheduler.t_warmup,\n",
    "                                        alpha_f=cfg.scheduler.alpha_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import Trainer\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    run_name=\"Scratch_model\",\n",
    "    seed = cfg.seed,\n",
    "    algorithms=algorithms,\n",
    "    model=model,\n",
    "    #loggers=logger,\n",
    "   optimizers=optimizer,\n",
    "   schedulers =scheduler,\n",
    "    progress_bar=cfg.progress_bar,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    precision=cfg.precision,\n",
    "    save_folder=\"From_scratch_train\",\n",
    "    save_num_checkpoints_to_keep=100,\n",
    "    save_interval=\"1ep\",\n",
    "    max_duration=cfg.max_duration,\n",
    "    save_overwrite=True,\n",
    "    #autoresume=True,\n",
    "    log_to_console=True,\n",
    "    console_log_interval=\"1ep\",\n",
    "    #load_path=\"From_scratch_train/latest-rank0.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
