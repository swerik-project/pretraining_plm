{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from data_swerick import create_dataset_swerick\n",
    "from evaluation import evaluation_task,regression_year\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch,data_collator):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model = preprocessing.create_model_MLM(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               protocole  \\\n",
      "0      data/1867/prot-1867--ak--0118.xml   \n",
      "1      data/1867/prot-1867--ak--0121.xml   \n",
      "2      data/1867/prot-1867--ak--0123.xml   \n",
      "3      data/1867/prot-1867--ak--0124.xml   \n",
      "4      data/1867/prot-1867--ak--0125.xml   \n",
      "...                                  ...   \n",
      "12394   data/202122/prot-202122--135.xml   \n",
      "12395   data/202122/prot-202122--137.xml   \n",
      "12396   data/202122/prot-202122--138.xml   \n",
      "12397   data/202122/prot-202122--141.xml   \n",
      "12398   data/202122/prot-202122--142.xml   \n",
      "\n",
      "                                                   texte  \n",
      "0      Sedan, i kraft af Rikets Regeringsform, lagtim...  \n",
      "1      Den 21 Januari. 25 mande af tiden för dessa va...  \n",
      "2      Den 23 Januari. - 55 Onsdagen den 23 Januari. ...  \n",
      "3      60 Den 24 Januari, f. m, Thorsdagen den 24 Jan...  \n",
      "4      66 Den 25 Januari. N:o 12, med delgifvande af ...  \n",
      "...                                                  ...  \n",
      "12394  § 1 Justering av protokoll Protokollet för den...  \n",
      "12395  § 1 Justering av protokoll Protokollen för den...  \n",
      "12396  § 1 Justering av protokoll Protokollet för den...  \n",
      "12397  § 1 Anmälan om återtagande av plats i riksdage...  \n",
      "12398  § 1 Anmälan om subsidiaritetsprövningar Talman...  \n",
      "\n",
      "[12399 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset1=pd.read_pickle(\"swerick_data_random_train.pkl\")\n",
    "dataset2=pd.read_pickle(\"swerick_data_random_valid.pkl\")\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 12399\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 2673\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#datasest\n",
    "data_files = {\"train\": \"swerick_data_random_train.pkl\", \"test\": \"swerick_data_random_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372ae99b6e784bb09beddbfcf54903c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtokenize_dataset(swerick_dataset,tokenizer)\n\u001b[1;32m      3\u001b[0m tokenized_datasets\n",
      "File \u001b[0;32m~/swerick/preprocessing.py:49\u001b[0m, in \u001b[0;36mtokenize_dataset\u001b[0;34m(dataset, tokenizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_dataset\u001b[39m(dataset,tokenizer):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m examples: tokenize_function(examples, tokenizer), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocole\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3483\u001b[0m         batch,\n\u001b[1;32m   3484\u001b[0m         indices,\n\u001b[1;32m   3485\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   3486\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   3487\u001b[0m     )\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "File \u001b[0;32m~/swerick/preprocessing.py:50\u001b[0m, in \u001b[0;36mtokenize_dataset.<locals>.<lambda>\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_dataset\u001b[39m(dataset,tokenizer):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m---> 50\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m examples: tokenize_function(examples, tokenizer), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocole\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/swerick/preprocessing.py:13\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples, tokenizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples,tokenizer):\n\u001b[0;32m---> 13\u001b[0m     result \u001b[38;5;241m=\u001b[39m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexte\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mis_fast:\n\u001b[1;32m     15\u001b[0m         result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mword_ids(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]))]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2872\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m         )\n\u001b[1;32m   2957\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2959\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2960\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2961\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2962\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2963\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2964\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2965\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2966\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2967\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2968\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2969\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2970\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2971\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2972\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2973\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2974\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2976\u001b[0m     )\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2997\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3149\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3140\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3141\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3142\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3147\u001b[0m )\n\u001b[0;32m-> 3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3150\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3151\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   3152\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   3153\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   3154\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3155\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   3156\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   3157\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3158\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3159\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3160\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3161\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3162\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3163\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3164\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3165\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3166\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3167\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    505\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    506\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    507\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_datasets =preprocessing.tokenize_dataset(swerick_dataset,tokenizer)\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tokenized_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets=tokenized_datasets.remove_columns(\"protocole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm_datasets \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mgrouping_dataset(tokenized_datasets,chunk_size)\n\u001b[1;32m      2\u001b[0m lm_datasets\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "lm_datasets = preprocessing.grouping_dataset(tokenized_datasets,chunk_size)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lm_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid={\"valid\":\"swerick_data_random_valid.pkl\"}\n",
    "valid_dataset = load_dataset(\"pandas\",data_files=data_valid) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtokenize_dataset(valid_dataset,tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "valid_dataset =preprocessing.tokenize_dataset(valid_dataset,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=preprocessing.grouping_dataset(valid_dataset,chunk_size)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"wb\") as f:\n",
    "     pickle.dump(valid_dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 762794\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\",\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial with a manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "    num_rows: 3663965\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"])\n",
    "\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset[\"valid\"],data_collator)\n",
    "\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)\n",
    "print(\"ok\")\n",
    "\n",
    "#for batch in train_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "#for batch in eval_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(eval_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 4576, 49794,     4,  ..., 49796,    66, 20114],\n",
      "        [ 2189,   127, 44794,  ...,    54,   825,  8917],\n",
      "        [    7,   361,  1119,  ...,  6322, 26201,   421],\n",
      "        ...,\n",
      "        [    4,    31,   408,  ..., 31153,   169, 49791],\n",
      "        [47053, 49799,  2870,  ...,    10, 25440,   604],\n",
      "        [    4,    19,    36,  ...,    48,   217,   127]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[-100, -100, 3426,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1031, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [3491, -100, -100,  ..., -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataloader.dataset)\n",
    "print(eval_dataloader)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_ids\"].device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    train =DataLoader(\n",
    "    lm_dataset_bis[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n",
    "    train = [inputs.to(device) for inputs in train_dataloader]\n",
    "    return train\n",
    "\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bis = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_bis=model_bis.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bis.eval()\n",
    "\n",
    "total_loss = 0.0  # Variable to accumulate total loss\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bis(**batch)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()   # Accumulate the batch loss\n",
    "\n",
    "# Calculate the average loss\n",
    "average_loss = total_loss / len(eval_dataloader)\n",
    "\n",
    "print(f\"Initial Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model_bis.parameters(), lr=1.3e-5)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "losses_train=[]\n",
    "losses_test=[]\n",
    "#train_dataloader = get_dataloader()\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model_bis.train()\n",
    "    print(next(model_bis.parameters()).device)\n",
    "    print(epoch)\n",
    "    params_before_optimization = [param.data.clone() for param in model_bis.parameters()]\n",
    "    total_loss_train = 0.0 \n",
    "    train_dataloader = get_dataloader()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model_bis(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        params_after_optimization = [param.data for param in model_bis.parameters()]\n",
    "        parameters_changed = any((param_before != param_after).any() for param_before, param_after in zip(params_before_optimization, params_after_optimization))\n",
    "        #if parameters_changed==True :\n",
    "             # print(parameters_changed) \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    losses_train.append(total_loss_train/len(train_dataloader))\n",
    "    print(\"losses_train\",losses_train)\n",
    "\n",
    "    # Evaluation\n",
    "    model_bis.eval()\n",
    "    losses=[]\n",
    "    total_loss_eval=0.0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bis(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        total_loss_eval +=loss.item()\n",
    "\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "       perplexity = float(\"inf\")\n",
    "\n",
    "    losses_test.append(total_loss_eval/len(eval_dataloader))\n",
    "\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    print(\"losses_test\",losses_test)\n",
    "\n",
    "print(\"epoch\",num_train_epochs)\n",
    "plt.plot(range(num_train_epochs),losses_train,label=\"train Loss\")\n",
    "\n",
    "plt.plot(range(num_train_epochs),losses_test,label=\"test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_train)\n",
    "print(losses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"finetuning_manual\"\n",
    "model_bis.save_pretrained(file_path)\n",
    "tokenizer.save_pretrained(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"losses.pkl\"\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump({'losses_train': losses_train, 'losses_test': losses_test}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"./test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_long=AutoModelForMaskedLM.from_pretrained(\"./finetuning_hugging-finetuned-imdb/checkpoint-259384\")\n",
    "model_long=model_long.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(\"./test_model\")\n",
    "model=model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-4408250\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_kb=AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n",
      "Manual perplexity...\n",
      " Perplexity: 2.526427403888958\n",
      "Accuracy...\n",
      "Accuracy: 0.7799194246074795\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model_hugging_face,valid_dataloader,\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-4179250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_task(model,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(0.13192342221736908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données à partir du fichier JSON\n",
    "with open(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-4408250/trainer_state.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epoch_train = []\n",
    "epoch_test=[]\n",
    "\n",
    "for entry in data['log_history']:\n",
    "    if 'loss' in entry:\n",
    "        train_loss.append(entry['loss'])\n",
    "        epoch_train.append((entry['epoch']))\n",
    "    elif 'eval_loss' in entry:\n",
    "        eval_loss.append(entry['eval_loss'])\n",
    "        epoch_test.append((entry['epoch']))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_train, train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epoch_test, eval_loss, label='Evaluation Loss', marker='o')\n",
    "plt.title('Training and Evaluation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id= tokenizer.pad_token_id\n",
    "sep_token_id = tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_token(model,dataloader):\n",
    "    model.eval()\n",
    "    correct_pred=[]\n",
    "    incorrect_pred=[]\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        with torch.no_grad():\n",
    "            outputs=model(**batch)\n",
    "        predictions=torch.argmax(outputs.logits,dim=-1)\n",
    "        indices_tokens_masked = torch.nonzero(batch[\"labels\"] != -100, as_tuple=False)\n",
    "        correct_indices=[]\n",
    "        incorrect_indices=[]\n",
    "        for id,label in enumerate(batch[\"labels\"][indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]]):\n",
    "            if label.item() == predictions[indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]][id]:\n",
    "                correct_indices.append(id)\n",
    "            else :\n",
    "                incorrect_indices.append(id)\n",
    "        correct_pred.extend(batch['labels'][indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]][correct_indices])\n",
    "        incorrect_pred.extend(batch['labels'][indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]][incorrect_indices])\n",
    "\n",
    "    return correct_pred,incorrect_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_correct,hugging_face_incorrect = evaluate_model_token(model_hugging_face,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+',\n",
       " '6',\n",
       " '«',\n",
       " '?',\n",
       " 'J',\n",
       " ',',\n",
       " 'å',\n",
       " 'y',\n",
       " 'ë',\n",
       " '$',\n",
       " 'E',\n",
       " 'k',\n",
       " 'U',\n",
       " 'á',\n",
       " \"'\",\n",
       " 'Y',\n",
       " 'f',\n",
       " 'w',\n",
       " '>',\n",
       " 'm',\n",
       " 'G',\n",
       " '_',\n",
       " '2',\n",
       " '&',\n",
       " 'O',\n",
       " 'n',\n",
       " 'i',\n",
       " 'ö',\n",
       " 'ä',\n",
       " 'I',\n",
       " '£',\n",
       " ':',\n",
       " '-',\n",
       " '.',\n",
       " 'd',\n",
       " 'D',\n",
       " '*',\n",
       " 'V',\n",
       " '[',\n",
       " ']',\n",
       " 'ç',\n",
       " 'A',\n",
       " 'Q',\n",
       " '!',\n",
       " 'l',\n",
       " 'K',\n",
       " 'é',\n",
       " 'h',\n",
       " 'q',\n",
       " 'à',\n",
       " 'b',\n",
       " 'T',\n",
       " 'P',\n",
       " 'c',\n",
       " 'N',\n",
       " '8',\n",
       " 'g',\n",
       " 'ü',\n",
       " '§',\n",
       " '%',\n",
       " 'L',\n",
       " 'Z',\n",
       " '=',\n",
       " 'r',\n",
       " '4',\n",
       " 'a',\n",
       " '\"',\n",
       " '#',\n",
       " '—',\n",
       " 'ã',\n",
       " ';',\n",
       " '1',\n",
       " 'W',\n",
       " 'S',\n",
       " 'É',\n",
       " 'M',\n",
       " 'H',\n",
       " 'R',\n",
       " ')',\n",
       " 'j',\n",
       " 'F',\n",
       " 'Ö',\n",
       " 'Ä',\n",
       " 'z',\n",
       " 'ø',\n",
       " '|',\n",
       " 'Å',\n",
       " '€',\n",
       " '(',\n",
       " 'è',\n",
       " 'x',\n",
       " '»',\n",
       " 'B',\n",
       " 's',\n",
       " '0',\n",
       " 'v',\n",
       " 'Ó',\n",
       " 't',\n",
       " '5',\n",
       " '<',\n",
       " 'ó',\n",
       " ' ',\n",
       " 'X',\n",
       " 'e',\n",
       " 'u',\n",
       " '9',\n",
       " '3',\n",
       " 'p',\n",
       " 'o',\n",
       " 'C',\n",
       " '/',\n",
       " '7']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_face_incorrect_unique = list(set(hugging_face_incorrect))\n",
    "hugging_face_incorrect_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_correct,hugging_incorrect=evaluate_model_token(model_hugging_face,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_correct,base_incorrect=evaluate_model_token(model_kb,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def decoding_text(list):\n",
    "    counter = Counter(list)\n",
    "\n",
    "    # Trier les éléments par leur fréquence décroissante\n",
    "    sorted_numbers = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extraire les chiffres triés\n",
    "    unique_sorted_numbers = [num for num, _ in sorted_numbers]\n",
    "\n",
    "    decoded_texts = []\n",
    "    for tensor in unique_sorted_numbers:\n",
    "        decoded_text = tokenizer.decode(tensor.item())\n",
    "        decoded_texts.append(decoded_text)\n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_base_correct = decoding_text(base_correct)\n",
    "decoded_hugging_correct = decoding_text(hugging_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5899161482519455\n",
      "0.7792149671790606\n"
     ]
    }
   ],
   "source": [
    "print(len(decoded_base_correct)/(len(decoded_base_correct)+len(base_incorrect)))\n",
    "print(len(decoded_hugging_correct)/(len(decoded_hugging_correct)+len(hugging_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['flät',\n",
       " '##oarer',\n",
       " 'FAST',\n",
       " '##tain',\n",
       " 'köttbullar',\n",
       " 'Stadens',\n",
       " '##omter',\n",
       " 'gled',\n",
       " 'Smart',\n",
       " 'Längd',\n",
       " '##vita',\n",
       " 'lättad',\n",
       " 'Up',\n",
       " 'sankt',\n",
       " '##smaskin',\n",
       " '##etecken',\n",
       " 'Eget',\n",
       " '##stjärnor',\n",
       " '##icers',\n",
       " '##bero',\n",
       " '##IER',\n",
       " '##alom',\n",
       " '##frusna',\n",
       " '##butik',\n",
       " 'invester',\n",
       " 'avsiktligt',\n",
       " '##ANN',\n",
       " 'Europat',\n",
       " 'Måndag',\n",
       " 'vandringen',\n",
       " 'that',\n",
       " 'Örgryte',\n",
       " '##ändskt',\n",
       " 'Alex',\n",
       " 'nybyggt',\n",
       " '##växel',\n",
       " '##aton',\n",
       " 'Burton',\n",
       " '##YK',\n",
       " '##ugget',\n",
       " '##artan',\n",
       " 'östern',\n",
       " 'Globen',\n",
       " 'Mood',\n",
       " '##ward',\n",
       " 'avveck',\n",
       " '##ales',\n",
       " '##avd',\n",
       " '##såsen',\n",
       " '##miral',\n",
       " 'grytan',\n",
       " 'vänskapen',\n",
       " '##harmon',\n",
       " '##ssäsongen',\n",
       " '##embl',\n",
       " 'Raf',\n",
       " 'Stein',\n",
       " 'avhandlingen',\n",
       " 'uppn',\n",
       " 'musikdirektör',\n",
       " '##ografier',\n",
       " 'trampade',\n",
       " '##less',\n",
       " 'Cirk',\n",
       " '##igan',\n",
       " '##127',\n",
       " '##oplan',\n",
       " 'Teresa',\n",
       " '##ipa',\n",
       " 'skotten',\n",
       " 'sun',\n",
       " 'mästar',\n",
       " '##sutställningen',\n",
       " 'Emilia',\n",
       " 'Österb',\n",
       " '##ävlan',\n",
       " '##skiv',\n",
       " '668',\n",
       " 'uppen',\n",
       " '##merna',\n",
       " 'gamm',\n",
       " '##rukna',\n",
       " '##lekar',\n",
       " '##gnost',\n",
       " '##oning',\n",
       " '786',\n",
       " '##provinsen',\n",
       " '749',\n",
       " 'Hopkins',\n",
       " 'GL',\n",
       " '##RF',\n",
       " 'grädd',\n",
       " 'Ambul',\n",
       " '##ätare',\n",
       " '##fönster',\n",
       " '##uvan',\n",
       " '##löpande',\n",
       " '##utex',\n",
       " '##flöj',\n",
       " 'Järvs',\n",
       " 'publikt',\n",
       " '##hett',\n",
       " 'Augusta',\n",
       " '##köttet',\n",
       " 'enl',\n",
       " 'korsar',\n",
       " 'frodas',\n",
       " 'baken',\n",
       " 'Rust',\n",
       " 'egenhändigt',\n",
       " '050',\n",
       " '##sökte',\n",
       " 'motgång',\n",
       " 'Wret',\n",
       " '##moln',\n",
       " 'deporter',\n",
       " '##spänning',\n",
       " 'nyöpp',\n",
       " '##amet',\n",
       " '##ungan',\n",
       " 'Cec',\n",
       " 'ugg',\n",
       " '##sholmen',\n",
       " 'jättelika',\n",
       " '##cia',\n",
       " 'övergiven',\n",
       " 'svängarna',\n",
       " 'Chans',\n",
       " '##66',\n",
       " '##sborgs',\n",
       " '##artor',\n",
       " '##eci',\n",
       " 'tab',\n",
       " '##unch',\n",
       " 'samlare',\n",
       " '709',\n",
       " 'Wes',\n",
       " 'Monta',\n",
       " '##skaja',\n",
       " '##ynes',\n",
       " 'Endre',\n",
       " '##sven',\n",
       " '##glöm',\n",
       " 'arbet',\n",
       " 'käng',\n",
       " '##ÖK',\n",
       " 'Wen',\n",
       " 'Petit',\n",
       " 'mv',\n",
       " '##85',\n",
       " '##analysen',\n",
       " 'tjuvarna',\n",
       " '##haus',\n",
       " 'Tell',\n",
       " '##rakten',\n",
       " 'efterträda',\n",
       " 'foton',\n",
       " '##sprin',\n",
       " '##skylt',\n",
       " 'Clar',\n",
       " 'angel',\n",
       " 'astrona',\n",
       " '##reation',\n",
       " '##attar',\n",
       " 'Rydell',\n",
       " 'Kostar',\n",
       " '##nick',\n",
       " 'Nordqvist',\n",
       " '##ktis',\n",
       " 'Hirsch',\n",
       " 'egyptiska',\n",
       " 'Bonniers',\n",
       " 'galen',\n",
       " '##edöm',\n",
       " '##ologerna',\n",
       " '##italet',\n",
       " '##lagor',\n",
       " '##ussar',\n",
       " '##ADE',\n",
       " 'illustrationer',\n",
       " 'förkn',\n",
       " 'Aut',\n",
       " '##jten',\n",
       " 'Tf',\n",
       " 'tha',\n",
       " '##mir',\n",
       " 'Klipp',\n",
       " 'Said',\n",
       " 'Kro',\n",
       " '##ther',\n",
       " 'Matts',\n",
       " '##more',\n",
       " 'Gere',\n",
       " '##hjulet',\n",
       " 'Reyn',\n",
       " 'Eskilst',\n",
       " 'Philadelphia',\n",
       " 'klostret',\n",
       " '##järna',\n",
       " 'Ander',\n",
       " '##makaren',\n",
       " 'Friday',\n",
       " '##hos',\n",
       " 'äkten',\n",
       " 'Orm',\n",
       " '##nette',\n",
       " 'krigar',\n",
       " 'knän',\n",
       " '755',\n",
       " 'välplanerade',\n",
       " 'Db',\n",
       " '##skontakt',\n",
       " 'krama',\n",
       " 'misstänktes',\n",
       " 'Peters',\n",
       " '##kvällen',\n",
       " 'Stephen',\n",
       " 'kvarstod',\n",
       " '##rädgården',\n",
       " 'ingivas',\n",
       " '##ässan',\n",
       " 'daghemmet',\n",
       " '##rockar',\n",
       " '##ION',\n",
       " '##artjänsten',\n",
       " '##cirkel',\n",
       " 'backarna',\n",
       " 'Chansen',\n",
       " 'protestantiska',\n",
       " 'soul',\n",
       " 'AIK',\n",
       " 'made',\n",
       " '##fester',\n",
       " '##Sk',\n",
       " 'Valborg',\n",
       " '##minen',\n",
       " '##sgar',\n",
       " '##räffande',\n",
       " 'Vit',\n",
       " '##52',\n",
       " '##ofog',\n",
       " 'lättskött',\n",
       " '757',\n",
       " '##yrkom',\n",
       " 'Alexis',\n",
       " 'puff',\n",
       " 'insänd',\n",
       " 'Lang',\n",
       " 'hektiska',\n",
       " '##IEN',\n",
       " 'kista',\n",
       " '##ÅR',\n",
       " 'bister',\n",
       " '##saft',\n",
       " '##anch',\n",
       " '##smästaren',\n",
       " '##ippen',\n",
       " 'intermezz',\n",
       " 'tjugofem',\n",
       " 'aw',\n",
       " '##smatch',\n",
       " 'bokstä',\n",
       " 'Hedem',\n",
       " '##ankt',\n",
       " 'torra',\n",
       " '##ublic',\n",
       " 'lagets',\n",
       " 'BEK',\n",
       " 'brittiskt',\n",
       " '##ertig',\n",
       " 'Förresten',\n",
       " 'tav',\n",
       " '##estation',\n",
       " '##aså',\n",
       " '##icia',\n",
       " 'irländ',\n",
       " '##värr',\n",
       " 'Smör',\n",
       " '##edrag',\n",
       " 'bortglömda',\n",
       " 'administ',\n",
       " '##bloms',\n",
       " 'nedanstående',\n",
       " '##eley',\n",
       " '##åkare',\n",
       " 'käns',\n",
       " '##jerg',\n",
       " '##bbats',\n",
       " 'doc',\n",
       " 'formuleras',\n",
       " '##ronic',\n",
       " 'Google',\n",
       " 'font',\n",
       " '##leverantör',\n",
       " '##stadt',\n",
       " 'envis',\n",
       " '##agat',\n",
       " 'Bean',\n",
       " '##finnas',\n",
       " 'reag',\n",
       " 'aman',\n",
       " '##andagen',\n",
       " 'Palmer',\n",
       " 'braskamin',\n",
       " 'lår',\n",
       " 'befordrades',\n",
       " '##sker',\n",
       " '##eled',\n",
       " '##EET',\n",
       " '##ollar',\n",
       " 'Silv',\n",
       " '##ornet',\n",
       " '##sutställningar',\n",
       " 'Barcelona',\n",
       " 'kyr',\n",
       " '##hålet',\n",
       " 'Stenson',\n",
       " 'Takt',\n",
       " 'Freeman',\n",
       " 'Hallström',\n",
       " '##ÅNG',\n",
       " 'Cop',\n",
       " 'Ping',\n",
       " '##igenom',\n",
       " '##anas',\n",
       " 'andades',\n",
       " '##ifik',\n",
       " 'People',\n",
       " '##uel',\n",
       " '##öjt',\n",
       " 'Stenbeck',\n",
       " '##itteringen',\n",
       " 'tillbehör',\n",
       " '##skara',\n",
       " 'fle',\n",
       " 'Pool',\n",
       " '##eig',\n",
       " 'BAK',\n",
       " '##smiljön',\n",
       " 'Maja',\n",
       " 'Limhamn',\n",
       " 'Palmgren',\n",
       " '##fisken',\n",
       " 'äkt',\n",
       " 'Jes',\n",
       " 'sjunk',\n",
       " 'Corp',\n",
       " '##skommunikation',\n",
       " '##yle',\n",
       " 'Solv',\n",
       " '##ogie',\n",
       " '##östra',\n",
       " 'Kronprinsessan',\n",
       " 'Filmen',\n",
       " '##karlen',\n",
       " '##amo',\n",
       " 'skakande',\n",
       " 'tyget',\n",
       " '##ucka',\n",
       " 'katoliker',\n",
       " '##198',\n",
       " '##itos',\n",
       " 'häd',\n",
       " '##useets',\n",
       " 'Hew',\n",
       " '##rond',\n",
       " '##åverkan',\n",
       " '##imerat',\n",
       " '##repren',\n",
       " 'Eneby',\n",
       " 'präktig',\n",
       " '##eker',\n",
       " '##ures',\n",
       " 'plattan',\n",
       " '712',\n",
       " '##ÖS',\n",
       " '##plattor',\n",
       " 'Grat',\n",
       " '##rader',\n",
       " '##ettes',\n",
       " 'Ps',\n",
       " '##179',\n",
       " 'Tåget',\n",
       " '##ioter',\n",
       " '##spis',\n",
       " 'Rivière',\n",
       " 'Ces',\n",
       " 'Dana',\n",
       " 'Ond',\n",
       " 'barnvänligt',\n",
       " '##stidningen',\n",
       " 'Bjerk',\n",
       " 'Sat',\n",
       " '##iren',\n",
       " 'Osa',\n",
       " 'karri',\n",
       " 'försvunnen',\n",
       " '##ember',\n",
       " 'Häcken',\n",
       " '##uddar',\n",
       " '##olika',\n",
       " 'Fridhem',\n",
       " 'påpek',\n",
       " '##vänder',\n",
       " '##sleden',\n",
       " 'tillträd',\n",
       " '##lir',\n",
       " '##ejo',\n",
       " 'Lönneberga',\n",
       " '##ansök',\n",
       " '##andar',\n",
       " 'Out',\n",
       " 'Mort',\n",
       " '##iansen',\n",
       " '##liss',\n",
       " 'Häl',\n",
       " 'sekunden',\n",
       " '##talien',\n",
       " '##någ',\n",
       " '##lay',\n",
       " '##NING',\n",
       " 'EFTER',\n",
       " '##zzo',\n",
       " '##akes',\n",
       " 'Ladd',\n",
       " '##ply',\n",
       " '##veriet',\n",
       " 'uppsatser',\n",
       " 'Hipp',\n",
       " '##skytt',\n",
       " 'Hud',\n",
       " 'djär',\n",
       " '##ville',\n",
       " '##skjuten',\n",
       " 'vap',\n",
       " 'Beat',\n",
       " 'Skicka',\n",
       " 'Jones',\n",
       " '##gäst',\n",
       " '##lum',\n",
       " 'Snäll',\n",
       " 'Lagerbäck',\n",
       " 'Tjugo',\n",
       " '##43',\n",
       " 'Corn',\n",
       " 'Anläggningen',\n",
       " 'Stellan',\n",
       " '##Ne',\n",
       " '##spur',\n",
       " '##mart',\n",
       " 'Fart',\n",
       " '##segling',\n",
       " 'Dell',\n",
       " 'Bremen',\n",
       " '##150',\n",
       " '##aug',\n",
       " '##hyllor',\n",
       " '##steater',\n",
       " '##järt',\n",
       " '##svall',\n",
       " 'tänd',\n",
       " 'skutt',\n",
       " 'italienare',\n",
       " 'Kurserna',\n",
       " 'Hyre',\n",
       " 'välkom',\n",
       " '##ove',\n",
       " '##haw',\n",
       " 'kick',\n",
       " 'Argent',\n",
       " '##sfallet',\n",
       " 'Bent',\n",
       " '##rass',\n",
       " 'tredjepar',\n",
       " 'Ou',\n",
       " 'summerar',\n",
       " '##giro',\n",
       " '##lipper',\n",
       " '##lish',\n",
       " '##ardin',\n",
       " 'vetemjöl',\n",
       " '724',\n",
       " 'massage',\n",
       " '##umb',\n",
       " 'omdiskuterade',\n",
       " 'Kaf',\n",
       " '##emoni',\n",
       " 'rodd',\n",
       " 'ML',\n",
       " '##antas',\n",
       " '##geln',\n",
       " '##iaden',\n",
       " '##holt',\n",
       " '##128',\n",
       " '##Marie',\n",
       " '##musiker',\n",
       " '##grenska',\n",
       " 'Mimi',\n",
       " '##mässa',\n",
       " '##skyddade',\n",
       " 'månadskostnad',\n",
       " 'lagas',\n",
       " 'befint',\n",
       " '##ative',\n",
       " 'Dubb',\n",
       " '##producent',\n",
       " 'Nicar',\n",
       " 'olag',\n",
       " 'VÄST',\n",
       " 'handpenning',\n",
       " 'bätt',\n",
       " '##Ü',\n",
       " '##sponden',\n",
       " 'put',\n",
       " '##eper',\n",
       " 'Contin',\n",
       " 'illustr',\n",
       " 'arb',\n",
       " '##alagen',\n",
       " 'Reb',\n",
       " '##infar',\n",
       " 'Mild',\n",
       " 'Bomb',\n",
       " 'maskerad',\n",
       " 'lekfull',\n",
       " '##sbär',\n",
       " '##hol',\n",
       " '##atel',\n",
       " '##enc',\n",
       " '##arek',\n",
       " 'Jok',\n",
       " '##administrativa',\n",
       " 'express',\n",
       " '##efer',\n",
       " '##OCK',\n",
       " 'Zamb',\n",
       " 'Fut',\n",
       " 'Lap',\n",
       " 'förkunskaper',\n",
       " 'koma',\n",
       " 'Lennartsson',\n",
       " '##61',\n",
       " 'MODER',\n",
       " '##erman',\n",
       " 'soft',\n",
       " 'hits',\n",
       " 'spricker',\n",
       " '##säcken',\n",
       " '##emellan',\n",
       " 'Eger',\n",
       " '##orge',\n",
       " '##ningskort',\n",
       " 'motvind',\n",
       " 'livssyn',\n",
       " 'INGEN',\n",
       " 'Kerr',\n",
       " '##176',\n",
       " 'byrås',\n",
       " '##ivan',\n",
       " 'återfanns',\n",
       " 'ubåten',\n",
       " 'omöj',\n",
       " '##boden',\n",
       " '##RING',\n",
       " 'jubil',\n",
       " 'Lug',\n",
       " 'Mey',\n",
       " 'Dix',\n",
       " '##ulo',\n",
       " '##ustens',\n",
       " '##ATA',\n",
       " '##ylvan',\n",
       " 'Vågar',\n",
       " 'Toronto',\n",
       " 'Söndagar',\n",
       " '##onerande',\n",
       " '##skod',\n",
       " '##bytare',\n",
       " 'Södert',\n",
       " 'Jylland',\n",
       " '##burn',\n",
       " '##atu',\n",
       " '##OP',\n",
       " '##illo',\n",
       " 'avbröt',\n",
       " 'Wass',\n",
       " '##götlands',\n",
       " 'skärmen',\n",
       " 'sw',\n",
       " 'à',\n",
       " '##ugården',\n",
       " '##smottag',\n",
       " '##histor',\n",
       " '##deman',\n",
       " '##701',\n",
       " '##kamraten',\n",
       " '##111',\n",
       " '##klund',\n",
       " '##frik',\n",
       " '617',\n",
       " 'Mak',\n",
       " '##kir',\n",
       " '##ruvorna',\n",
       " 'Sura',\n",
       " 'Greg',\n",
       " 'Milles',\n",
       " 'Rus',\n",
       " '##oil',\n",
       " 'Vett',\n",
       " '##estra',\n",
       " '##SKOL',\n",
       " '##183',\n",
       " 'Hedvig',\n",
       " '595',\n",
       " 'FBI',\n",
       " 'segt',\n",
       " '##isto',\n",
       " 'rådgiv',\n",
       " '##pelet',\n",
       " '##samhäll',\n",
       " '##gärderna',\n",
       " 'musklerna',\n",
       " '##ierark',\n",
       " 'skålar',\n",
       " '##uvud',\n",
       " 'Long',\n",
       " '##ungarna',\n",
       " 'estetiskt',\n",
       " '##brö',\n",
       " 'Wag',\n",
       " '##bjudna',\n",
       " '##äckar',\n",
       " 'ÅLDER',\n",
       " 'kombin',\n",
       " 'Skytt',\n",
       " '##efull',\n",
       " 'dad',\n",
       " 'Love',\n",
       " 'potten',\n",
       " 'Inside',\n",
       " '##oule',\n",
       " 'Nordmakedonien',\n",
       " '##onius',\n",
       " '##imb',\n",
       " 'serve',\n",
       " 'Hid',\n",
       " 'berättelserna',\n",
       " 'slar',\n",
       " '##fota',\n",
       " 'Riss',\n",
       " '##ph',\n",
       " '##omy',\n",
       " 'kantonen',\n",
       " 'måltiden',\n",
       " '##204',\n",
       " 'Jokkm',\n",
       " '##engar',\n",
       " '##papperen',\n",
       " '##Ol',\n",
       " 'Haj',\n",
       " 'Deb',\n",
       " '##stick',\n",
       " '##IAL',\n",
       " '##skuponger',\n",
       " '##villa',\n",
       " 'Kontoret',\n",
       " 'Lyd',\n",
       " 'strip',\n",
       " 'Food',\n",
       " '##smäk',\n",
       " 'Sov',\n",
       " '##cl',\n",
       " '##gal',\n",
       " 'sås',\n",
       " 'Joachim',\n",
       " '##84',\n",
       " 'allmäng',\n",
       " '##omagasin',\n",
       " 'komedi',\n",
       " 'CNN',\n",
       " 'likvidator',\n",
       " '##sim',\n",
       " '##ellertid',\n",
       " '##tendenten',\n",
       " 'Trea',\n",
       " '##305',\n",
       " 'Steen',\n",
       " '##ertz',\n",
       " 'China',\n",
       " '##illen',\n",
       " '##imir',\n",
       " 'Nanny',\n",
       " '##såker',\n",
       " 'tul',\n",
       " 'kompisarna',\n",
       " '##kören',\n",
       " '##spapper',\n",
       " '##nöj',\n",
       " '##CKER',\n",
       " 'Sep',\n",
       " 'fans',\n",
       " '##ubbar',\n",
       " 'Sandhamn',\n",
       " '##cel',\n",
       " 'Jann',\n",
       " 'Broman',\n",
       " '##fire',\n",
       " '##fulle',\n",
       " 'BIL',\n",
       " 'Mari',\n",
       " '##urg',\n",
       " '##unkten',\n",
       " 'action',\n",
       " '##abs',\n",
       " 'Lagercrantz',\n",
       " 'Turt',\n",
       " '##hild',\n",
       " 'Bellman',\n",
       " '##åster',\n",
       " '##inom',\n",
       " 'inrikt',\n",
       " '##96',\n",
       " '##sfören',\n",
       " '##bads',\n",
       " 'Storg',\n",
       " '##republik',\n",
       " 'illustrerade',\n",
       " '##rövar',\n",
       " '##vällen',\n",
       " 'sydväst',\n",
       " '##nik',\n",
       " 'romantisk',\n",
       " 'konser',\n",
       " '##lägret',\n",
       " '##rain',\n",
       " 'Petrus',\n",
       " 'Sophia',\n",
       " 'soffan',\n",
       " 'festa',\n",
       " '##gänget',\n",
       " 'Dog',\n",
       " '##hästarna',\n",
       " 'hort',\n",
       " 'ip',\n",
       " '##SF',\n",
       " 'Stack',\n",
       " 'Lee',\n",
       " '##HJ',\n",
       " '##senter',\n",
       " '##laine',\n",
       " 'ostad',\n",
       " '##BERGS',\n",
       " '##UF',\n",
       " '##emaskin',\n",
       " 'Eventuella',\n",
       " 'Pear',\n",
       " 'MC',\n",
       " 'Gené',\n",
       " '##benen',\n",
       " '##avdelningens',\n",
       " 'Typ',\n",
       " 'inneh',\n",
       " 'giftet',\n",
       " '##idag',\n",
       " '754',\n",
       " '##bety',\n",
       " 'Herceg',\n",
       " 'Was',\n",
       " '##denna',\n",
       " 'uttagningen',\n",
       " '##hållna',\n",
       " '##installationer',\n",
       " 'Loh',\n",
       " 'präg',\n",
       " '##hoppet',\n",
       " '##56',\n",
       " '##ight',\n",
       " 'ledningsm',\n",
       " '##keberg',\n",
       " 'blund',\n",
       " '##hood',\n",
       " '##annen',\n",
       " 'Sein',\n",
       " 'Hustrun',\n",
       " 'Gård',\n",
       " 'Sle',\n",
       " '##heden',\n",
       " 'förkyld',\n",
       " '##enet',\n",
       " '##svagnar',\n",
       " '##snack',\n",
       " '##smaskiner',\n",
       " '##END',\n",
       " '##weiz',\n",
       " 'Mind',\n",
       " '##svägarna',\n",
       " 'jurid',\n",
       " 'kora',\n",
       " 'Gården',\n",
       " '##raste',\n",
       " 'pas',\n",
       " 'Musta',\n",
       " '##splanet',\n",
       " 'Gid',\n",
       " 'tår',\n",
       " 'Lend',\n",
       " 'råttor',\n",
       " 'Bollen',\n",
       " 'Bend',\n",
       " 'härlig',\n",
       " '##skärmar',\n",
       " '##sext',\n",
       " 'Jö',\n",
       " 'klingar',\n",
       " 'Cram',\n",
       " '##card',\n",
       " 'las',\n",
       " 'Great',\n",
       " '##ated',\n",
       " 'ound',\n",
       " 'Western',\n",
       " 'samhällskrit',\n",
       " 'dol',\n",
       " '##57',\n",
       " '##scenter',\n",
       " '##ÄD',\n",
       " 'Torn',\n",
       " 'Järvsö',\n",
       " 'Sha',\n",
       " '##dos',\n",
       " 'VIL',\n",
       " 'spär',\n",
       " 'minn',\n",
       " '##skoncernen',\n",
       " 'grenarna',\n",
       " 'Chil',\n",
       " 'grin',\n",
       " 'framlidne',\n",
       " '##46',\n",
       " 'välfört',\n",
       " 'Sop',\n",
       " '##orama',\n",
       " '##£',\n",
       " '##rique',\n",
       " 'grotta',\n",
       " '##ael',\n",
       " 'Bolibompa',\n",
       " 'Fordringar',\n",
       " 'distansen',\n",
       " 'Mich',\n",
       " '##ovskij',\n",
       " '##blandningen',\n",
       " 'dist',\n",
       " 'varumärket',\n",
       " 'José',\n",
       " 'Kille',\n",
       " 'färgad',\n",
       " 'Stol',\n",
       " '##wal',\n",
       " 'strateg',\n",
       " '##IX',\n",
       " 'sopp',\n",
       " '##uger',\n",
       " '##brän',\n",
       " 'prover',\n",
       " '##historier',\n",
       " 'Pur',\n",
       " '##ffin',\n",
       " 'Församlingen',\n",
       " '##göt',\n",
       " '##Q',\n",
       " 'Bianca',\n",
       " '##68',\n",
       " 'Pic',\n",
       " 'Alto',\n",
       " 'Ups',\n",
       " 'slal',\n",
       " '##91',\n",
       " 'föred',\n",
       " 'Vardags',\n",
       " '##fattare',\n",
       " 'Anfall',\n",
       " '##etti',\n",
       " '##chin',\n",
       " 'prem',\n",
       " '##iday',\n",
       " 'Vax',\n",
       " '##ynda',\n",
       " 'litter',\n",
       " 'proport',\n",
       " 'hipp',\n",
       " '##49',\n",
       " 'Betydligt',\n",
       " 'dödens',\n",
       " 'MEN',\n",
       " '##usar',\n",
       " '740',\n",
       " 'Pel',\n",
       " '##omer',\n",
       " 'Stadsbyggnad',\n",
       " '##emit',\n",
       " 'hörna',\n",
       " '##anmälan',\n",
       " '##aled',\n",
       " '##elsystem',\n",
       " 'gult',\n",
       " 'förhören',\n",
       " 'bölja',\n",
       " 'Gammel',\n",
       " '##urar',\n",
       " 'Lägenhet',\n",
       " 'Kajsa',\n",
       " '##sutrymmen',\n",
       " '##ilding',\n",
       " 'inarbetad',\n",
       " 'spöken',\n",
       " '##ries',\n",
       " '##skrafter',\n",
       " 'Utsikt',\n",
       " 'Aber',\n",
       " '##manland',\n",
       " '##huvudstaden',\n",
       " '##andas',\n",
       " '##rael',\n",
       " '##beln',\n",
       " '##iné',\n",
       " '##skägg',\n",
       " 'orgeln',\n",
       " 'Känd',\n",
       " '##fruar',\n",
       " '##ilas',\n",
       " 'äm',\n",
       " 'tackat',\n",
       " '##bd',\n",
       " 'Mellby',\n",
       " 'Kabel',\n",
       " 'Fleming',\n",
       " '##seln',\n",
       " 'nyförvär',\n",
       " 'markt',\n",
       " 'ordn',\n",
       " '##teckningar',\n",
       " '##rekvensen',\n",
       " '##uper',\n",
       " '##ersätt',\n",
       " '##irl',\n",
       " 'Abram',\n",
       " 'Reich',\n",
       " '##snatur',\n",
       " 'LEJON',\n",
       " 'TINGS',\n",
       " 'Lju',\n",
       " 'Indust',\n",
       " '##iblioteken',\n",
       " '##ädje',\n",
       " 'Boko',\n",
       " 'lagerb',\n",
       " '##aljer',\n",
       " 'rätter',\n",
       " 'vim',\n",
       " 'folkdans',\n",
       " 'Force',\n",
       " '##jobbet',\n",
       " '##301',\n",
       " '##qu',\n",
       " 'Allen',\n",
       " '##årsju',\n",
       " 'gass',\n",
       " '##artid',\n",
       " '##unken',\n",
       " '##haninge',\n",
       " 'Cruz',\n",
       " '##reporter',\n",
       " 'mart',\n",
       " 'förmed',\n",
       " 'illamående',\n",
       " '##ydde',\n",
       " 'Vänner',\n",
       " '##122',\n",
       " '##götland',\n",
       " 'Vass',\n",
       " '##ötte',\n",
       " '##ELSE',\n",
       " '##abliss',\n",
       " 'sjöutsikt',\n",
       " '##isar',\n",
       " 'årtion',\n",
       " 'skåde',\n",
       " 'RAD',\n",
       " 'king',\n",
       " 'Tam',\n",
       " '##ón',\n",
       " '##chn',\n",
       " '##sgat',\n",
       " 'Nyår',\n",
       " 'fingert',\n",
       " '##alender',\n",
       " 'källar',\n",
       " '##bow',\n",
       " 'dalen',\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_tokens = list(set(decoded_hugging_correct)-set(decoded_base_correct))\n",
    "print(len(good_tokens))\n",
    "good_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3231899"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking(batch,i):\n",
    "    word_ids=batch[\"word_ids\"]\n",
    " \n",
    "    masked_input_id=batch[\"input_ids\"].copy()\n",
    "    attention_mask=batch[\"attention_mask\"].copy()\n",
    " \n",
    "    labels=[[-100]*max_length]*len(batch[\"labels\"])\n",
    "    for z in range(len(masked_input_id)):\n",
    "        if batch[\"input_ids\"][z][i] ==tokenizer.pad_token_id or batch[\"input_ids\"][z][i] ==tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z][i]=batch[\"input_ids\"][z][i]\n",
    "        masked_input_id[z][i]=tokenizer.mask_token_id\n",
    "  \n",
    "        \n",
    "        word=tokenizer.decode(batch[\"input_ids\"][z][i])\n",
    "   \n",
    "        future_token=[j for j,_ in enumerate(word_ids[z]) if word_ids[z][j]==word_ids[z][i] and j>i]\n",
    "\n",
    "        for j in future_token:\n",
    "            labels[z][j]=batch[\"input_ids\"][z][j]\n",
    "    \n",
    "            masked_input_id[z][j]=tokenizer.mask_token_id\n",
    "           \n",
    "\n",
    "        masked_input_id[z]=np.array(masked_input_id[z])\n",
    "        attention_mask[z]=np.array(attention_mask[z])\n",
    "        labels[z]=np.array(labels[z])\n",
    "   \n",
    "    output_dict = {\"input_ids\": masked_input_id, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    return {k: v for k, v in output_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking_bis(batch, i):\n",
    "    word_ids = batch[\"word_ids\"]\n",
    "    masked_input_id = batch[\"input_ids\"].copy()\n",
    "    attention_mask = batch[\"attention_mask\"].copy()\n",
    "    \n",
    "    labels = np.full_like(masked_input_id, -100)\n",
    "    \n",
    "    for z, seq in enumerate(masked_input_id):\n",
    "        if seq[i] == tokenizer.pad_token_id or seq[i] == tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z, i] = seq[i]\n",
    "        masked_input_id[z][i] = tokenizer.mask_token_id\n",
    "        future_token = [j for j, _ in enumerate(word_ids[z]) if word_ids[z][j] == word_ids[z][i] and j > i]\n",
    "        \n",
    "        for j in future_token:\n",
    "            labels[z][j] = batch[\"input_ids\"][z][j]\n",
    "            masked_input_id[z][j] = tokenizer.mask_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": masked_input_id,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pll = 0\n",
    "batch_size=64\n",
    "for i in  range(max_length):\n",
    "    print(i)\n",
    "    losses=[]\n",
    "    eval_dataset_log = lm_datasets[\"test\"].map(\n",
    "        lambda examples: insert_special_masking_bis(examples,i),\n",
    "        batched=True,\n",
    "        remove_columns= lm_datasets[\"test\"].column_names\n",
    "    )\n",
    "    print(\"daatset\")\n",
    "    eval_dataloader = preprocessing.create_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
    "    print(\"dataloader\")\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch={key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output=model_hugging_face(**batch)\n",
    "        print(\"output\")\n",
    "        loss=output.loss\n",
    "        losses.append(loss.repeat(eval_dataloader.batch_size))\n",
    "        print(\"loss\")\n",
    "        break\n",
    "    losses = torch.cat(losses)\n",
    "    print(\"loss\")\n",
    "    #losses = losses[: len(eval_dataloader.dataset)]\n",
    "    pll += torch.mean(losses)\n",
    "\n",
    "\n",
    "pll /=max_length\n",
    "pll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_task(model_kb,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for ix, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['content'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    print(df[\"tag\"])\n",
    "    labels = torch.tensor(df['tag'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Create binary label where seg = 1\n",
    "df = df[df[\"content\"].notnull()]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "input_ids, attention_masks, labels = encode(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_long.config.name_or_path}-imdb\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    # Add other training arguments as needed\n",
    "    logging_steps=892,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True\n",
    ")\n",
    "print(training_args.device)\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model_long,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(result['eval_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
