{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from datasets import Dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import comprehension_model\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import pearsonr,spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_tokens(valid_dataset, top_n=1000):\n",
    "    token_counts = defaultdict(int)\n",
    "\n",
    "    for example in valid_dataset: \n",
    "        labels = example['labels']\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                token_counts[label] += 1\n",
    "    \n",
    "    \n",
    "    return token_counts\n",
    "\n",
    "# Example usage\n",
    "# # Assuming valid_dataset and tokenizer are already loaded\n",
    "# most_frequent_tokens = get_most_frequent_tokens(valid_dataset)\n",
    "\n",
    "def special_token(token,example):\n",
    "    return token in example['labels']\n",
    "\n",
    "def filter_and_process_dataset(valid_dataset, valid_token_list, tokenizer, preprocessing, max_examples=10000, max_filtered=1000):\n",
    "    filtered_datasets = {}\n",
    "\n",
    "    for token_id in most_frequent_tokens:\n",
    "        # Filter the dataset for the current token\n",
    "        valid_filtered_dataset = valid_dataset.select(range(max_examples)).filter(lambda example: special_token(token_id, example))\n",
    "        \n",
    "        # Skip if the filtered dataset is empty\n",
    "        if len(valid_filtered_dataset) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Process the filtered dataset with the context mask\n",
    "        valid_sentence_filtered = valid_filtered_dataset.map(lambda example: preprocessing.get_context_with_mask(example, token_id, tokenizer))\n",
    "        \n",
    "        # Limit to max_filtered examples for efficiency\n",
    "        limited_dataset = valid_sentence_filtered.select(range(min(len(valid_sentence_filtered), max_filtered)))\n",
    "        \n",
    "        filtered_datasets[token_id] =limited_dataset\n",
    "        print(f\"Processed token ID: {token_id} with {len(limited_dataset)} examples\")\n",
    "\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "# filtered_datasets = filter_and_process_dataset(valid_dataset,valid_tokens_list,tokenizer,preprocessing)\n",
    "# with open(\"word_embedding_distance_layer12.csv\",\"w\") as file :\n",
    "#     writer=csv.writer(file)\n",
    "#     writer.writerow([\"indices\", \"token\",\"euclidean_distances\",\"cosine similarity\" ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_norms(model):\n",
    "    embeddings = model.bert.embeddings.word_embeddings.weight.detach()\n",
    "    embeddings_norm =model.bert.embeddings.LayerNorm(embeddings)\n",
    "    norms = torch.norm(embeddings_norm, dim=1)\n",
    "    return norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_normalized(model):\n",
    "    embeddings = model.bert.embeddings.word_embeddings.weight.detach()\n",
    "    embeddings_norm =model.bert.embeddings.LayerNorm(embeddings)\n",
    "    return embeddings_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_token_ids(dataset):\n",
    "    unique_token_ids = set()\n",
    "    for example in dataset:\n",
    "        unique_token_ids.update(example['input_ids'])\n",
    "    return torch.tensor(sorted(unique_token_ids))\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_token(token,example):\n",
    "    return token in example['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kb = preprocessing.create_model_MLM(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exbert_tokenizer=AutoTokenizer.from_pretrained(\"exbert_tokenizer\")\n",
    "model_exbert = AutoModelForMaskedLM.from_pretrained(\"exbert-finetuned-imdb/checkpoint-5327520\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "swerick_tokenizer= PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/home/laurinemeier/swerick/pretraining_scratch/checkpoint-5258900/tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\")\n",
    "    \n",
    "    \n",
    "config = transformers.BertConfig.from_pretrained(\"/home/laurinemeier/swerick/pretraining_scratch/checkpoint-5258900\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"/home/laurinemeier/swerick/pretraining_scratch/checkpoint-5258900\",config=config,trust_remote_code=True)\n",
    "mosaicBert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "small_valid_dataset = preprocessing.create_deterministic_eval_dataset(valid_dataset.select(range(10000)),data_collator)\n",
    "small_valid_dataloader=preprocessing.create_dataloader(small_valid_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Statsrådet\"\n",
    "token_id = tokenizer.convert_tokens_to_ids(word)\n",
    "valid_filtered_dataset = valid_dataset.select(range(100000)).filter(lambda example : special_token(token_id,example))\n",
    "valid_filtered_dataloader=preprocessing.create_dataloader(valid_filtered_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "import numpy as np\n",
    "\n",
    "gamma_1 = model_kb.bert.encoder.layer[8].intermediate.dense.weight.detach().cpu().numpy()\n",
    "index=4\n",
    "\n",
    "input_ids = torch.tensor(valid_dataset[-10][\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "attention_mask = torch.tensor(valid_dataset[-10][\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "labels = torch.tensor(valid_dataset[-10][\"labels\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# Obtenez les sorties des modèles\n",
    "output = model_kb(input_ids=input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "hidden_state = output.hidden_states[0].detach()\n",
    "#hidden_state = hidden_states[index].detach()\n",
    "Attention = model_kb.bert.encoder.layer[3].attention\n",
    "First_layer = model_kb.bert.encoder.layer[3].intermediate.dense\n",
    "X = Attention(hidden_state)[0]\n",
    "X_first_layer = First_layer(X)\n",
    "GELU_derivative = 0.5 * (1 + torch.tanh(np.sqrt(2 / torch.pi) * (X_first_layer + 0.044715 * X_first_layer**3)) + X_first_layer* (1-torch.tanh(np.sqrt(2 / torch.pi) * (X_first_layer + 0.044715 * X_first_layer**3))**2) * (np.sqrt(2 / torch.pi) * (1 + 0.044715 * X_first_layer**2)))\n",
    "\n",
    "print (GELU_derivative.size())                                                                                                                                                                                           \n",
    "GELU_derivative = GELU_derivative.reshape(128,-1).detach().cpu().numpy()\n",
    "print (GELU_derivative)  \n",
    "#Q = model_kb.bert.encoder.layer[index].attention.self.query(hidden_state)\n",
    "#K = model_kb.bert.encoder.layer[index].attention.self.key(hidden_state)\n",
    "d=769\n",
    "#scores = torch.matmul(Q,K.transpose(-2,-1))/np.sqrt(d)\n",
    "\n",
    "# jacobians = []\n",
    "\n",
    "# # Calculer la jacobienne pour chaque ligne\n",
    "# for i in range(scores.size(1)):\n",
    "#     s = scores[0,i]\n",
    "#     jacobian =jacobian_softmax(s)\n",
    "#     jacobians.append(jacobian)\n",
    "        \n",
    "\n",
    "norm_2 = np.linalg.norm(GELU_derivative, 2)\n",
    "print(norm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent words  in training corpus\n",
    "\n",
    "most_frequent_tokens = get_most_frequent_tokens(lm_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sim = {}\n",
    "\n",
    "with open('cosine_similarity_words_emebdding_cpt.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            key, value = line.split(': ')\n",
    "            token_sim[key]=float(value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token frequency in the training corpus\n",
    "# \n",
    "token_frequency_dict = {}\n",
    "with open(\"token_frequency.txt\",'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[0]==':':\n",
    "                token,frequency = ':',line[3:]\n",
    "            else :\n",
    "                token, frequency = line.split(':')\n",
    "                \n",
    "            token = token.strip()\n",
    "            frequency = int(frequency.strip())\n",
    "            token_frequency_dict[token] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correaltion between frequency and cosine similarity in word embedding \n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "#frequency_dict = {word: count for word, count in most_frequent_words}\n",
    "ids_in_corpus=[tokenizer.convert_tokens_to_ids(token) for token in list(token_frequency_dict.keys())]\n",
    "print(ids_in_corpus)\n",
    "token_norm = [exbert_norms[index] for index in ids_in_corpus]\n",
    "\n",
    "\n",
    "pearson_corr, _ = pearsonr(list(token_frequency_dict.values()),token_norm)\n",
    "print(f'Corrélation de Pearson : {pearson_corr}')\n",
    "\n",
    "# Calculer la corrélation de Spearman\n",
    "spearman_corr, _ = spearmanr(list(token_frequency_dict.values()),token_norm)\n",
    "print(f'Corrélation de Spearman : {spearman_corr}')\n",
    "\n",
    "# Combine frequencies and cosine similarities\n",
    "frequency_similarity_pairs = [(freq, norm) for freq, norm in zip(list(token_frequency_dict.values()),token_norm)]\n",
    "freqs, sims = zip(*frequency_similarity_pairs)\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr, p_value = pearsonr(freqs, sims)\n",
    "print(f\"Pearson correlation: {pearson_corr}, P-value: {p_value}\")\n",
    "spearman_corr, p_value = spearmanr(freqs, sims)\n",
    "print(f\"Spearman correlation: {spearman_corr}, P-value: {p_value}\")\n",
    "\n",
    "plt.scatter(sims,freqs, alpha=0.5)\n",
    "plt.title('Token Frequency vs. Norm Embedding')\n",
    "plt.xlabel('Norm embedding')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(frequency_similarity_pairs, columns=['Frequency', 'norm_embedding'])\n",
    "\n",
    "# Bin the frequencies\n",
    "df['Frequency Bin'] = pd.qcut(df['norm_embedding'], q=10, duplicates='drop')\n",
    "\n",
    "# Plot box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.boxplot(column='Frequency', by='Frequency Bin', grid=False, showfliers=False)\n",
    "plt.title(' Frequency Bin by Norm Embedding')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Norm Bin')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frequency_last_100 = [frequency_dict[word] for word in token_sim[-100:]]\n",
    "print(\"mean frequency last 100\",np.mean(frequency_last_100))\n",
    "\n",
    "frequency_first_100 = [frequency_dict[word] for word in token_sim[:100]]\n",
    "print(\"mean frequency first 100\",np.mean(frequency_first_100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(frequency_last_100, frequency_first_100, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in frequencies is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in frequencies is not statistically significant.\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(frequency_last_100, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Frequency Distribution - Last 100 Tokens')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(frequency_first_100, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title('Frequency Distribution - First 100 Tokens')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([frequency_last_100, frequency_first_100], labels=['Last 100 Tokens', 'First 100 Tokens'])\n",
    "plt.title('Frequency Box Plot')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def print_statistics(data, label):\n",
    "    print(f\"Statistics for {label}:\")\n",
    "    print(f\"  Mean: {np.mean(data)}\")\n",
    "    print(f\"  Median: {np.median(data)}\")\n",
    "    print(f\"  Mode: {max(set(data), key=data.count)}\")\n",
    "    print(f\"  Standard Deviation: {np.std(data)}\")\n",
    "    print(f\"  Min: {np.min(data)}\")\n",
    "    print(f\"  Max: {np.max(data)}\")\n",
    "    print()\n",
    "\n",
    "print_statistics(frequency_last_100, \"Last 100 Tokens\")\n",
    "print_statistics(frequency_first_100, \"First 100 Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "euclidean_measures =[]\n",
    "cosine_measures = []\n",
    "tokens=[]\n",
    "for token in filtered_datasets.keys() :\n",
    "    valid_sentence_filtered = filtered_datasets[token]\n",
    "    dataloader  =preprocessing.create_dataloader(valid_sentence_filtered,1,default_data_collator)\n",
    "    baseline_embeddings = comprehension_model.get_embeddings(model_kb, dataloader, tokenizer)\n",
    "    finetuned_embeddings = comprehension_model.get_embeddings(model_hugging_face, dataloader, tokenizer)\n",
    "    euclidean_distances = [euclidean(baseline, finetuned) for baseline, finetuned in zip(baseline_embeddings[-1], finetuned_embeddings[-1])]\n",
    "    euclidean_measures.append(np.mean(euclidean_distances))\n",
    "    cosine_measure = cosine_similarity(baseline_embeddings[-1], finetuned_embeddings[-1])\n",
    "    avg_cosine = np.mean(cosine_measure)\n",
    "    cosine_measures.append(avg_cosine)\n",
    "    tokens.append(token)\n",
    "    with open(\"word_embedding_distance_layer12.csv\", \"a\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i, token,np.mean(euclidean_distances),avg_cosine ])\n",
    "    print(tokenizer.decode(token))\n",
    "    \n",
    "high_separated_indices12 = sorted(range(len(euclidean_measures)), key=lambda i: euclidean_measures[i], reverse=True)[:50]\n",
    "least_separated_indices12 = sorted(range(len(euclidean_measures)), key=lambda i: euclidean_measures[i])[:50]\n",
    "high_separated_values12 = [euclidean_measures[i] for i in high_separated_indices12]\n",
    "least_separated_values12 = [euclidean_measures[i] for i in least_separated_indices12]\n",
    "print(high_separated_values12)\n",
    "print(least_separated_values12)\n",
    "high_separated_tokens12 = [tokens[i] for i in high_separated_indices12]\n",
    "least_separated_tokens12 = [tokens[i] for i in least_separated_indices12]\n",
    "high_separated_words12 = [tokenizer.decode([idx]) for idx in high_separated_tokens12]\n",
    "least_separated_words12 = [tokenizer.decode([idx]) for idx in least_separated_tokens12]\n",
    "print(high_separated_words12)\n",
    "print(least_separated_words12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "cos_similarities = cosine_similarity(baseline_embeddings[0], finetuned_embeddings[0])\n",
    "euclidean_distances = [euclidean(baseline, finetuned) for baseline, finetuned in zip(baseline_embeddings[0], finetuned_embeddings[0])]\n",
    "\n",
    "print(\"Average Cosine Similarity:\", np.mean(cos_similarities))\n",
    "print(\"Average Euclidean Distance:\", np.mean(euclidean_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(model):\n",
    "    parameters = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        parameters[name] = param.detach().cpu().numpy()\n",
    "    return parameters\n",
    "\n",
    "params_model_1 = extract_parameters(model_kb)\n",
    "params_model_2 = extract_parameters(model_hugging_face)\n",
    "multi_E_ft = model_hugging_face.state_dict()['bert.embeddings.word_embeddings.weight'].T\n",
    "print(multi_E_ft.shape)\n",
    "\n",
    "\n",
    "displacement_vectors = {}\n",
    "for name in params_model_1:\n",
    "    displacement_vectors[name] = (params_model_1[name] - params_model_2[name])\n",
    "    \n",
    "name=f\"bert.encoder.layer.{11}.intermediate.dense.weight\"  \n",
    "print(params_model_1[name].shape)\n",
    "vec = np.dot(displacement_vectors[name],multi_E_ft.cpu().numpy())\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(vec)[-top_k:]\n",
    "print(top_k_indices)\n",
    "top_k_words = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
    "\n",
    "# Print the top k words\n",
    "print(\"Top k words with the largest embedding changes:\")\n",
    "for word in top_k_words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study of norm of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_norms = get_embedding_norms(model_kb)\n",
    "finetuned_norms = get_embedding_norms(model_hugging_face)\n",
    "exbert_norms = get_embedding_norms(model_exbert)\n",
    "#spa_norms =get_embedding_norms(mosaicBert).tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(range(len(baseline_norms.tolist())))\n",
    "tokens_exbert = exbert_tokenizer.convert_ids_to_tokens(range(len(exbert_norms.tolist())))\n",
    "#tokens_spa = swerick_tokenizer.convert_ids_to_tokens(range(len(spa_norms)))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(np.arange(len(baseline_norms.tolist())),baseline_norms.tolist(),label='Baseline Model')\n",
    "plt.scatter(np.arange(len(finetuned_norms.tolist())),finetuned_norms.tolist(), label='spa Model')\n",
    "plt.xlabel('Token Index')\n",
    "plt.ylabel('Norm of Embedding')\n",
    "plt.title('Embedding Norms for Each Token')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T statictis comparaison of norms\n",
    "\n",
    "t_stat, p_value = ttest_ind(baseline_norms.tolist(), finetuned_norms.tolist())\n",
    "\n",
    "# Print the t-statistic and p-value\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(baseline_norms.tolist(), bins=30, alpha=0.5, label='Model KB')\n",
    "plt.hist(finetuned_norms.tolist(), bins=30, alpha=0.5, label='Model spaBERT')\n",
    "plt.title('Distribution of Embedding Norms for KB bert and SparlBERT')\n",
    "plt.xlabel('Norm')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_changes = baseline_norms  - finetuned_norms\n",
    "values,topk=torch.topk(finetuned_norms,100)\n",
    "\n",
    "token_norm=[tokenizer.convert_ids_to_tokens(token.item()) for token in topk]\n",
    "token_changes = list(zip(tokens, embedding_changes))\n",
    "sorted_token_changes = sorted(token_changes, key=lambda x: x[1])\n",
    "sorted_token=[token for token,change in sorted_token_changes]\n",
    "positive_norm = [token for token,change in sorted_token_changes if change >0]\n",
    "negative_norm = [token for token,change in sorted_token_changes if change <0]\n",
    "print(negative_norm[:100])\n",
    "print(positive_norm[-100 :])\n",
    "print(token_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_token_norms_cpt = sorted(zip(tokens,token_norm), key=lambda x: x[1])\n",
    "sorted_token_cpt=[token for token, norm in sorted_token_norms_cpt]\n",
    "print(sorted_token_cpt[-100:])\n",
    "print(sorted_token_cpt[:100])\n",
    "\n",
    "sorted_token_norms_kb = sorted(zip(tokens,baseline_norms), key=lambda x: x[1])\n",
    "sorted_token_kb=[token for token, norm in sorted_token_norms_kb]\n",
    "print(sorted_token_kb[-100:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation Frequence - Norm\n",
    "\n",
    "frequencies = token_frequency_dict.values()\n",
    "norms = [finetuned_norms[tokenizer.convert_tokens_to_ids(token)].detach().cpu().numpy() for token in list(token_frequency_dict.keys())]\n",
    "correlation, p_value = spearmanr(list(frequencies), norms)\n",
    "\n",
    "print(f'Correlation between token frequencies and embedding norms: {correlation}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Tracer un graphique de dispersion\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(frequencies, norms, alpha=0.5)\n",
    "plt.title('Scatter plot of Token Frequencies vs Embedding Norms')\n",
    "plt.xlabel('Token Frequency')\n",
    "plt.ylabel('Embedding Norm')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study correlation between Cosine similarity - fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ref_weight =get_embedding_normalized(model_hugging_face)\n",
    "bias_weight = get_embedding_normalized(model_kb)\n",
    "eb_weight = get_embedding_normalized(model_exbert)\n",
    "\n",
    "similarities_cpt=[]\n",
    "similarities_eb=[]\n",
    "for token in tokenizer.vocab.keys():\n",
    "    similarities_cpt.append(cosine_similarity(ref_weight[tokenizer.convert_tokens_to_ids(token)],bias_weight[tokenizer.convert_tokens_to_ids(token)]))\n",
    "    similarities_eb.append(cosine_similarity(eb_weight[exbert_tokenizer.convert_tokens_to_ids(token)],ref_weight[tokenizer.convert_tokens_to_ids(token)]))\n",
    "\n",
    "token_similarity_cpt = sorted(zip(tokenizer.vocab.keys(), similarities_cpt), key=lambda x: x[1], reverse=True)\n",
    "token_similarity_eb = sorted(zip(tokenizer.vocab.keys(), similarities_eb), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "classified_words_cpt = [token_id for token_id, similarity in token_similarity_cpt]\n",
    "similarities_cpt = [sim for token,sim in token_similarity_cpt]\n",
    "classified_words_eb = [token_id for token_id, similarity in token_similarity_eb]\n",
    "similarities_eb = [sim for token,sim in token_similarity_eb]\n",
    "def print_statistics(data, label):\n",
    "    print(f\"Statistics for {label}:\")\n",
    "    print(f\"  Mean: {np.mean(data)}\")\n",
    "    print(f\"  Median: {np.median(data)}\")\n",
    "    print(f\"  Mode: {max(set(data), key=data.count)}\")\n",
    "    print(f\"  Standard Deviation: {np.std(data)}\")\n",
    "    print(f\"  Min: {np.min(data)}\")\n",
    "    print(f\"  Max: {np.max(data)}\")\n",
    "    print()\n",
    "\n",
    "print_statistics(similarities_cpt, \"Cosine Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token with high or low cosine similairty \n",
    "\n",
    "#Low similarity \n",
    "print(classified_words_cpt[:200])\n",
    "#High similarity \n",
    "print(classified_words_cpt[-200:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence = \"Herr [MASK] von Ehrenheim : Anledningen till den framställning\"\n",
    "input_kb=tokenizer(sentence,return_tensors='pt').to(device)\n",
    "input_eb=exbert_tokenizer(sentence,return_tensors='pt').to(device)\n",
    "input_mb=swerick_tokenizer(sentence,return_tensors='pt').to(device)\n",
    "\n",
    "index  =2\n",
    "print(index)\n",
    "criterion =nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model_kb(**input_kb,\n",
    "        output_attentions=True\n",
    "    )\n",
    "    outputs2=model_exbert(**input_eb)\n",
    "    outputs3=model_hugging_face(**input_kb)\n",
    "    outputs4=mosaicBert(**input_mb)\n",
    "    \n",
    "softmax_probs_kb = F.softmax(outputs1.logits.squeeze()[index], dim=-1)\n",
    "sorted_probs_kb, sorted_indices_kb = torch.sort(softmax_probs_kb, descending=True)\n",
    "sorted_tokens_kb = [tokenizer.decode([idx]) for idx in sorted_indices_kb[:10]]\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_tokens_kb, sorted_probs_kb[:10].cpu().numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Top 10 Predicted Tokens and their Probabilities by KB bert')\n",
    "plt.show()\n",
    "\n",
    "softmax_probs_eb = F.softmax(outputs2.logits.squeeze()[index], dim=-1)\n",
    "sorted_probs_eb, sorted_indices_eb = torch.sort(softmax_probs_eb, descending=True)\n",
    "sorted_tokens_eb = [exbert_tokenizer.decode([idx]) for idx in sorted_indices_eb[:10]]\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_tokens_eb, sorted_probs_eb[:10].cpu().numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Top 10 Predicted Tokens and their Probabilities by sBERTex')\n",
    "plt.show()\n",
    "\n",
    "softmax_probs_cpt = F.softmax(outputs3.logits.squeeze()[index], dim=-1)\n",
    "sorted_probs_cpt, sorted_indices_cpt = torch.sort(softmax_probs_cpt, descending=True)\n",
    "sorted_tokens_cpt = [tokenizer.decode([idx]) for idx in sorted_indices_cpt[:10]]\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_tokens_cpt, sorted_probs_cpt[:10].cpu().numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Top 10 Predicted Tokens and their Probabilities by cptBERT')\n",
    "plt.show()\n",
    "\n",
    "softmax_probs_sp = F.softmax(outputs4.logits.squeeze()[index], dim=-1)\n",
    "sorted_probs_sp, sorted_indices_sp = torch.sort(softmax_probs_sp, descending=True)\n",
    "sorted_tokens_sp = [swerick_tokenizer.decode([idx]) for idx in sorted_indices_sp[:10]]\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_tokens_sp, sorted_probs_sp[:10].cpu().numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Top 10 Predicted Tokens and their Probabilities by SparBERT')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
