{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import comprehension_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kb = preprocessing.create_model_MLM(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "small_valid_dataset = preprocessing.create_deterministic_eval_dataset(valid_dataset.select(range(10000)),data_collator)\n",
    "small_valid_dataloader=preprocessing.create_dataloader(small_valid_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exbert = AutoModelForMaskedLM.from_pretrained(\"exbert-finetuned-imdb/checkpoint-1271340\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "config = transformers.BertConfig.from_pretrained(\"pretraining_from_scratch/checkpoint-3944175\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"pretraining_from_scratch/checkpoint-3944175\",config=config,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_tokens(valid_dataset, top_n=1000):\n",
    "    token_counts = Counter()\n",
    "    \n",
    "    # Count the frequency of each token in the labels\n",
    "    for example in valid_dataset:\n",
    "        labels = example['labels']\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                token_counts[label] += 1\n",
    "    \n",
    "    # Get the most common tokens\n",
    "    most_common_tokens = token_counts.most_common(top_n)\n",
    "    most_frequent_tokens = [token for token, count in most_common_tokens]\n",
    "    \n",
    "    return most_frequent_tokens\n",
    "\n",
    "# Example usage\n",
    "# Assuming valid_dataset and tokenizer are already loaded\n",
    "most_frequent_tokens = get_most_frequent_tokens(valid_dataset)\n",
    "\n",
    "\n",
    "def filter_and_process_dataset(valid_dataset, valid_token_list, tokenizer, preprocessing, max_examples=10000, max_filtered=1000):\n",
    "    filtered_datasets = {}\n",
    "\n",
    "    for token_id in most_frequent_tokens:\n",
    "        # Filter the dataset for the current token\n",
    "        valid_filtered_dataset = valid_dataset.select(range(max_examples)).filter(lambda example: special_token(token_id, example))\n",
    "        \n",
    "        # Skip if the filtered dataset is empty\n",
    "        if len(valid_filtered_dataset) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Process the filtered dataset with the context mask\n",
    "        valid_sentence_filtered = valid_filtered_dataset.map(lambda example: preprocessing.get_context_with_mask(example, token_id, tokenizer))\n",
    "        \n",
    "        # Limit to max_filtered examples for efficiency\n",
    "        limited_dataset = valid_sentence_filtered.select(range(min(len(valid_sentence_filtered), max_filtered)))\n",
    "        \n",
    "        filtered_datasets[token_id] =limited_dataset\n",
    "        print(f\"Processed token ID: {token_id} with {len(limited_dataset)} examples\")\n",
    "\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "filtered_datasets = filter_and_process_dataset(valid_dataset,valid_tokens_list,tokenizer,preprocessing)\n",
    "with open(\"word_embedding_distance_layer12.csv\",\"w\") as file :\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow([\"indices\", \"token\",\"euclidean_distances\",\"cosine similarity\" ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "euclidean_measures =[]\n",
    "cosine_measures = []\n",
    "tokens=[]\n",
    "for token in filtered_datasets.keys() :\n",
    "    valid_sentence_filtered = filtered_datasets[token]\n",
    "    dataloader  =preprocessing.create_dataloader(valid_sentence_filtered,1,default_data_collator)\n",
    "    baseline_embeddings = comprehension_model.get_embeddings(model_kb, dataloader, tokenizer)\n",
    "    finetuned_embeddings = comprehension_model.get_embeddings(model_hugging_face, dataloader, tokenizer)\n",
    "    euclidean_distances = [euclidean(baseline, finetuned) for baseline, finetuned in zip(baseline_embeddings[-1], finetuned_embeddings[-1])]\n",
    "    euclidean_measures.append(np.mean(euclidean_distances))\n",
    "    cosine_measure = cosine_similarity(baseline_embeddings[-1], finetuned_embeddings[-1])\n",
    "    avg_cosine = np.mean(cosine_measure)\n",
    "    cosine_measures.append(avg_cosine)\n",
    "    tokens.append(token)\n",
    "    with open(\"word_embedding_distance_layer12.csv\", \"a\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i, token,np.mean(euclidean_distances),avg_cosine ])\n",
    "    print(tokenizer.decode(token))\n",
    "    \n",
    "high_separated_indices12 = sorted(range(len(euclidean_measures)), key=lambda i: euclidean_measures[i], reverse=True)[:50]\n",
    "least_separated_indices12 = sorted(range(len(euclidean_measures)), key=lambda i: euclidean_measures[i])[:50]\n",
    "high_separated_values12 = [euclidean_measures[i] for i in high_separated_indices12]\n",
    "least_separated_values12 = [euclidean_measures[i] for i in least_separated_indices12]\n",
    "print(high_separated_values12)\n",
    "print(least_separated_values12)\n",
    "high_separated_tokens12 = [tokens[i] for i in high_separated_indices12]\n",
    "least_separated_tokens12 = [tokens[i] for i in least_separated_indices12]\n",
    "high_separated_words12 = [tokenizer.decode([idx]) for idx in high_separated_tokens12]\n",
    "least_separated_words12 = [tokenizer.decode([idx]) for idx in least_separated_tokens12]\n",
    "print(high_separated_words12)\n",
    "print(least_separated_words12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "cos_similarities = cosine_similarity(baseline_embeddings[0], finetuned_embeddings[0])\n",
    "euclidean_distances = [euclidean(baseline, finetuned) for baseline, finetuned in zip(baseline_embeddings[0], finetuned_embeddings[0])]\n",
    "\n",
    "print(\"Average Cosine Similarity:\", np.mean(cos_similarities))\n",
    "print(\"Average Euclidean Distance:\", np.mean(euclidean_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the kernelized correlation coefficient function\n",
    "def kernelized_corr_coef(x, y, K1, K2, K12, emb_mean1, emb_mean2, n):\n",
    "    mu1, mu2 = x @ emb_mean1, y @ emb_mean2\n",
    "    print(mu1.shape)\n",
    "    print(mu2.shape)\n",
    "    mu12 = torch.outer(mu1, mu2)\n",
    "    x_norm, y_norm = map(lambda x, K, mu: (1/n * (x @ K) * x).sum(dim=-1) - mu**2, [x, y], [K1, K2], [mu1, mu2])\n",
    "    return (1 / n * x @ K12 @ y.T - mu12) / torch.sqrt(torch.outer(x_norm, y_norm))\n",
    "\n",
    "\n",
    "# Collect parameters from models\n",
    "num_layers = 12\n",
    "\n",
    "multi_WQ_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.self.query.weight\"].T for j in range(num_layers)])\n",
    "multi_WV_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.self.value.weight\"].T for j in range(num_layers)])    \n",
    "multi_WK_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.self.key.weight\"].T for j in range(num_layers)])\n",
    "multi_K_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.intermediate.dense.weight\"].T for j in range(num_layers)])\n",
    "multi_V_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.output.dense.weight\"] for j in range(num_layers)])\n",
    "multi_WO_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.output.dense.weight\"] for j in range(num_layers)])\n",
    "multi_E_kb = model_kb.state_dict()['bert.embeddings.word_embeddings.weight'].T\n",
    "\n",
    "multi_WQ_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.self.query.weight\"].T for j in range(num_layers)])\n",
    "multi_WV_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.self.value.weight\"].T for j in range(num_layers)])    \n",
    "multi_WK_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.self.key.weight\"].T for j in range(num_layers)])\n",
    "multi_K_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.intermediate.dense.weight\"].T for j in range(num_layers)])\n",
    "multi_V_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.output.dense.weight\"] for j in range(num_layers)])\n",
    "multi_WO_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.output.dense.weight\"] for j in range(num_layers)])\n",
    "multi_E_ft = model_hugging_face.state_dict()['bert.embeddings.word_embeddings.weight'].T\n",
    "\n",
    "# Ensure correct kernel matrices calculation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kernel11 = (multi_E_kb @ multi_E_kb.T).to(device)\n",
    "kernel22 = (multi_E_ft @ multi_E_ft.T).to(device)\n",
    "kernel12 = (multi_E_kb @ multi_E_ft.T).to(device)\n",
    "\n",
    "emb_mean1 = torch.mean(multi_E_kb, dim=1).to(device)\n",
    "emb_mean2 = torch.mean(multi_E_ft, dim=1).to(device)\n",
    "\n",
    "param1, param2 = multi_V_kb, multi_V_ft\n",
    "\n",
    "# Calculate the kernelized correlation coefficient\n",
    "S = kernelized_corr_coef(param1.to(device), param2.to(device), kernel11, kernel22, kernel12, emb_mean1, emb_mean2, n=len(tokenizer))\n",
    "print(S)\n",
    "layer_size = param1.shape[0] // num_layers\n",
    "\n",
    "S_agg = S.view(num_layers, layer_size, num_layers, layer_size).abs().mean([-1, -3]).cpu().numpy()\n",
    "print(S_agg.shape)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(S_agg)\n",
    "plt.title('Pearson Correlation between feed forward output layers of finetuned model and baseline model ')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token present in the training dataset\n",
    "\n",
    "token_frequencies = Counter()\n",
    "\n",
    "\n",
    "for example in lm_datasets[\"train\"]:\n",
    "    token_frequencies.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent token\n",
    "\n",
    "n = int(len(token_frequencies.keys())*0.8) \n",
    "most_common_tokens = token_frequencies.most_common(n)\n",
    "most_common_ids, most_common_freqs = zip(*most_common_tokens)\n",
    "most_common_ids_tensor = torch.tensor(most_common_ids)\n",
    "\n",
    "print(\"Les IDs des tokens les plus fréquents et leurs fréquences :\")\n",
    "for token_id, freq in zip(most_common_ids, most_common_freqs):\n",
    "    print(f\"Token ID: {token_id}, Fréquence: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehension_model.change_embedding_word(model_kb,model_hugging_face,valid_dataset,tokenizer,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph represent the most changed word embedding thorugh epochs\n",
    "\n",
    "checkpoint_directory = 'finetuning/finetuning_hugging_whitespace-finetuned-imdb'\n",
    "checkpoint_files = os.listdir(checkpoint_directory)\n",
    "\n",
    "checkpoint_files.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)))\n",
    "epoch_data = {}\n",
    "word_color = {}\n",
    "total_frequencies = {}\n",
    "initial_embeddings = model_kb.bert.embeddings.word_embeddings.weight.detach()[most_common_ids_tensor]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 5))\n",
    "# Training loop\n",
    "epoch=4\n",
    "for checkpoint in checkpoint_files:\n",
    "    epoch +=1\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"finetuning/finetuning_hugging_whitespace-finetuned-imdb/\" + checkpoint)\n",
    "    model=model.to(device)\n",
    "    #initial_embeddings = embeddings\n",
    "    embeddings = model.bert.embeddings.word_embeddings.weight.detach()[most_common_ids_tensor]\n",
    "\n",
    "    embedding_changes = torch.norm(embeddings - initial_embeddings, dim=1)\n",
    "    \n",
    "    # Get the top 5 changes\n",
    "    _, top_indices = torch.topk(embedding_changes, 1)\n",
    "    top_words = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in top_indices]\n",
    "    top_changes = embedding_changes[top_indices].tolist()\n",
    "    for word in top_words:\n",
    "            if word not in word_color:\n",
    "                word_color[word] = colors[len(word_color) % len(colors)]\n",
    "            if word not in total_frequencies :\n",
    "                total_frequencies[word] =1\n",
    "            if word  in total_frequencies :\n",
    "                total_frequencies[word] +=1\n",
    "        \n",
    "    # Store data\n",
    "    epoch_data[epoch] = list(zip(top_words, top_changes))\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "already_labeled = set()\n",
    "for epoch in epoch_data:\n",
    "    words, changes = zip(*epoch_data[epoch])\n",
    "    if words[0] not in already_labeled:\n",
    "        ax.plot([epoch] * 1, changes, marker='o', linestyle='', markersize=10,color=word_color[words[0]],label=words[0])\n",
    "        already_labeled.add(words[0])\n",
    "    else : \n",
    "        ax.plot([epoch] * 1, changes, marker='o', linestyle='', markersize=10,color=word_color[words[0]])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Change in Norm of Embeddings')\n",
    "ax.set_title('Most changed word embedding by Epoch')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "#Saliency scores\n",
    "\n",
    "\n",
    "\n",
    "model_hugging_face.eval()\n",
    "\n",
    "# Prepare the text and encode it\n",
    "text = \"skrift om ersättning för resekostnad till och från [MASK] samt ar\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_mask = encoded_input['attention_mask'].to(device)\n",
    "embeddings = model_hugging_face.bert.embeddings(input_ids)\n",
    "embeddings.retain_grad()\n",
    "output = model_hugging_face(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "logits = output.logits\n",
    "\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# Extract the logit corresponding to the masked token\n",
    "masked_token_logit = logits[0, mask_token_index, :]\n",
    "\n",
    "# Choose the target logit (e.g., the highest probability logit)\n",
    "target_logit = masked_token_logit.max()\n",
    "\n",
    "# Backward pass to compute the gradients\n",
    "model_hugging_face.zero_grad()\n",
    "target_logit.backward()\n",
    "# Get the input embeddings\n",
    "grads = embeddings.grad\n",
    "print(grads)\n",
    "# Compute the saliency map as the L2 norm of the gradients\n",
    "saliency = torch.norm(grads, dim=-1).squeeze()\n",
    "\n",
    "# Detach and move the saliency map to the CPU\n",
    "saliency = saliency.detach().cpu().numpy()\n",
    "\n",
    "# Tokenize the input text to match the saliency scores with the tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())\n",
    "\n",
    "# Display the tokens and their saliency scores\n",
    "for token, score in zip(tokens, saliency):\n",
    "    print(f\"Token: {token}, Saliency: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "inputs = valid_filtered_dataset[0]\n",
    "index = inputs[\"labels\"].index(token_id)\n",
    "print(index)\n",
    "criterion =nn.CrossEntropyLoss(reduction='none')\n",
    "input_ids = torch.tensor(inputs['input_ids'][index-9:index+3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "attention_mask = torch.tensor(inputs[\"attention_mask\"][index-9:index+3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "labels = torch.tensor(inputs[\"labels\"][index-9:index+3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "print(tokenizer.decode(inputs['input_ids'][index-9:index+3]))\n",
    "print(labels)\n",
    "with torch.no_grad():\n",
    "    outputs = model_hugging_face(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "        output_attentions=True\n",
    "    )\n",
    "softmax_probs = F.softmax(outputs.logits.squeeze()[-3], dim=-1)\n",
    "sorted_probs, sorted_indices = torch.sort(softmax_probs, descending=True)\n",
    "sorted_tokens = [tokenizer.decode([idx]) for idx in sorted_indices[:10]]\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_tokens, sorted_probs[:10].cpu().numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Top 10 Predicted Tokens and their Probabilities')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
