{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import comprehension_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kb = preprocessing.create_model_MLM(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset,data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\"])\n",
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)\n",
    "small_valid_dataset = preprocessing.create_deterministic_eval_dataset(valid_dataset.select(range(10000)),data_collator)\n",
    "small_valid_dataloader=preprocessing.create_dataloader(small_valid_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exbert = AutoModelForMaskedLM.from_pretrained(\"exbert-finetuned-imdb/checkpoint-1271340\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "config = transformers.BertConfig.from_pretrained(\"pretraining_from_scratch/checkpoint-3944175\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"pretraining_from_scratch/checkpoint-3944175\",config=config,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_tokens(valid_dataset, top_n=1000):\n",
    "    token_counts = defaultdict(int)\n",
    "\n",
    "    for example in valid_dataset: \n",
    "        labels = example['labels']\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                token_counts[label] += 1\n",
    "    \n",
    "    \n",
    "    return token_counts\n",
    "\n",
    "# Example usage\n",
    "# # Assuming valid_dataset and tokenizer are already loaded\n",
    "# most_frequent_tokens = get_most_frequent_tokens(valid_dataset)\n",
    "\n",
    "def special_token(token,example):\n",
    "    return token in example['labels']\n",
    "\n",
    "def filter_and_process_dataset(valid_dataset, valid_token_list, tokenizer, preprocessing, max_examples=10000, max_filtered=1000):\n",
    "    filtered_datasets = {}\n",
    "\n",
    "    for token_id in most_frequent_tokens:\n",
    "        # Filter the dataset for the current token\n",
    "        valid_filtered_dataset = valid_dataset.select(range(max_examples)).filter(lambda example: special_token(token_id, example))\n",
    "        \n",
    "        # Skip if the filtered dataset is empty\n",
    "        if len(valid_filtered_dataset) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Process the filtered dataset with the context mask\n",
    "        valid_sentence_filtered = valid_filtered_dataset.map(lambda example: preprocessing.get_context_with_mask(example, token_id, tokenizer))\n",
    "        \n",
    "        # Limit to max_filtered examples for efficiency\n",
    "        limited_dataset = valid_sentence_filtered.select(range(min(len(valid_sentence_filtered), max_filtered)))\n",
    "        \n",
    "        filtered_datasets[token_id] =limited_dataset\n",
    "        print(f\"Processed token ID: {token_id} with {len(limited_dataset)} examples\")\n",
    "\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "# filtered_datasets = filter_and_process_dataset(valid_dataset,valid_tokens_list,tokenizer,preprocessing)\n",
    "# with open(\"word_embedding_distance_layer12.csv\",\"w\") as file :\n",
    "#     writer=csv.writer(file)\n",
    "#     writer.writerow([\"indices\", \"token\",\"euclidean_distances\",\"cosine similarity\" ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_tokens = get_most_frequent_tokens(lm_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"sv_core_news_sm\")\n",
    "\n",
    "# Function to identify named entities\n",
    "def identify_named_entities(tokens):\n",
    "    named_entities = set()\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        for ent in doc.ents :\n",
    "            if (ent.label_ =='PRS') and ent.text[0].isupper():\n",
    "                print(ent)\n",
    "                named_entities.add(token)\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "named_entities = identify_named_entities(token_sim)\n",
    "print(named_entities)\n",
    "print(token_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_noun_similarities = [similarity for token_id, similarity in cclassified_words if token_id in named_entities]\n",
    "other_token_similarities = [similarity for token_id, similarity in cclassified_words if token_id not in named_entities]\n",
    "# Calculer les similarités cosinus moyennes\n",
    "avg_proper_noun_similarity = np.mean(proper_noun_similarities)\n",
    "avg_other_token_similarity = np.mean(other_token_similarities)\n",
    "\n",
    "print(f\"Similarité cosinus moyenne pour les noms propres: {avg_proper_noun_similarity}\")\n",
    "print(f\"Similarité cosinus moyenne pour les autres tokens: {avg_other_token_similarity}\")\n",
    "\n",
    "# Effectuer un test t\n",
    "t_stat, p_value = ttest_ind(proper_noun_similarities, other_token_similarities, equal_var=False)\n",
    "print(f\"Statistique t: {t_stat}, Valeur p: {p_value}\")\n",
    "\n",
    "# Interprétation\n",
    "if p_value < 0.05:\n",
    "    print(\"La différence de similarités cosinus est statistiquement significative.\")\n",
    "else:\n",
    "    print(\"La différence de similarités cosinus n'est pas statistiquement significative.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(proper_noun_similarities, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Cosine Distribution - Proper Noun')\n",
    "plt.xlabel('cosine')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(other_token_similarities, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title('Cosine Distribution -Other')\n",
    "plt.xlabel('cosine')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([proper_noun_similarities,other_token_similarities], labels=['Noun', 'Other'])\n",
    "plt.title('Cosine Box Plot')\n",
    "plt.ylabel('COsine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_noun_similarities = [(token, similarity) for token, similarity in cclassified_words if token in named_entities]\n",
    "\n",
    "# Trier les similarités cosinus par ordre croissant\n",
    "sorted_proper_noun_similarities = sorted(proper_noun_similarities, key=lambda x: x[1])\n",
    "sorted_proper_noun_similarities [-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correaltion between frequency and cosine similarity in word embedding \n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "frequency_dict = {word: count for word, count in most_frequent_words}\n",
    "token_sim = [token for token,_ in cclassified_words]\n",
    "aligned_frequency_scores = [frequency_dict[word] for word in token_sim]\n",
    "cosine_similarity=[sim for word,sim in cclassified_words]\n",
    "\n",
    "\n",
    "pearson_corr, _ = pearsonr(cosine_similarity, aligned_frequency_scores)\n",
    "print(f'Corrélation de Pearson : {pearson_corr}')\n",
    "\n",
    "# Calculer la corrélation de Spearman\n",
    "spearman_corr, _ = spearmanr(cosine_similarity, aligned_frequency_scores)\n",
    "print(f'Corrélation de Spearman : {spearman_corr}')\n",
    "\n",
    "# Combine frequencies and cosine similarities\n",
    "frequency_similarity_pairs = [(freq, sim) for sim, freq in zip(cosine_similarity, aligned_frequency_scores )]\n",
    "freqs, sims = zip(*frequency_similarity_pairs)\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr, p_value = pearsonr(freqs, sims)\n",
    "print(f\"Pearson correlation: {pearson_corr}, P-value: {p_value}\")\n",
    "spearman_corr, p_value = spearmanr(freqs, sims)\n",
    "print(f\"Spearman correlation: {spearman_corr}, P-value: {p_value}\")\n",
    "\n",
    "plt.scatter(sims,freqs, alpha=0.5)\n",
    "plt.title('Token Frequency vs. Cosine Similarity')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(frequency_similarity_pairs, columns=['Frequency', 'Cosine Similarity'])\n",
    "\n",
    "# Bin the frequencies\n",
    "df['Frequency Bin'] = pd.qcut(df['Cosine Similarity'], q=10, duplicates='drop')\n",
    "\n",
    "# Plot box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.boxplot(column='Frequency', by='Frequency Bin', grid=False, showfliers=False)\n",
    "plt.title(' Frequency Bin by Cosine Similarity')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Cosine Bin')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frequency_last_100 = [frequency_dict[word] for word in token_sim[-100:]]\n",
    "print(\"mean frequency last 100\",np.mean(frequency_last_100))\n",
    "\n",
    "frequency_first_100 = [frequency_dict[word] for word in token_sim[:100]]\n",
    "print(\"mean frequency first 100\",np.mean(frequency_first_100))\n",
    "\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(frequency_last_100, frequency_first_100, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in frequencies is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in frequencies is not statistically significant.\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(frequency_last_100, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Frequency Distribution - Last 100 Tokens')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(frequency_first_100, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title('Frequency Distribution - First 100 Tokens')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([frequency_last_100, frequency_first_100], labels=['Last 100 Tokens', 'First 100 Tokens'])\n",
    "plt.title('Frequency Box Plot')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def print_statistics(data, label):\n",
    "    print(f\"Statistics for {label}:\")\n",
    "    print(f\"  Mean: {np.mean(data)}\")\n",
    "    print(f\"  Median: {np.median(data)}\")\n",
    "    print(f\"  Mode: {max(set(data), key=data.count)}\")\n",
    "    print(f\"  Standard Deviation: {np.std(data)}\")\n",
    "    print(f\"  Min: {np.min(data)}\")\n",
    "    print(f\"  Max: {np.max(data)}\")\n",
    "    print()\n",
    "\n",
    "print_statistics(frequency_last_100, \"Last 100 Tokens\")\n",
    "print_statistics(frequency_first_100, \"First 100 Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "euclidean_measures =[]\n",
    "cosine_measures = []\n",
    "tokens=[]\n",
    "for token in filtered_datasets.keys() :\n",
    "    valid_sentence_filtered = filtered_datasets[token]\n",
    "    dataloader  =preprocessing.create_dataloader(valid_sentence_filtered,1,default_data_collator)\n",
    "    baseline_embeddings = comprehension_model.get_embeddings(model_kb, dataloader, tokenizer)\n",
    "    finetuned_embeddings = comprehension_model.get_embeddings(model_hugging_face, dataloader, tokenizer)\n",
    "    euclidean_distances = [euclidean(baseline, finetuned) for baseline, finetuned in zip(baseline_embeddings[-1], finetuned_embeddings[-1])]\n",
    "    euclidean_measures.append(np.mean(euclidean_distances))\n",
    "    cosine_measure = cosine_similarity(baseline_embeddings[-1], finetuned_embeddings[-1])\n",
    "    avg_cosine = np.mean(cosine_measure)\n",
    "    cosine_measures.append(avg_cosine)\n",
    "    tokens.append(token)\n",
    "    with open(\"word_embedding_distance_layer12.csv\", \"a\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i, token,np.mean(euclidean_distances),avg_cosine ])\n",
    "    print(tokenizer.decode(token))\n",
    "    \n",
    "high_separated_indices12 = sorted(range(len(euclidean_measures)), key=lambda i: euclidean_measures[i], reverse=True)[:50]\n",
    "least_separated_indices12 = sorted(range(len(euclidean_measures)), key=lambda i: euclidean_measures[i])[:50]\n",
    "high_separated_values12 = [euclidean_measures[i] for i in high_separated_indices12]\n",
    "least_separated_values12 = [euclidean_measures[i] for i in least_separated_indices12]\n",
    "print(high_separated_values12)\n",
    "print(least_separated_values12)\n",
    "high_separated_tokens12 = [tokens[i] for i in high_separated_indices12]\n",
    "least_separated_tokens12 = [tokens[i] for i in least_separated_indices12]\n",
    "high_separated_words12 = [tokenizer.decode([idx]) for idx in high_separated_tokens12]\n",
    "least_separated_words12 = [tokenizer.decode([idx]) for idx in least_separated_tokens12]\n",
    "print(high_separated_words12)\n",
    "print(least_separated_words12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "cos_similarities = cosine_similarity(baseline_embeddings[0], finetuned_embeddings[0])\n",
    "euclidean_distances = [euclidean(baseline, finetuned) for baseline, finetuned in zip(baseline_embeddings[0], finetuned_embeddings[0])]\n",
    "\n",
    "print(\"Average Cosine Similarity:\", np.mean(cos_similarities))\n",
    "print(\"Average Euclidean Distance:\", np.mean(euclidean_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face.bert.encoder.layer[0].output.dense.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the kernelized correlation coefficient function\n",
    "def kernelized_corr_coef(x, y, K1, K2, K12, emb_mean1, emb_mean2, n):\n",
    "    with torch.no_grad():\n",
    "        mu1, mu2 = x @ emb_mean1, y @ emb_mean2\n",
    "        mu12 = torch.outer(mu1, mu2)\n",
    "        x_norm, y_norm = map(lambda x, K, mu: (1/n * (x @ K) * x).sum(dim=-1) - mu**2, [x, y], [K1, K2], [mu1, mu2])\n",
    "        return (1 / n * x @ K12 @ y.T - mu12) / torch.sqrt(torch.outer(x_norm, y_norm))\n",
    "\n",
    "# Collect parameters from models\n",
    "num_layers = 12\n",
    "\n",
    "multi_WQ_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.self.query.weight\"].T for j in range(num_layers)])\n",
    "multi_WV_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.self.value.weight\"].T for j in range(num_layers)])    \n",
    "multi_WK_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.self.key.weight\"].T for j in range(num_layers)])\n",
    "multi_K_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.intermediate.dense.weight\"] for j in range(num_layers)])\n",
    "multi_V_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.output.dense.weight\"].T for j in range(num_layers)])\n",
    "multi_WO_kb = torch.cat([model_kb.state_dict()[f\"bert.encoder.layer.{j}.attention.output.dense.weight\"] for j in range(num_layers)])\n",
    "multi_E_kb = model_kb.state_dict()['bert.embeddings.word_embeddings.weight'].T\n",
    "\n",
    "multi_WQ_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.self.query.weight\"].T for j in range(num_layers)])\n",
    "multi_WV_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.self.value.weight\"].T for j in range(num_layers)])    \n",
    "multi_WK_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.self.key.weight\"].T for j in range(num_layers)])\n",
    "multi_K_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.intermediate.dense.weight\"] for j in range(num_layers)])\n",
    "multi_V_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.output.dense.weight\"].T for j in range(num_layers)])\n",
    "multi_WO_ft = torch.cat([model_hugging_face.state_dict()[f\"bert.encoder.layer.{j}.attention.output.dense.weight\"] for j in range(num_layers)])\n",
    "multi_E_ft = model_hugging_face.state_dict()['bert.embeddings.word_embeddings.weight'].T\n",
    "\n",
    "# Ensure correct kernel matrices calculation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    kernel11 = (multi_E_kb @ multi_E_kb.T).to(device)\n",
    "    kernel22 = (multi_E_ft @ multi_E_ft.T).to(device)\n",
    "    kernel12 = (multi_E_kb @ multi_E_ft.T).to(device)\n",
    "    emb_mean1 = torch.mean(multi_E_kb, dim=1).to(device)\n",
    "    emb_mean2 = torch.mean(multi_E_ft, dim=1).to(device)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(7.5, 4.25))\n",
    "for i, (title, param1, param2) in enumerate(tqdm([(\"$K$\", multi_K_kb,multi_K_ft), (\"$V$\", multi_K_kb, multi_K_ft), (\"$W_K$\", multi_WK_kb , multi_WK_ft), \n",
    "                                                    (\"$W_Q$\", multi_WQ_kb,multi_WQ_ft), (\"$W_V$\", multi_WV_kb, multi_WV_ft), (\"$W_O$\", multi_WO_kb, multi_WO_ft)])):\n",
    "    with torch.no_grad():\n",
    "        S = kernelized_corr_coef(param1.to(device), param2.to(device), \n",
    "                            kernel11, kernel22, kernel12, \n",
    "                            emb_mean1, emb_mean2, n=len(tokenizer))\n",
    "        layer_size = param1.shape[0] // num_layers\n",
    "        S_agg = S.view(num_layers, layer_size, num_layers, layer_size).abs().mean([-1, -3]).cpu().numpy()\n",
    "        \n",
    "    # Plotting each subplot\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(S_agg)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Clean up memory\n",
    "    del S, S_agg\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "plt.savefig(\"Correlation_between_layers_embedding_projection.pdf\")\n",
    "\n",
    "# Final memory clean up\n",
    "del kernel11, kernel22, kernel12\n",
    "del multi_WQ_kb, multi_WV_kb, multi_WK_kb, multi_K_kb, multi_V_kb, multi_WO_kb, multi_E_kb\n",
    "del multi_WQ_ft, multi_WV_ft, multi_WK_ft, multi_K_ft, multi_V_ft, multi_WO_ft, multi_E_ft\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token present in the training dataset\n",
    "\n",
    "token_frequencies = Counter()\n",
    "\n",
    "\n",
    "for example in lm_datasets[\"train\"]:\n",
    "    token_frequencies.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent token\n",
    "\n",
    "n = int(len(token_frequencies.keys())*0.8) \n",
    "most_common_tokens = token_frequencies.most_common(n)\n",
    "most_common_ids, most_common_freqs = zip(*most_common_tokens)\n",
    "most_common_ids_tensor = torch.tensor(most_common_ids)\n",
    "\n",
    "print(\"Les IDs des tokens les plus fréquents et leurs fréquences :\")\n",
    "for token_id, freq in zip(most_common_ids, most_common_freqs):\n",
    "    print(f\"Token ID: {token_id}, Fréquence: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehension_model.change_embedding_word(model_kb,model_hugging_face,valid_dataset,tokenizer,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def unique_token_ids(dataset):\n",
    "    unique_token_ids = set()\n",
    "    for example in dataset:\n",
    "        unique_token_ids.update(example['input_ids'])\n",
    "    return torch.tensor(sorted(unique_token_ids))\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Ensure tensors are flattened (1D) to compute vector cosine similarity\n",
    "    tensor1_flat = tensor1.view(-1)\n",
    "    tensor2_flat = tensor2.view(-1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(tensor1_flat.unsqueeze(0), tensor2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "unique_token_id = unique_token_ids(lm_datasets[\"train\"])\n",
    "print(unique_token_id)\n",
    "ref_weight = model_kb.bert.embeddings.word_embeddings.weight.detach()[unique_token_id]\n",
    "bias_weight = model_hugging_face.bert.embeddings.word_embeddings.weight.detach()[unique_token_id]\n",
    "\n",
    "similarities = [cosine_similarity(ref_weight[i], bias_weight[i]) for i in range(len(unique_token_id))]\n",
    "    \n",
    "token_similarity = sorted(zip(unique_token_id, similarities), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "classified_words = [(tokenizer.convert_ids_to_tokens(token_id.item()), similarity) for token_id, similarity in token_similarity]\n",
    "    \n",
    "for wrodclassified_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cosine_similarity_words_emebdding_cpt\", 'w') as file:\n",
    "        for word, similarity in cclassified_words:\n",
    "            file.write(f\"{word}: {similarity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_100_tokens = token_similarity[:100]\n",
    "first_classifier_100 = [(tokenizer.convert_ids_to_tokens(token_id.item())) for token_id, similarity in first_100_tokens]\n",
    "last_100_tokens =token_similarity[-100:]\n",
    "last_classifier_100 = [(tokenizer.convert_ids_to_tokens(token_id.item())) for token_id, similarity in last_100_tokens]\n",
    "\n",
    "print(last_classifier_100)\n",
    "print(first_classifier_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph represent the most changed word embedding thorugh epochs\n",
    "\n",
    "checkpoint_directory = 'finetuning/finetuning_hugging_whitespace-finetuned-imdb'\n",
    "checkpoint_files = os.listdir(checkpoint_directory)\n",
    "\n",
    "checkpoint_files.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)))\n",
    "epoch_data = {}\n",
    "word_color = {}\n",
    "total_frequencies = {}\n",
    "initial_embeddings = model_kb.bert.embeddings.word_embeddings.weight.detach()[most_common_ids_tensor]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 5))\n",
    "# Training loop\n",
    "epoch=4\n",
    "for checkpoint in checkpoint_files:\n",
    "    epoch +=1\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"finetuning/finetuning_hugging_whitespace-finetuned-imdb/\" + checkpoint)\n",
    "    model=model.to(device)\n",
    "    #initial_embeddings = embeddings\n",
    "    embeddings = model.bert.embeddings.word_embeddings.weight.detach()[most_common_ids_tensor]\n",
    "\n",
    "    embedding_changes = torch.norm(embeddings - initial_embeddings, dim=1)\n",
    "    \n",
    "    # Get the top 5 changes\n",
    "    _, top_indices = torch.topk(embedding_changes, 1)\n",
    "    top_words = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in top_indices]\n",
    "    top_changes = embedding_changes[top_indices].tolist()\n",
    "    for word in top_words:\n",
    "            if word not in word_color:\n",
    "                word_color[word] = colors[len(word_color) % len(colors)]\n",
    "            if word not in total_frequencies :\n",
    "                total_frequencies[word] =1\n",
    "            if word  in total_frequencies :\n",
    "                total_frequencies[word] +=1\n",
    "        \n",
    "    # Store data\n",
    "    epoch_data[epoch] = list(zip(top_words, top_changes))\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "already_labeled = set()\n",
    "for epoch in epoch_data:\n",
    "    words, changes = zip(*epoch_data[epoch])\n",
    "    if words[0] not in already_labeled:\n",
    "        ax.plot([epoch] * 1, changes, marker='o', linestyle='', markersize=10,color=word_color[words[0]],label=words[0])\n",
    "        already_labeled.add(words[0])\n",
    "    else : \n",
    "        ax.plot([epoch] * 1, changes, marker='o', linestyle='', markersize=10,color=word_color[words[0]])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Change in Norm of Embeddings')\n",
    "ax.set_title('Most changed word embedding by Epoch')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "#Saliency scores\n",
    "\n",
    "\n",
    "\n",
    "model_hugging_face.eval()\n",
    "\n",
    "# Prepare the text and encode it\n",
    "text = \"skrift om ersättning för resekostnad till och från [MASK] samt ar\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_mask = encoded_input['attention_mask'].to(device)\n",
    "embeddings = model_hugging_face.bert.embeddings(input_ids)\n",
    "embeddings.retain_grad()\n",
    "output = model_hugging_face(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "logits = output.logits\n",
    "\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# Extract the logit corresponding to the masked token\n",
    "masked_token_logit = logits[0, mask_token_index, :]\n",
    "\n",
    "# Choose the target logit (e.g., the highest probability logit)\n",
    "target_logit = masked_token_logit.max()\n",
    "\n",
    "# Backward pass to compute the gradients\n",
    "model_hugging_face.zero_grad()\n",
    "target_logit.backward()\n",
    "# Get the input embeddings\n",
    "grads = embeddings.grad\n",
    "print(grads)\n",
    "# Compute the saliency map as the L2 norm of the gradients\n",
    "saliency = torch.norm(grads, dim=-1).squeeze()\n",
    "\n",
    "# Detach and move the saliency map to the CPU\n",
    "saliency = saliency.detach().cpu().numpy()\n",
    "\n",
    "# Tokenize the input text to match the saliency scores with the tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())\n",
    "\n",
    "# Display the tokens and their saliency scores\n",
    "for token, score in zip(tokens, saliency):\n",
    "    print(f\"Token: {token}, Saliency: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "inputs = valid_filtered_dataset[0]\n",
    "index = inputs[\"labels\"].index(token_id)\n",
    "print(index)\n",
    "criterion =nn.CrossEntropyLoss(reduction='none')\n",
    "input_ids = torch.tensor(inputs['input_ids'][index-9:index+3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "attention_mask = torch.tensor(inputs[\"attention_mask\"][index-9:index+3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "labels = torch.tensor(inputs[\"labels\"][index-9:index+3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "print(tokenizer.decode(inputs['input_ids'][index-9:index+3]))\n",
    "print(labels)\n",
    "with torch.no_grad():\n",
    "    outputs = model_hugging_face(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "        output_attentions=True\n",
    "    )\n",
    "softmax_probs = F.softmax(outputs.logits.squeeze()[-3], dim=-1)\n",
    "sorted_probs, sorted_indices = torch.sort(softmax_probs, descending=True)\n",
    "sorted_tokens = [tokenizer.decode([idx]) for idx in sorted_indices[:10]]\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_tokens, sorted_probs[:10].cpu().numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Top 10 Predicted Tokens and their Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_token(token,example):\n",
    "    return token in example['labels']\n",
    "word = \"Statsrådet\"\n",
    "token_id = tokenizer.convert_tokens_to_ids(word)\n",
    "valid_filtered_dataset = valid_dataset.select(range(100000)).filter(lambda example : special_token(token_id,example))\n",
    "valid_filtered_dataloader=preprocessing.create_dataloader(valid_filtered_dataset,64,default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOGIT LENS\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch.nn.functional as F\n",
    "# Charger le modèle et le tokenizer\n",
    "\n",
    "def get_probability_distribution(sentence,model):\n",
    "# Mettre le modèle en mode évaluation\n",
    "    model.eval()\n",
    "    print(tokenizer.decode(sentence[\"input_ids\"]))\n",
    "\n",
    "    input_ids = torch.tensor(sentence ['input_ids'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    labels = torch.tensor(sentence ['labels'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask =torch.tensor(sentence ['attention_mask'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    token_ids = sentence[\"labels\"].index(token_id)\n",
    "    print(token_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states \n",
    "    \n",
    "\n",
    "\n",
    "    embedding_matrix = model.bert.embeddings.word_embeddings.weight  \n",
    "\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states):\n",
    "        logits = hidden_state @ embedding_matrix.T \n",
    "\n",
    "        # Obtenir les probabilités avec softmax\n",
    "        probs = torch.nn.functional.softmax(logits.squeeze()[token_ids], dim=-1)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        # Pour chaque token dans la séquence, obtenir les 5 premiers tokens prédits et leurs probabilités\n",
    "        print(f\"Layer {layer_idx}\")\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probs, 5)\n",
    "        top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "        for prob, token in zip(top_probs, top_tokens):\n",
    "            print(f\"  {token}: {prob.item()}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    softmax_probs = F.softmax(outputs.logits.squeeze()[token_ids], dim=-1)\n",
    "    top_probs, top_indices = torch.topk(softmax_probs, 5)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    for prob, token in zip(top_probs, top_tokens):\n",
    "        print(f\"  {token}: {prob.item()}\")\n",
    "    \n",
    "    \n",
    "get_probability_distribution (valid_filtered_dataset[2],model_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch.nn.functional as F\n",
    "get_probability_distribution(valid_filtered_dataset[2],model_hugging_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuned lens\n",
    "import torch.nn as nn\n",
    "def train_translators (model,dataloader,num_epochs=5,learning_rate=1e-4):\n",
    "\n",
    "    class Translator(nn.Module):\n",
    "        def __init__(self, hidden_size, vocab_size):\n",
    "            super(Translator, self).__init__()\n",
    "            self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    hidden_size = model.config.hidden_size\n",
    "    vocab_size = model.config.vocab_size\n",
    "\n",
    "    translators = nn.ModuleList([Translator(hidden_size, vocab_size) for _ in range(num_layers)])\n",
    "    translators.to(device)\n",
    "    # Training parameters\n",
    "    num_epochs = num_epochs\n",
    "    learning_rate = learning_rate\n",
    "    \n",
    "    for layer_index in range(1,13) :\n",
    "        \n",
    "        optimizer = torch.optim.Adam(translators[layer_index-1].parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in dataloader:\n",
    "                batch={key: value.to(device) for key,value in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "                    hidden_states = outputs.hidden_states\n",
    "                    final_logits = outputs.logits\n",
    "\n",
    "                loss = 0\n",
    "                projected_logits = translators[layer_idx](hidden_states[layer_index].to(device))\n",
    "                distillation_loss = F.mse_loss(projected_logits, final_logits)\n",
    "                loss += distillation_loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "        print(f\"layer {layer_index }\")\n",
    "    return translators\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translators_kb=train_translators(model_kb,small_valid_dataloader)\n",
    "translators_ft = train_translators(model_hugging_face,small_valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_distribution_tuned(sentence,model,translators):\n",
    "# Mettre le modèle en mode évaluation\n",
    "    model.eval()\n",
    "   \n",
    "\n",
    "    input_ids = torch.tensor(sentence ['input_ids'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    labels = torch.tensor(sentence ['labels'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask =torch.tensor(sentence ['attention_mask'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    token_ids = sentence[\"labels\"].index(token_id)\n",
    "    print(token_ids)\n",
    "    print(tokenizer.decode(sentence[\"input_ids\"][token_ids-1:token_ids+4]))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states \n",
    "    \n",
    "    predictions = {}\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states[1:]):\n",
    "        logits = translators[layer_idx](hidden_state) \n",
    "        probs = torch.nn.functional.softmax(logits.squeeze()[token_ids], dim=-1)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        # Pour chaque token dans la séquence, obtenir les 5 premiers tokens prédits et leurs probabilités\n",
    "        top_probs, top_indices = torch.topk(probs, 5)\n",
    "        top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "        predictions[f\"Layer {layer_idx + 1}\"] = [(token, prob.item()) for token, prob in zip(top_tokens, top_probs)]\n",
    "        \n",
    "\n",
    "    softmax_probs = F.softmax(outputs.logits.squeeze()[token_ids], dim=-1)\n",
    "    top_probs, top_indices = torch.topk(softmax_probs, 5)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    predictions[\"output\"]=[(token, prob.item()) for token, prob in zip(top_tokens, top_probs)]\n",
    "    for prob, token in zip(top_probs, top_tokens):\n",
    "        print(f\"  {token}: {prob.item()}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "sentence = valid_filtered_dataset[16]\n",
    "token_ids= sentence[\"labels\"].index(token_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_probability_distribution_tuned(sentence,model_kb,translators_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_probability_distribution_tuned(sentence,model_hugging_face,translators_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def pivot_graph(predictions, model_name, sentence):\n",
    "    data = []\n",
    "    for layer in range(1, 13):\n",
    "        tokens_probs = predictions[f\"Layer {layer}\"]\n",
    "        for rank, (token, prob) in enumerate(tokens_probs):\n",
    "            data.append({\n",
    "                \"Layer\": layer,\n",
    "                \"Rank\": rank + 1,\n",
    "                \"Token\": token,\n",
    "                \"Probability\": prob\n",
    "            })\n",
    "    for rank, (token, prob) in enumerate(predictions[\"output\"]):\n",
    "        data.append({\n",
    "            \"Layer\": \"output\",\n",
    "            \"Rank\": rank + 1,\n",
    "            \"Token\": token,\n",
    "            \"Probability\": prob\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    pivot_table = df.pivot(index=\"Layer\", columns=\"Rank\", values=\"Probability\")\n",
    "    token_pivot = df.pivot(index=\"Layer\", columns=\"Rank\", values=\"Token\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.text(0.5, 1.15, sentence, horizontalalignment='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "    ax = sns.heatmap(pivot_table, annot=token_pivot, fmt='', cmap=\"Blues\", cbar_kws={'label': 'Probability'})\n",
    "    ax.set_title(f\"Tuned Lens Predictions for {model_name}\")\n",
    "    ax.set_xlabel(\"Top Predicted Tokens\")\n",
    "    ax.set_ylabel(\"Layer\")\n",
    "\n",
    "    ax.set_xticklabels([str(i) for i in range(1, pivot_table.columns.size + 1)], rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sentence_token = tokenizer.decode(sentence[\"input_ids\"][token_ids-10:token_ids+3])\n",
    "print(sentence_token)\n",
    "pivot_graph(prediction_kb,'KB Bert',sentence_token)\n",
    "pivot_graph(prediction_ft,'Finetuned Model',sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre essai\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "def verify_all_layers_scratch_in_base(model, model_scratch):\n",
    "    n_in = 0\n",
    "    for layer in model_scratch.state_dict().keys():\n",
    "        if layer in model.state_dict().keys():\n",
    "            n_in += 1\n",
    "    return n_in == len(model_scratch.state_dict())\n",
    "\n",
    "\n",
    "def copy_layers_from_pretrained(model, model_scratch, log=False):\n",
    "    for layer in model_scratch.state_dict().keys():\n",
    "        model_scratch.state_dict()[layer] = model_scratch.state_dict()[layer].copy_(model.state_dict()[layer])\n",
    "    \n",
    "    if log:\n",
    "        print(\"Copy done successfully!\")\n",
    "    \n",
    "    return model_scratch\n",
    "\n",
    "\n",
    "def generate_prediction_trajectory(model, sentence, masked_index,token_id):\n",
    "    input_ids = torch.tensor(sentence['input_ids'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    labels = torch.tensor(sentence['labels'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask =torch.tensor(sentence ['attention_mask'],dtype=torch.long).unsqueeze(0).to(device)\n",
    "    token_ids = sentence[\"labels\"].index(token_id)\n",
    "    print(token_ids)\n",
    "    print(tokenizer.decode(sentence[\"input_ids\"][token_ids-1:token_ids+4]))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    final_predictions = logits.argmax(dim=-1)\n",
    "    label_pred_final = token_id\n",
    "    label_confidence_final = logits.max(dim=-1)\n",
    "\n",
    "    _, indexes_sorted = torch.sort(logits, descending=True)\n",
    "    final_label_rank = torch.where(indexes_sorted[0, masked_index] == label_pred_final)[0].item()\n",
    "\n",
    "    prediction_trajectory = []\n",
    "    rank_trajectory = []\n",
    "    interm_label_confidence_evolution = []\n",
    "\n",
    "    total_hidden_layers = model.config.num_hidden_layers\n",
    "\n",
    "    for num_interm_layers in tqdm(range(1, total_hidden_layers + 1)):\n",
    "        config = BertConfig.from_pretrained(model.config.name_or_path)\n",
    "        config.num_hidden_layers = num_interm_layers\n",
    "\n",
    "        model_scratch = BertForMaskedLM(config)\n",
    "        model_scratch.to(device)\n",
    "\n",
    "        if verify_all_layers_scratch_in_base(model, model_scratch):\n",
    "            model_interm = copy_layers_from_pretrained(model, model_scratch)\n",
    "            outputs_interm = model_interm(**inputs)\n",
    "            logits_interm = outputs_interm.logits\n",
    "            label_pred_interm = logits_interm.argmax(dim=-1)[0, masked_index].item()\n",
    "            interm_label_confidence = F.softmax(logits_interm, dim=-1)[0, masked_index, label_pred_interm].item()\n",
    "\n",
    "            interm_final_label_confidence = F.softmax(logits_interm, dim=-1)[0, masked_index, label_pred_final].item()\n",
    "            \n",
    "            _, indexes_sorted_interm = torch.sort(logits_interm, descending=True)\n",
    "            interm_final_label_rank = torch.where(indexes_sorted_interm[0, masked_index] == label_pred_final)[0].item()\n",
    "\n",
    "            prediction_trajectory.append(tokenizer.convert_ids_to_tokens(label_pred_interm))\n",
    "            rank_trajectory.append(torch.where(indexes_sorted[0, masked_index] == label_pred_final)[0].item())\n",
    "            interm_label_confidence_evolution.append(label_confidence_final)\n",
    "\n",
    "    prediction_trajectory.append(tokenizer.convert_ids_to_tokens(final_predictions[0, masked_index].item()))\n",
    "    rank_trajectory.append(interm_final_label_rank)\n",
    "    interm_label_confidence_evolution.append(interm_label_confidence)\n",
    "    return prediction_trajectory, rank_trajectory, interm_label_confidence_evolution\n",
    "\n",
    "\n",
    "\n",
    "prediction_kb,rank_kb,proba_kb = generate_prediction_trajectory(model_kb,sentence,token_ids,token_id)\n",
    "prediction_ft,rank_ft,proba_ft = generate_prediction_trajectory(model_hugging_face,sentence,token_ids,token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot_trajectory(prediction_trajectories, probabilities, y_labels, figsize=(25, 10), fontsize=15):\n",
    "    tableau = np.array(prediction_trajectories)\n",
    "    gradient = np.array(probabilities)\n",
    "\n",
    "    normalized_probs = (gradient - np.min(gradient)) / (np.max(gradient) - np.min(gradient))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(normalized_probs, cmap='viridis', aspect='auto')\n",
    "\n",
    "    ax.set_xticks(np.arange(tableau.shape[1]))\n",
    "    ax.set_xticklabels([f'layer {i+1}' for i in range(tableau.shape[1]-1)]+ ['Output'])\n",
    "    ax.set_xlabel('num_hidden_layers')\n",
    "\n",
    "    ax.set_yticks(np.arange(len(y_labels)))\n",
    "    ax.set_yticklabels(y_labels,  color='black', fontsize=fontsize)\n",
    "\n",
    "    for i in range(tableau.shape[0]):\n",
    "        for j in range(tableau.shape[1]):\n",
    "            ax.text(j, i, tableau[i, j], ha='center', va='center', color='w' if normalized_probs[i, j] <= 0.5 else 'black', fontsize=fontsize)\n",
    "\n",
    "    cbar = fig.colorbar(cax, fraction=0.03, pad=0.05)\n",
    "    cbar.set_label('Probability')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_trajectory([prediction_kb,prediction_ft],[proba_kb,proba_ft], y_labels = [\"KB Bert\",\"Finetuned\"], figsize = (40, 15), fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
