{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import argparse\n",
    "import preprocessing\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at finetuning_hugging_python-finetuned-imdb/checkpoint-920384 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"finetuning_hugging_python-finetuned-imdb/checkpoint-920384\")\n",
    "model_finetuned=model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_NaN(subset,example):\n",
    "    return example[subset] is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_label(dataset,nb_obs,label_name):\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Calculer le nombre d'observations pour chaque étiquette\n",
    "    grouped_data = df.groupby(label_name)\n",
    "\n",
    "    # Calculer le nombre d'observations par étiquette pour obtenir une répartition uniforme\n",
    "    total_samples = nb_obs\n",
    "    samples_per_label = total_samples // len(grouped_data.groups)\n",
    "\n",
    "    # Créer une liste pour stocker les observations échantillonnées\n",
    "    sampled_data = []\n",
    "\n",
    "    # Prélever aléatoirement les observations pour chaque groupe de label\n",
    "    for group_label, group_data in grouped_data.groups.items():\n",
    "        group_dataset=dataset.select(group_data)\n",
    "        label_data = group_dataset.shuffle(seed=np.random.randint(1, 1000)).select(range(min(len(group_data), samples_per_label)))\n",
    "        sampled_data.extend(label_data)\n",
    "\n",
    "    # Mélanger les observations pour obtenir un ordre aléatoire\n",
    "    np.random.shuffle(sampled_data)\n",
    "\n",
    "    # Créer un Dataset Hugging Face à partir des observations échantillonnées\n",
    "    sampled_dataset = Dataset.from_dict({key: [example[key] for example in sampled_data] for key in sampled_data[0]})\n",
    "    \n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_random(dataset,nb_ob):\n",
    "    dataset=dataset.shuffle()\n",
    "    echantillon_aleatoire = dataset.select(range(nb_ob))\n",
    "    return echantillon_aleatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"Note\"],padding=True, truncation=True,max_length=512)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, accuracy = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        output = model(input_ids,\n",
    "            token_type_ids=None, \n",
    "            attention_mask=input_mask, \n",
    "            labels=labels)\n",
    "        loss += output.loss.item()\n",
    "        preds_batch = torch.argmax(output.logits, axis=1)\n",
    "        batch_acc = torch.mean((preds_batch == labels).float())\n",
    "        accuracy.append(batch_acc)\n",
    "        \n",
    "    accuracy = torch.mean(torch.tensor(accuracy))\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(protocole):\n",
    "    match = re.search(r'/(\\d+)/', protocole)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        return int(year[:4])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hemmamarknaden under 1970. Alla som ägde tillg...</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Att sedan köplusten inte mobiliserades med sam...</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Oavsett propagandan föreligger verkliga svårig...</td>\n",
       "      <td>seg</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Liknande förändringar har skett och sker dagli...</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Men frågan återstår: Vad finns det för reell a...</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>Herr Sköld frågar:</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>Herr Nilsson i Kristinehamn var rädd</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5184</th>\n",
       "      <td>Jag kan fortsätta med att citera handelsminist...</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>Slutligen uttalar Leif Blomberg:</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>Fru talman! Börje Vestlund, som får träda in i...</td>\n",
       "      <td>seg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  tag  split\n",
       "27    hemmamarknaden under 1970. Alla som ägde tillg...  seg  train\n",
       "28    Att sedan köplusten inte mobiliserades med sam...  seg  train\n",
       "29    Oavsett propagandan föreligger verkliga svårig...  seg    val\n",
       "30    Liknande förändringar har skett och sker dagli...  seg  train\n",
       "31    Men frågan återstår: Vad finns det för reell a...  seg  train\n",
       "...                                                 ...  ...    ...\n",
       "5157                                 Herr Sköld frågar:  seg  train\n",
       "5178               Herr Nilsson i Kristinehamn var rädd  seg  train\n",
       "5184  Jag kan fortsätta med att citera handelsminist...  seg  train\n",
       "5213                   Slutligen uttalar Leif Blomberg:  seg  train\n",
       "5220  Fru talman! Börje Vestlund, som får träda in i...  seg  train\n",
       "\n",
       "[2270 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Blomquist_2022_BERT/Base Classifcation Model/Training_iteration_2.csv\")\n",
    "df=df.rename(columns={'text':'content'})\n",
    "df = df[df[\"tag\"] !=\"unknown\"]\n",
    "df[df[\"tag\"]==\"seg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "# Utiliser groupby pour regrouper les données en fonction de la valeur de la colonne 'split'\n",
    "for split_value, group_df in df.groupby('split'):\n",
    "    # Stocker chaque DataFrame dans le dictionnaire avec une clé correspondant à la valeur de 'split'\n",
    "    dfs[split_value] = group_df.copy()\n",
    "\n",
    "# Accéder aux DataFrames séparés\n",
    "df_train= dfs['train'].drop(columns=['split'])\n",
    "df_test = dfs['test'].drop(columns=['split'])\n",
    "df_val = dfs['val'].drop(columns=['split'])\n",
    "\n",
    "df_train.to_csv(\"swerick_data_intro_train.csv\")\n",
    "df_test.to_csv(\"swerick_data_intro_test.csv\")\n",
    "df_train.to_csv(\"swerick_data_intro_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"swerick_data_party_train.pkl\", \"test\": \"swerick_data_party_test.pkl\"}\n",
    "party_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1794a95685494ec3bdd710c76852d704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b367d034cbee4dbfb97cf2a52dc0a3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'text', 'tag'],\n",
      "        num_rows: 3289\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'text', 'tag'],\n",
      "        num_rows: 973\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"swerick_data_intro_train.csv\", \"test\": \"swerick_data_intro_test.csv\"}\n",
    "party_dataset = load_dataset(\"csv\",data_files=data_files)\n",
    "print(party_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [extract_date_from_filename(row['protocole']) for row in party_dataset['train']]\n",
    "dates_test = [extract_date_from_filename(row['protocole']) for row in party_dataset['test']]\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date', dates)\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date', dates_test)\n",
    "\n",
    "print(party_dataset[\"train\"][0])\n",
    "print(party_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"valid\": \"swerick_data_party_valid.pkl\"}\n",
    "party_valid_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"valid\": \"swerick_data_intro_valid.csv\"}\n",
    "party_valid_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_valid = [extract_date_from_filename(row['protocole']) for row in party_valid_dataset['valid']]\n",
    "party_valid_dataset['valid'] = party_valid_dataset['valid'].add_column('date', dates_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "date_scaler = StandardScaler()\n",
    "dates_train_2d = [[date] for date in party_dataset['train'][\"date\"]]\n",
    "dates_test_2d=[[date] for date in party_dataset['test'][\"date\"]]\n",
    "date_scaler.fit(dates_train_2d)\n",
    "dates_train =date_scaler.transform(dates_train_2d)\n",
    "dates_test =date_scaler.transform(dates_test_2d)\n",
    "print(dates_train)\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date_scaled', dates_train.squeeze())\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date_scaled', dates_test.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].filter(lambda x : filter_NaN(\"party\",x))\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(party_dataset[\"train\"][\"gender\"])\n",
    "label_names = label_encoder.classes_\n",
    "label_dict={ i : label_names[i] for i in  range(len(label_names))}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels_gender.pkl\", \"wb\") as fp:   \n",
    "   pickle.dump(label_names, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.pkl\",\"rb\") as f :\n",
    "    label_names=pickle.load(f)\n",
    "\n",
    "print(label_names.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_party_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_train_datasets = subset_label(party_dataset[\"train\"],5000,\"party\")\n",
    "party_test_datasets = subset_label(party_dataset[\"test\"],5000,\"party\")\n",
    "party_valid_datasets = subset_label(party_valid_dataset[\"valid\"],5000,\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1 = subset_random(party_dataset[\"train\"],100)\n",
    "train_set2 = subset_random(party_dataset[\"train\"],200)\n",
    "train_set3 = subset_random(party_dataset[\"train\"],500)\n",
    "train_set4= subset_random(party_dataset[\"train\"],1000)\n",
    "test_set = subset_random(party_dataset[\"test\"],10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(party_train_datasets)\n",
    "print(party_test_datasets)\n",
    "print(party_valid_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset = concatenate_datasets([train_set,test_set,valid_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets = train_set.map(tokenize_function,batched=True )\n",
    "tokenized_test_datasets = test_set.map(tokenize_function,batched=True )\n",
    "tokenized_valid_datasets = valid_set.map(tokenize_function,batched=True )\n",
    "tokenized_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_test_datasets=tokenized_test_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_test_datasets=tokenized_test_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_test_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_valid_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers=4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        tokenized_train_datasets,\n",
    "        shuffle=True,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "        tokenized_valid_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "# Not used atm\n",
    "test_loader = DataLoader(\n",
    "        tokenized_test_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs =10\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(label_dict),\n",
    "        id2label=label_dict).to(\"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "num_training_steps = len(train_loader) * n_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "\n",
    "# Linear warmup and step decay\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_pickle(\"swerick_data_party_train.pkl\")\n",
    "df = df.rename(columns={\"Note\":\"content\",\"party\" : \"tag\"})\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "print(type(df[\"tag\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.dropna(subset=\"tag\")\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test_set.to_pandas()\n",
    "print(df)\n",
    "df = df.rename(columns={\"Note\":\"content\", \"date_scaled\" : \"tag\"})\n",
    "print(df)\n",
    "df.to_csv(\"swerick_subsetdata_date_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_regression.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_regression.py --base_model \"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\" --model_filename \"trained/regression_date_hugging_face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 compare_models_regression.py --model_filename1 \"trained/regression_date\" --model_filename2 \"trained/regression_date_hugging_face\" --data_path \"swerick_subsetdata_date_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year regression\n",
      "training\n",
      "stdout: \u001b[32m15:11:07 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m15:11:07 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m15:11:07 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([ 0.8181,  0.9596,  0.6059,  0.1579,  0.3465, -1.4926, -1.3983,  0.8889,\n",
      "         0.5116, -2.3179, -2.3650, -1.5162,  0.6767, -1.3747, -0.1958,  0.6531,\n",
      "         0.7002,  0.3230, -1.1153,  0.6295,  0.4644,  0.7474,  0.8417,  0.8417,\n",
      "        -1.9878,  0.4880,  0.9124,  0.6767, -0.1722, -1.9406,  0.5823, -1.5398,\n",
      "         0.3701, -1.4219,  0.7474,  0.3701, -1.1153,  0.2994, -0.2193, -0.7381,\n",
      "         0.7710,  1.0303,  0.5823,  0.8653,  0.9596, -1.8699,  1.0539, -2.5301,\n",
      "        -0.8324, -1.0210,  0.6295,  0.9596,  0.6295,  0.3465,  0.2287, -2.2943,\n",
      "         0.5823, -0.1015,  0.6295,  0.4173, -1.3747,  0.6059,  0.8417, -1.9642,\n",
      "         0.5588, -0.7145,  0.3230,  0.7710,  0.8889,  0.3701, -1.6105, -0.2901,\n",
      "         0.5352,  0.7945,  0.7474, -1.6577,  0.9124,  0.3701,  0.8653,  0.8417,\n",
      "        -0.1722,  0.1108, -0.4787,  0.6531,  0.7945, -1.2804, -0.1486,  0.5588,\n",
      "         0.7945,  0.4880,  0.5823,  0.7474,  0.5116,  0.6531,  0.7002,  0.6295,\n",
      "         0.5823,  0.8889,  0.1579,  0.6059])\u001b[0m\n",
      "length train loader : 70\n",
      "length valid loader : 15\n",
      "KBLab/bert-base-swedish-cased\n",
      "finetuning_hugging_whitespace-finetuned-imdb/checkpoint-2919750\n",
      "\u001b[34m15:11:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at finetuning_hugging_whitespace-finetuned-imdb/checkpoint-2919750 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/laurinemeier/swerick/train_regression.py\", line 292, in <module>\n",
      "    main(args)\n",
      "  File \"/home/laurinemeier/swerick/train_regression.py\", line 195, in main\n",
      "    output = model1(input_ids,token_type_ids=None,attention_mask =input_mask,labels=labels)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 1564, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 1013, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 607, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 497, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 427, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 309, in forward\n",
      "    value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 36.38 MiB is free. Process 5390 has 13.79 GiB memory in use. Including non-PyTorch memory, this process has 9.50 GiB memory in use. Of the allocated memory 9.13 GiB is allocated by PyTorch, and 70.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "comparing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regression_year\n\u001b[0;32m----> 3\u001b[0m regression_year(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuning_hugging_whitespace-finetuned-imdb/checkpoint-2919750\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswerick_subsetdata_date_train100.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/swerick/evaluation.py:191\u001b[0m, in \u001b[0;36mregression_year\u001b[0;34m(model_filename, data_path)\u001b[0m\n\u001b[1;32m    188\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command2, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# Wait for the process to finish and get the output\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Print the output\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stdout\u001b[38;5;241m.\u001b[39mdecode())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from evaluation import regression_year\n",
    "\n",
    "regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-2919750\",\"swerick_subsetdata_date_train100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import regression_year\n",
    "with open(\"comparison_results.txt\", \"w\") :\n",
    "        pass\n",
    "for i in range (10):\n",
    "    train_set = subset_random(party_dataset[\"train\"],1000)\n",
    "    df=train_set.to_pandas()\n",
    "    df=df.rename(columns={\"Note\":\"content\", \"date_scaled\" : \"tag\"})\n",
    "    df.to_csv(\"swerick_subsetdata_date_train_robust.csv\")\n",
    "    regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-3148750\",\"swerick_subsetdata_date_train_robust.csv\")\n",
    "\n",
    "\n",
    "losses_model2, r2_model2 = [], []\n",
    "losses_model1, r2_model1 = [], []\n",
    "with open(\"comparison_results.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        loss_model1, r2_model1_val, loss_model2, r2_model2_val = line.strip().split(',')\n",
    "        losses_model1.append(float(loss_model1))\n",
    "        r2_model1.append(float(r2_model1_val))\n",
    "\n",
    "        losses_model2.append(float(loss_model2))\n",
    "        r2_model2.append(float(r2_model2_val))\n",
    "\n",
    "print(sum(losses_model1)/len(losses_model1))\n",
    "print(sum(losses_model2)/len(losses_model2))\n",
    "print(sum(r2_model1)/len(r2_model1))\n",
    "print(sum(r2_model2)/len(r2_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def r2_score(outputs, labels):\n",
    "    predictions = outputs.logits.squeeze()\n",
    "    labels_mean = torch.mean(labels.float())\n",
    "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
    "    ss_res = torch.sum((labels - predictions) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for ix, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['content'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['tag'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, valid_r2 = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].float().to(device)\n",
    "        output = model(input_ids,token_type_ids=None,attention_mask=input_mask,labels=labels)\n",
    "        loss +=output.loss.item()\n",
    "        r2 = r2_score(output, labels)\n",
    "        valid_r2.append(r2.item())\n",
    "        \n",
    "    r2 = torch.mean(torch.tensor(valid_r2))\n",
    "    return loss, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd \n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"trained/regression_date\",\n",
    "    num_labels=1,\n",
    ").to(device)\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"trained/regression_date_hugging_face\",\n",
    "    num_labels=1).to(device)\n",
    "\n",
    "df = pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "input_ids, attention_masks, labels = encode(df, tokenizer)\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "test_loader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=16,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "loss1,r2=evaluate(model1,test_loader)\n",
    "loss2,r22=evaluate(model2,test_loader)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"\\nLoss model 1:\", loss1 * 16/ len(test_loader))\n",
    "print(\"\\nR2 model1:\",torch.mean(torch.tensor(r2)))\n",
    "\n",
    "\n",
    "print(\"\\nLoss model 2:\", loss2* 16 / len(test_loader))\n",
    "print(\"\\nR2 model2:\",torch.mean(torch.tensor(r22)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --data_path \"swerick_subsetdata_party_train.csv\" --label_names $label_names_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --model_filename \"trained_hugging_face_party_classification\" --base_model \"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-343500\" --data_path \"swerick_subsetdata_party_train.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
