{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import argparse\n",
    "import preprocessing\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at finetuning_hugging_python-finetuned-imdb/checkpoint-920384 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"finetuning_hugging_python-finetuned-imdb/checkpoint-920384\")\n",
    "model_finetuned=model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_NaN(subset,example):\n",
    "    return example[subset] is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(dataset,nb_obs,label_name):\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Calculer le nombre d'observations pour chaque étiquette\n",
    "    grouped_data = df.groupby(label_name)\n",
    "\n",
    "    # Calculer le nombre d'observations par étiquette pour obtenir une répartition uniforme\n",
    "    total_samples = nb_obs\n",
    "    samples_per_label = total_samples // len(grouped_data.groups)\n",
    "\n",
    "    # Créer une liste pour stocker les observations échantillonnées\n",
    "    sampled_data = []\n",
    "\n",
    "    # Prélever aléatoirement les observations pour chaque groupe de label\n",
    "    for group_label, group_data in grouped_data.groups.items():\n",
    "        group_dataset=dataset.select(group_data)\n",
    "        label_data = group_dataset.shuffle(seed=np.random.randint(1, 1000)).select(range(min(len(group_data), samples_per_label)))\n",
    "        sampled_data.extend(label_data)\n",
    "\n",
    "    # Mélanger les observations pour obtenir un ordre aléatoire\n",
    "    np.random.shuffle(sampled_data)\n",
    "\n",
    "    # Créer un Dataset Hugging Face à partir des observations échantillonnées\n",
    "    sampled_dataset = Dataset.from_dict({key: [example[key] for example in sampled_data] for key in sampled_data[0]})\n",
    "    \n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"Note\"],padding=True, truncation=True,max_length=512)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, accuracy = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        output = model(input_ids,\n",
    "            token_type_ids=None, \n",
    "            attention_mask=input_mask, \n",
    "            labels=labels)\n",
    "        loss += output.loss.item()\n",
    "        preds_batch = torch.argmax(output.logits, axis=1)\n",
    "        batch_acc = torch.mean((preds_batch == labels).float())\n",
    "        accuracy.append(batch_acc)\n",
    "        \n",
    "    accuracy = torch.mean(torch.tensor(accuracy))\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 3378877\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 725974\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"swerick_data_party_train.pkl\", \"test\": \"swerick_data_party_test.pkl\"}\n",
    "party_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    valid: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 725974\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"valid\": \"swerick_data_party_valid.pkl\"}\n",
    "party_valid_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b991a36849324a03bfb5584bdd3a12f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3167502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1991215b1ca94e7389a0b6cb84496c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/676000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].filter(lambda x : filter_NaN(\"party\",x))\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ff2b908ee14272994d2810f204a75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/676000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'man', 1: 'woman'}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(party_dataset[\"train\"][\"gender\"])\n",
    "label_names = label_encoder.classes_\n",
    "label_dict={ i : label_names[i] for i in  range(len(label_names))}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man' 'woman']\n"
     ]
    }
   ],
   "source": [
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels_gender.pkl\", \"wb\") as fp:   \n",
    "   pickle.dump(label_names, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"vänstern\"', 'Andra kammarens center', 'Andra kammarens frihandelsparti', 'Bondeförbundet', 'Centern (partigrupp 1873-1882)', 'Centern (partigrupp 1885-1887)', 'Centerpartiet', 'Det förenade högerpartiet', 'Ehrenheimska partiet', 'Folkpartiet', 'Folkpartiet (1895–1900)', 'Friesenska diskussionsklubben', 'Frihandelsvänliga centern', 'Frisinnade folkpartiet', 'Frisinnade försvarsvänner', 'Frisinnade landsföreningen', 'Första kammarens konservativa grupp', 'Första kammarens ministeriella grupp', 'Första kammarens minoritetsparti', 'Första kammarens moderata parti', 'Första kammarens nationella parti', 'Första kammarens protektionistiska parti', 'Gamla lantmannapartiet', 'Högerns riksdagsgrupp', 'Högerpartiet', 'Högerpartiet de konservativa', 'Jordbrukarnas fria grupp', 'Junkerpartiet', 'Kilbomspartiet', 'Kommunistiska partiet', 'Kristdemokraterna', 'Lantmanna- och borgarepartiet inom andrakammaren', 'Lantmannapartiet', 'Lantmannapartiets filial', 'Liberala riksdagspartiet', 'Liberala samlingspartiet', 'Liberalerna', 'Medborgerlig samling (1964–1968)', 'Miljöpartiet', 'Moderaterna', 'Nationella framstegspartiet', 'Ny demokrati', 'Nya centern (partigrupp 1883-1887)', 'Nya lantmannapartiet', 'Nyliberala partiet', 'Skånska partiet', 'Socialdemokraterna', 'Socialdemokratiska vänstergruppen', 'Socialistiska partiet', 'Stockholmsbänken', 'Sverigedemokraterna', 'Sveriges kommunistiska parti', 'Vänsterpartiet', 'borgmästarepartiet', 'frihandelsvänlig vilde', 'frisinnad vilde', 'högervilde', 'ministeriella partiet', 'partilös', 'politisk vilde', 'vänstervilde']\n"
     ]
    }
   ],
   "source": [
    "with open(\"labels.pkl\",\"rb\") as f :\n",
    "    label_names=pickle.load(f)\n",
    "\n",
    "print(label_names.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd66216ff8b7498b82f317f5fa70a787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3167750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb7d5b1dc7343d29302a461e6bb589a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/676215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b08effd0de948f9adad4a48eea250c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/676215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_party_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m party_train_datasets \u001b[38;5;241m=\u001b[39m subset(party_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;241m5000\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m party_test_datasets \u001b[38;5;241m=\u001b[39m subset(party_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;241m5000\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m party_valid_datasets \u001b[38;5;241m=\u001b[39m subset(party_valid_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;241m5000\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m, in \u001b[0;36msubset\u001b[0;34m(dataset, nb_obs, label_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculer le nombre d'observations pour chaque étiquette\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m grouped_data \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mgroupby(label_name)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculer le nombre d'observations par étiquette pour obtenir une répartition uniforme\u001b[39;00m\n\u001b[1;32m      8\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m nb_obs\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "party_train_datasets = subset(party_dataset[\"train\"],5000,\"party\")\n",
    "party_test_datasets = subset(party_dataset[\"test\"],5000,\"party\")\n",
    "party_valid_datasets = subset(party_valid_dataset[\"valid\"],5000,\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j,m=random.randint(0,len(party_train_datasets)),random.randint(0,len(party_test_datasets)),random.randint(0,len(party_valid_datasets))\n",
    "train_set = party_train_datasets[i]\n",
    "test_set = party_test_datasets[j]\n",
    "valid_set = party_valid_datasets[m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(party_train_datasets)\n",
    "print(party_test_datasets)\n",
    "print(party_valid_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset = concatenate_datasets([train_set,test_set,valid_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf0c5d568d14d3187098a196fc89053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44069f30059b48a69af1fdc14a547fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc8a737ea74bd9b4f5d799b1562c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['protocole', 'Note', 'id', 'party', 'gender', 'party_labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets = train_set.map(tokenize_function,batched=True )\n",
    "tokenized_test_datasets = test_set.map(tokenize_function,batched=True )\n",
    "tokenized_valid_datasets = valid_set.map(tokenize_function,batched=True )\n",
    "tokenized_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_test_datasets=tokenized_test_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_test_datasets=tokenized_test_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_test_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_valid_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers=4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        tokenized_train_datasets,\n",
    "        shuffle=True,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "        tokenized_valid_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "# Not used atm\n",
    "test_loader = DataLoader(\n",
    "        tokenized_test_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "n_epochs =10\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(label_dict),\n",
    "        id2label=label_dict).to(\"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "num_training_steps = len(train_loader) * n_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "\n",
    "# Linear warmup and step decay\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 starts!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e676a3017ebb4c0aa370cd38b4da83a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b8de45011b4fdca04cf6f15bc8ac5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m valid_loss, valid_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader)\n\u001b[1;32m     31\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     32\u001b[0m valid_losses\u001b[38;5;241m.\u001b[39mappend(valid_loss)\n",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)):\n\u001b[0;32m----> 5\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m     input_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "device=\"cpu\"\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch} starts!\")\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "        model.zero_grad()   \n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        output = model(input_ids,\n",
    "                token_type_ids=None, \n",
    "                attention_mask=input_mask, \n",
    "                labels=labels)\n",
    "        loss = output.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    train_loss_avg = train_loss * batch_size / len(train_loader)\n",
    "    valid_loss_avg = valid_loss * batch_size / len(valid_loader)\n",
    "\n",
    "    print(f'Training Loss: {train_loss_avg:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss_avg:.3f}')\n",
    "    print(f'Validation accuracy: {valid_accuracy}')\n",
    "\n",
    "    # Store best model\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        print(\"Best validation loss so far\")\n",
    "        best_valid_loss = valid_loss\n",
    " \n",
    "    else:\n",
    "            print(\"Not the best validation loss so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58640bf698e843c3aabdd833e9d43487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "valid_loss, valid_accuracy = evaluate(model, valid_loader)\n",
    "\n",
    "train_losses.append(train_loss)\n",
    "valid_losses.append(valid_loss)\n",
    "\n",
    "train_loss_avg = train_loss * batch_size / len(train_loader)\n",
    "valid_loss_avg = valid_loss * batch_size / len(valid_loader)\n",
    "\n",
    "print(f'Training Loss: {train_loss_avg:.3f}')\n",
    "print(f'Validation Loss: {valid_loss_avg:.3f}')\n",
    "print(f'Validation accuracy: {valid_accuracy}')\n",
    "\n",
    "# Store best model\n",
    "\n",
    "if valid_loss < best_valid_loss:\n",
    "    print(\"Best validation loss so far\")\n",
    "    best_valid_loss = valid_loss\n",
    "else:\n",
    "        print(\"Not the best validation loss so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_pickle(\"swerick_data_party_train.pkl\")\n",
    "df = df.rename(columns={\"Note\":\"content\",\"party\" : \"tag\"})\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['party']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12983/2349304386.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"swerick_data_party_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"party\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6414\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6415\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6416\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6418\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6419\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ['party']"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"swerick_data_party_train.csv\")\n",
    "df =df.dropna(subset=\"tag\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.dropna(subset=\"tag\")\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train_set.to_pandas()\n",
    "df = df.rename(columns={\"Note\":\"content\",\"gender\" : \"tag\"})\n",
    "df.to_csv(\"swerick_subsetdata_gender_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = sorted(list(set(df[\"tag\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"vänstern\"', 'Andra kammarens center',\n",
       "       'Andra kammarens frihandelsparti', 'Bondeförbundet',\n",
       "       'Centern (partigrupp 1873-1882)', 'Centern (partigrupp 1885-1887)',\n",
       "       'Centerpartiet', 'Det förenade högerpartiet',\n",
       "       'Ehrenheimska partiet', 'Folkpartiet', 'Folkpartiet (1895–1900)',\n",
       "       'Friesenska diskussionsklubben', 'Frihandelsvänliga centern',\n",
       "       'Frisinnade folkpartiet', 'Frisinnade försvarsvänner',\n",
       "       'Frisinnade landsföreningen',\n",
       "       'Första kammarens konservativa grupp',\n",
       "       'Första kammarens ministeriella grupp',\n",
       "       'Första kammarens minoritetsparti',\n",
       "       'Första kammarens moderata parti',\n",
       "       'Första kammarens nationella parti',\n",
       "       'Första kammarens protektionistiska parti',\n",
       "       'Gamla lantmannapartiet', 'Högerns riksdagsgrupp', 'Högerpartiet',\n",
       "       'Högerpartiet de konservativa', 'Jordbrukarnas fria grupp',\n",
       "       'Junkerpartiet', 'Kilbomspartiet', 'Kommunistiska partiet',\n",
       "       'Kristdemokraterna',\n",
       "       'Lantmanna- och borgarepartiet inom andrakammaren',\n",
       "       'Lantmannapartiet', 'Lantmannapartiets filial',\n",
       "       'Liberala riksdagspartiet', 'Liberala samlingspartiet',\n",
       "       'Liberalerna', 'Medborgerlig samling (1964–1968)', 'Miljöpartiet',\n",
       "       'Moderaterna', 'Nationella framstegspartiet', 'Ny demokrati',\n",
       "       'Nya centern (partigrupp 1883-1887)', 'Nya lantmannapartiet',\n",
       "       'Nyliberala partiet', 'Skånska partiet', 'Socialdemokraterna',\n",
       "       'Socialdemokratiska vänstergruppen', 'Socialistiska partiet',\n",
       "       'Stockholmsbänken', 'Sverigedemokraterna',\n",
       "       'Sveriges kommunistiska parti', 'Vänsterpartiet',\n",
       "       'borgmästarepartiet', 'frihandelsvänlig vilde', 'frisinnad vilde',\n",
       "       'högervilde', 'ministeriella partiet', 'partilös',\n",
       "       'politisk vilde', 'vänstervilde'], dtype='<U48')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (643114865.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    $label_names_str\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Convertir la liste de noms de libellés en une chaîne séparée par des espaces avec chaque nom entouré de guillemets simples\n",
    "label_names_str = \" \".join([f'\"{name}\"' for name in label_names.tolist()])\n",
    "\n",
    "$label_names_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vänstern Andra kammarens center Andra kammarens frihandelsparti Bondeförbundet Centern (partigrupp 1873-1882) Centern (partigrupp 1885-1887) Centerpartiet Det förenade högerpartiet Ehrenheimska partiet Folkpartiet Folkpartiet (1895–1900) Friesenska diskussionsklubben Frihandelsvänliga centern Frisinnade folkpartiet Frisinnade försvarsvänner Frisinnade landsföreningen Första kammarens konservativa grupp Första kammarens ministeriella grupp Första kammarens minoritetsparti Första kammarens moderata parti Första kammarens nationella parti Första kammarens protektionistiska parti Gamla lantmannapartiet Högerns riksdagsgrupp Högerpartiet Högerpartiet de konservativa Jordbrukarnas fria grupp Junkerpartiet Kilbomspartiet Kommunistiska partiet Kristdemokraterna Lantmanna- och borgarepartiet inom andrakammaren Lantmannapartiet Lantmannapartiets filial Liberala riksdagspartiet Liberala samlingspartiet Liberalerna Medborgerlig samling (1964–1968) Miljöpartiet Moderaterna Nationella framstegspartiet Ny demokrati Nya centern (partigrupp 1883-1887) Nya lantmannapartiet Nyliberala partiet Skånska partiet Socialdemokraterna Socialdemokratiska vänstergruppen Socialistiska partiet Stockholmsbänken Sverigedemokraterna Sveriges kommunistiska parti Vänsterpartiet borgmästarepartiet frihandelsvänlig vilde frisinnad vilde högervilde ministeriella partiet partilös politisk vilde vänstervilde\n"
     ]
    }
   ],
   "source": [
    "!echo $label_names_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vänstern', 'Andra kammarens center', 'Andra kammarens frihandelsparti', 'Bondeförbundet', 'Centern (partigrupp 1873-1882)', 'Centern (partigrupp 1885-1887)', 'Centerpartiet', 'Det förenade högerpartiet', 'Ehrenheimska partiet', 'Folkpartiet', 'Folkpartiet (1895–1900)', 'Friesenska diskussionsklubben', 'Frihandelsvänliga centern', 'Frisinnade folkpartiet', 'Frisinnade försvarsvänner', 'Frisinnade landsföreningen', 'Första kammarens konservativa grupp', 'Första kammarens ministeriella grupp', 'Första kammarens minoritetsparti', 'Första kammarens moderata parti', 'Första kammarens nationella parti', 'Första kammarens protektionistiska parti', 'Gamla lantmannapartiet', 'Högerns riksdagsgrupp', 'Högerpartiet', 'Högerpartiet de konservativa', 'Jordbrukarnas fria grupp', 'Junkerpartiet', 'Kilbomspartiet', 'Kommunistiska partiet', 'Kristdemokraterna', 'Lantmanna- och borgarepartiet inom andrakammaren', 'Lantmannapartiet', 'Lantmannapartiets filial', 'Liberala riksdagspartiet', 'Liberala samlingspartiet', 'Liberalerna', 'Medborgerlig samling (1964–1968)', 'Miljöpartiet', 'Moderaterna', 'Nationella framstegspartiet', 'Ny demokrati', 'Nya centern (partigrupp 1883-1887)', 'Nya lantmannapartiet', 'Nyliberala partiet', 'Skånska partiet', 'Socialdemokraterna', 'Socialdemokratiska vänstergruppen', 'Socialistiska partiet', 'Stockholmsbänken', 'Sverigedemokraterna', 'Sveriges kommunistiska parti', 'Vänsterpartiet', 'borgmästarepartiet', 'frihandelsvänlig vilde', 'frisinnad vilde', 'högervilde', 'ministeriella partiet', 'partilös', 'politisk vilde', 'vänstervilde']\n",
      "{0: 'vänstern', 1: 'Andra kammarens center', 2: 'Andra kammarens frihandelsparti', 3: 'Bondeförbundet', 4: 'Centern (partigrupp 1873-1882)', 5: 'Centern (partigrupp 1885-1887)', 6: 'Centerpartiet', 7: 'Det förenade högerpartiet', 8: 'Ehrenheimska partiet', 9: 'Folkpartiet', 10: 'Folkpartiet (1895–1900)', 11: 'Friesenska diskussionsklubben', 12: 'Frihandelsvänliga centern', 13: 'Frisinnade folkpartiet', 14: 'Frisinnade försvarsvänner', 15: 'Frisinnade landsföreningen', 16: 'Första kammarens konservativa grupp', 17: 'Första kammarens ministeriella grupp', 18: 'Första kammarens minoritetsparti', 19: 'Första kammarens moderata parti', 20: 'Första kammarens nationella parti', 21: 'Första kammarens protektionistiska parti', 22: 'Gamla lantmannapartiet', 23: 'Högerns riksdagsgrupp', 24: 'Högerpartiet', 25: 'Högerpartiet de konservativa', 26: 'Jordbrukarnas fria grupp', 27: 'Junkerpartiet', 28: 'Kilbomspartiet', 29: 'Kommunistiska partiet', 30: 'Kristdemokraterna', 31: 'Lantmanna- och borgarepartiet inom andrakammaren', 32: 'Lantmannapartiet', 33: 'Lantmannapartiets filial', 34: 'Liberala riksdagspartiet', 35: 'Liberala samlingspartiet', 36: 'Liberalerna', 37: 'Medborgerlig samling (1964–1968)', 38: 'Miljöpartiet', 39: 'Moderaterna', 40: 'Nationella framstegspartiet', 41: 'Ny demokrati', 42: 'Nya centern (partigrupp 1883-1887)', 43: 'Nya lantmannapartiet', 44: 'Nyliberala partiet', 45: 'Skånska partiet', 46: 'Socialdemokraterna', 47: 'Socialdemokratiska vänstergruppen', 48: 'Socialistiska partiet', 49: 'Stockholmsbänken', 50: 'Sverigedemokraterna', 51: 'Sveriges kommunistiska parti', 52: 'Vänsterpartiet', 53: 'borgmästarepartiet', 54: 'frihandelsvänlig vilde', 55: 'frisinnad vilde', 56: 'högervilde', 57: 'ministeriella partiet', 58: 'partilös', 59: 'politisk vilde', 60: 'vänstervilde'}\n",
      "61\n",
      "\u001b[32m17:05:14 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m17:05:15 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m17:05:15 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([57, 57, 57, 57, 57, 32, 32, 32, 33, 32, 32, 57, 32, 32, 32, 32, 57, 32,\n",
      "        21, 57, 57, 57, 43, 57, 57, 57, 57, 57, 32, 32, 32, 57, 22, 32, 57, 32,\n",
      "        32, 32, 57, 57, 57, 57, 32, 32, 57, 57, 32, 22, 32, 57, 57, 57, 57, 57,\n",
      "        57, 32, 43, 57, 32, 32, 57, 32, 57, 32, 57, 57, 57, 32, 33, 57, 57, 57,\n",
      "        22, 32, 57, 32, 32, 57, 32, 32, 57, 22, 57, 57, 32, 32, 57, 32, 57, 57,\n",
      "        22, 32, 44, 32, 33, 57, 57, 57, 57, 32, 57, 57, 57, 32, 32, 57, 57, 57,\n",
      "        57, 57, 32, 32, 32, 57, 32, 33, 32, 32, 43, 57, 33, 57, 32, 32, 57, 57,\n",
      "        57, 32, 32, 57, 57, 32, 32, 57, 57, 32, 43, 57, 32, 57, 57, 57, 57, 57,\n",
      "        57, 57, 57, 57, 33, 57, 33, 57, 57, 32, 32, 57, 32, 57, 32, 57, 57, 32,\n",
      "        32, 57, 32, 44, 57, 57, 57, 32, 57, 32, 57, 33, 32, 22, 57, 57, 32, 32,\n",
      "        32, 57, 32, 22, 57, 57, 44, 57, 57, 22, 33, 57, 32, 57, 57, 57, 32, 57,\n",
      "        57, 57, 32, 32, 44, 57, 32, 32, 57, 57, 32, 57, 32, 57, 32, 32, 33, 57,\n",
      "        32, 57, 57, 57, 32, 57, 57, 32, 32, 32, 57, 57, 22, 57, 57, 22, 57, 57,\n",
      "        32, 57, 21, 43, 57, 57, 57, 32, 32, 32, 57, 43, 57, 32, 32, 43, 33, 22,\n",
      "        57, 32, 57, 33, 32, 57, 32, 32, 21, 57, 43, 57, 57, 57, 57, 57, 57, 57,\n",
      "        57, 32, 57, 22, 57, 57, 57,  5, 57, 32, 32, 57, 32, 32, 57, 57, 32, 57,\n",
      "        57, 57, 57, 32, 32, 57, 57, 57, 32, 32, 57, 32, 32, 32, 32, 57, 57, 32,\n",
      "        43, 32, 57, 32, 57, 43, 57, 43, 32, 57, 57, 32, 32, 32, 32, 32, 32, 57,\n",
      "        32, 32, 32, 57, 32, 57, 57, 57, 32, 44, 57, 32, 57, 32, 32, 32, 57, 32,\n",
      "        32, 32, 32, 57, 32, 33, 57, 57, 32, 32, 32, 57, 57, 57, 57, 57, 57, 32,\n",
      "        32, 57, 57, 57, 32, 32, 32, 32, 57, 57, 57, 57, 32, 32, 32, 32, 57, 57,\n",
      "        57, 57, 32, 32, 57, 32, 57, 57, 32, 33, 57, 32, 57, 32, 57, 32, 57, 57,\n",
      "        32, 32, 57, 32, 57, 57, 57, 57, 57, 32, 57, 57, 57, 57, 32, 32, 57, 32,\n",
      "        57, 57, 57, 57, 32, 32, 32, 57, 32, 57, 57, 43, 57, 57, 57, 32, 32, 57,\n",
      "        33, 57, 57, 32, 57, 22, 57, 21, 32, 57, 57, 32, 57, 32, 57, 32, 32, 57,\n",
      "        32,  5, 32, 57, 32, 57, 57, 57, 32, 32, 32, 44, 32, 32, 22, 57, 57, 57,\n",
      "        57, 32, 57, 32, 57, 32, 57, 32, 57, 33, 32, 57, 57, 32, 57, 57, 32, 57,\n",
      "        57, 57, 32, 32, 57, 57, 57, 32, 32, 57, 57, 57, 57, 57, 57, 57, 57, 57,\n",
      "        21, 57, 57, 22, 32, 57, 32, 32, 57, 32, 57, 32, 57, 57, 57, 32, 32, 57,\n",
      "        57, 32, 57, 32, 57, 57, 57, 32, 57, 32, 57, 57, 57, 32, 43, 32, 43, 32,\n",
      "        32, 32, 57, 32, 32, 32, 57, 57, 57, 57, 32, 32, 32, 43, 57, 57, 57, 57,\n",
      "        32, 32, 57, 32, 32, 57, 32, 57, 22, 57, 57, 57, 57, 32, 57, 57, 57, 57,\n",
      "        43, 57, 57, 32, 32, 57, 32, 57, 32, 22, 57, 57, 57, 57, 57, 33, 33, 32,\n",
      "        32, 57, 32, 32, 32, 57, 43, 57, 57, 57, 57, 57, 32, 32, 57, 32, 22, 32,\n",
      "        32, 21, 57, 43, 57, 57, 22, 57, 57, 32, 32, 32, 32, 32, 32, 57, 57, 33,\n",
      "        32, 57, 57, 32, 57, 32, 57, 57, 57, 22, 32, 32, 57, 32, 57, 32, 57, 57,\n",
      "        57, 57, 32, 32, 57, 57, 57, 57, 57, 32, 32, 57, 57, 32, 57, 32, 32, 57,\n",
      "        32, 22, 57, 57, 32, 32, 32, 57, 57, 32, 57, 57, 57, 57, 57, 32, 57, 32,\n",
      "        57, 33, 57, 57, 32, 57, 57, 32, 32, 32, 57, 32, 22, 32, 57, 57, 32, 57,\n",
      "        57, 32, 32, 32, 57, 32, 57, 32, 57, 57, 32, 32, 32, 57, 57, 43, 57, 32,\n",
      "        57, 32, 57, 57, 32, 32, 32, 32, 57, 32, 32, 33, 57, 32, 32, 57, 32, 57,\n",
      "        32, 32, 32,  5, 22, 57, 32, 22, 57, 32, 32, 57, 57, 57, 32, 32, 32, 22,\n",
      "        43, 32, 32, 57, 57, 32, 32, 57, 32, 43, 32, 57, 32, 43, 32, 57, 32, 57,\n",
      "        22, 57, 32, 57, 32, 32, 57, 22, 32, 57, 57, 43, 32, 57, 32, 32, 57, 57,\n",
      "        57, 22, 32, 32, 57, 57, 32, 57, 57, 32, 32, 57, 57, 32, 32, 57, 32, 57,\n",
      "        57, 57, 32, 32, 32, 32, 57, 32, 33, 57, 57, 57, 32, 57, 57, 32, 33, 32,\n",
      "        57, 57, 57, 57, 33, 32, 32, 32, 33, 32, 32, 57, 32, 57, 43, 32, 57, 32,\n",
      "        32, 32, 57, 32, 32, 32, 57, 32, 57, 57, 57, 57, 32, 32, 57, 57, 44, 44,\n",
      "        43, 32, 32, 57, 32, 32, 57, 32, 32, 32, 57, 32, 57, 32, 32, 57, 57, 57,\n",
      "        22, 57, 32, 57, 57, 57, 32, 32, 32, 57, 57, 57, 32, 32, 57, 57, 32, 32,\n",
      "        32, 57, 32, 32, 57, 57, 32, 57, 57, 32, 57, 57, 22, 57, 32, 32, 57, 32,\n",
      "        57, 32, 57, 32, 57, 57, 42, 32, 57, 57, 57, 32, 44, 32, 32, 57, 57, 57,\n",
      "        57, 32, 32, 57, 57, 32, 32, 32, 32, 57, 57, 22, 57, 32, 22, 57, 57, 57,\n",
      "        57, 57, 57, 32, 32, 43, 32, 57, 43, 32, 43, 57, 32, 57, 32, 32, 57, 32,\n",
      "        57, 57, 32, 32, 57, 57, 57, 22, 57, 32, 32, 57, 57, 57, 57, 57, 57, 57,\n",
      "        57, 57, 57, 32, 57, 22, 32, 57, 32, 32])\u001b[0m\n",
      "/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m17:05:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "  0%|                                                    | 0/38 [00:00<?, ?it/s]/home/laurinemeier/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|███████████████████████████████████████████| 38/38 [00:12<00:00,  3.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.90it/s]\n",
      "\u001b[34m17:05:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 47.128\u001b[0m\n",
      "\u001b[34m17:05:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 28.447\u001b[0m\n",
      "\u001b[34m17:05:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4038461446762085\u001b[0m\n",
      "\u001b[32m17:05:30 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m17:05:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.33it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.90it/s]\n",
      "\u001b[34m17:05:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 22.468\u001b[0m\n",
      "\u001b[34m17:05:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 22.018\u001b[0m\n",
      "\u001b[34m17:05:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4711538553237915\u001b[0m\n",
      "\u001b[32m17:05:44 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m17:05:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.02it/s]\n",
      "\u001b[34m17:05:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 19.520\u001b[0m\n",
      "\u001b[34m17:05:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 20.844\u001b[0m\n",
      "\u001b[34m17:05:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5288461446762085\u001b[0m\n",
      "\u001b[32m17:05:58 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m17:05:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.34it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.35it/s]\n",
      "\u001b[34m17:06:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 18.452\u001b[0m\n",
      "\u001b[34m17:06:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 20.550\u001b[0m\n",
      "\u001b[34m17:06:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.48557692766189575\u001b[0m\n",
      "\u001b[32m17:06:12 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m17:06:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.37it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.98it/s]\n",
      "\u001b[34m17:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 17.561\u001b[0m\n",
      "\u001b[34m17:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 20.772\u001b[0m\n",
      "\u001b[34m17:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.48557692766189575\u001b[0m\n",
      "\u001b[32m17:06:27 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m17:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.27it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.68it/s]\n",
      "\u001b[34m17:06:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 16.116\u001b[0m\n",
      "\u001b[34m17:06:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 20.223\u001b[0m\n",
      "\u001b[34m17:06:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5336538553237915\u001b[0m\n",
      "\u001b[32m17:06:40 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m17:06:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:12<00:00,  3.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.77it/s]\n",
      "\u001b[34m17:06:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 13.925\u001b[0m\n",
      "\u001b[34m17:06:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 22.105\u001b[0m\n",
      "\u001b[34m17:06:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5480769276618958\u001b[0m\n",
      "\u001b[32m17:06:55 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m17:06:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.18it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.67it/s]\n",
      "\u001b[34m17:07:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.071\u001b[0m\n",
      "\u001b[34m17:07:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 21.373\u001b[0m\n",
      "\u001b[34m17:07:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5769230723381042\u001b[0m\n",
      "\u001b[32m17:07:09 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m17:07:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:12<00:00,  3.15it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.64it/s]\n",
      "\u001b[34m17:07:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.793\u001b[0m\n",
      "\u001b[34m17:07:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 22.511\u001b[0m\n",
      "\u001b[34m17:07:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.567307710647583\u001b[0m\n",
      "\u001b[32m17:07:22 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m17:07:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:12<00:00,  3.16it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.43it/s]\n",
      "\u001b[34m17:07:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.834\u001b[0m\n",
      "\u001b[34m17:07:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 22.872\u001b[0m\n",
      "\u001b[34m17:07:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5528846383094788\u001b[0m\n",
      "\u001b[32m17:07:36 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 train_binary_bert.py --data_path \"swerick_subsetdata_party_train.csv\" --label_names $label_names_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:31:33 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m14:31:33 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m14:31:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([8, 8, 8, 8, 8, 3, 3, 3, 4, 3, 3, 8, 3, 3, 3, 3, 8, 3, 1, 8, 8, 8, 6, 8,\n",
      "        8, 8, 8, 8, 3, 3, 3, 8, 2, 3, 8, 3, 3, 3, 8, 8, 8, 8, 3, 3, 8, 8, 3, 2,\n",
      "        3, 8, 8, 8, 8, 8, 8, 3, 6, 8, 3, 3, 8, 3, 8, 3, 8, 8, 8, 3, 4, 8, 8, 8,\n",
      "        2, 3, 8, 3, 3, 8, 3, 3, 8, 2, 8, 8, 3, 3, 8, 3, 8, 8, 2, 3, 7, 3, 4, 8,\n",
      "        8, 8, 8, 3, 8, 8, 8, 3, 3, 8, 8, 8, 8, 8, 3, 3, 3, 8, 3, 4, 3, 3, 6, 8,\n",
      "        4, 8, 3, 3, 8, 8, 8, 3, 3, 8, 8, 3, 3, 8, 8, 3, 6, 8, 3, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 4, 8, 4, 8, 8, 3, 3, 8, 3, 8, 3, 8, 8, 3, 3, 8, 3, 7, 8, 8,\n",
      "        8, 3, 8, 3, 8, 4, 3, 2, 8, 8, 3, 3, 3, 8, 3, 2, 8, 8, 7, 8, 8, 2, 4, 8,\n",
      "        3, 8, 8, 8, 3, 8, 8, 8, 3, 3, 7, 8, 3, 3, 8, 8, 3, 8, 3, 8, 3, 3, 4, 8,\n",
      "        3, 8, 8, 8, 3, 8, 8, 3, 3, 3, 8, 8, 2, 8, 8, 2, 8, 8, 3, 8, 1, 6, 8, 8,\n",
      "        8, 3, 3, 3, 8, 6, 8, 3, 3, 6, 4, 2, 8, 3, 8, 4, 3, 8, 3, 3, 1, 8, 6, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 3, 8, 2, 8, 8, 8, 0, 8, 3, 3, 8, 3, 3, 8, 8, 3, 8,\n",
      "        8, 8, 8, 3, 3, 8, 8, 8, 3, 3, 8, 3, 3, 3, 3, 8, 8, 3, 6, 3, 8, 3, 8, 6,\n",
      "        8, 6, 3, 8, 8, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3, 8, 3, 8, 8, 8, 3, 7, 8, 3,\n",
      "        8, 3, 3, 3, 8, 3, 3, 3, 3, 8, 3, 4, 8, 8, 3, 3, 3, 8, 8, 8, 8, 8, 8, 3,\n",
      "        3, 8, 8, 8, 3, 3, 3, 3, 8, 8, 8, 8, 3, 3, 3, 3, 8, 8, 8, 8, 3, 3, 8, 3,\n",
      "        8, 8, 3, 4, 8, 3, 8, 3, 8, 3, 8, 8, 3, 3, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8,\n",
      "        8, 8, 3, 3, 8, 3, 8, 8, 8, 8, 3, 3, 3, 8, 3, 8, 8, 6, 8, 8, 8, 3, 3, 8,\n",
      "        4, 8, 8, 3, 8, 2, 8, 1, 3, 8, 8, 3, 8, 3, 8, 3, 3, 8, 3, 0, 3, 8, 3, 8,\n",
      "        8, 8, 3, 3, 3, 7, 3, 3, 2, 8, 8, 8, 8, 3, 8, 3, 8, 3, 8, 3, 8, 4, 3, 8,\n",
      "        8, 3, 8, 8, 3, 8, 8, 8, 3, 3, 8, 8, 8, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        1, 8, 8, 2, 3, 8, 3, 3, 8, 3, 8, 3, 8, 8, 8, 3, 3, 8, 8, 3, 8, 3, 8, 8,\n",
      "        8, 3, 8, 3, 8, 8, 8, 3, 6, 3, 6, 3, 3, 3, 8, 3, 3, 3, 8, 8, 8, 8, 3, 3,\n",
      "        3, 6, 8, 8, 8, 8, 3, 3, 8, 3, 3, 8, 3, 8, 2, 8, 8, 8, 8, 3, 8, 8, 8, 8,\n",
      "        6, 8, 8, 3, 3, 8, 3, 8, 3, 2, 8, 8, 8, 8, 8, 4, 4, 3, 3, 8, 3, 3, 3, 8,\n",
      "        6, 8, 8, 8, 8, 8, 3, 3, 8, 3, 2, 3, 3, 1, 8, 6, 8, 8, 2, 8, 8, 3, 3, 3,\n",
      "        3, 3, 3, 8, 8, 4, 3, 8, 8, 3, 8, 3, 8, 8, 8, 2, 3, 3, 8, 3, 8, 3, 8, 8,\n",
      "        8, 8, 3, 3, 8, 8, 8, 8, 8, 3, 3, 8, 8, 3, 8, 3, 3, 8, 3, 2, 8, 8, 3, 3,\n",
      "        3, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 3, 8, 4, 8, 8, 3, 8, 8, 3, 3, 3, 8, 3,\n",
      "        2, 3, 8, 8, 3, 8, 8, 3, 3, 3, 8, 3, 8, 3, 8, 8, 3, 3, 3, 8, 8, 6, 8, 3,\n",
      "        8, 3, 8, 8, 3, 3, 3, 3, 8, 3, 3, 4, 8, 3, 3, 8, 3, 8, 3, 3, 3, 0, 2, 8,\n",
      "        3, 2, 8, 3, 3, 8, 8, 8, 3, 3, 3, 2, 6, 3, 3, 8, 8, 3, 3, 8, 3, 6, 3, 8,\n",
      "        3, 6, 3, 8, 3, 8, 2, 8, 3, 8, 3, 3, 8, 2, 3, 8, 8, 6, 3, 8, 3, 3, 8, 8,\n",
      "        8, 2, 3, 3, 8, 8, 3, 8, 8, 3, 3, 8, 8, 3, 3, 8, 3, 8, 8, 8, 3, 3, 3, 3,\n",
      "        8, 3, 4, 8, 8, 8, 3, 8, 8, 3, 4, 3, 8, 8, 8, 8, 4, 3, 3, 3, 4, 3, 3, 8,\n",
      "        3, 8, 6, 3, 8, 3, 3, 3, 8, 3, 3, 3, 8, 3, 8, 8, 8, 8, 3, 3, 8, 8, 7, 7,\n",
      "        6, 3, 3, 8, 3, 3, 8, 3, 3, 3, 8, 3, 8, 3, 3, 8, 8, 8, 2, 8, 3, 8, 8, 8,\n",
      "        3, 3, 3, 8, 8, 8, 3, 3, 8, 8, 3, 3, 3, 8, 3, 3, 8, 8, 3, 8, 8, 3, 8, 8,\n",
      "        2, 8, 3, 3, 8, 3, 8, 3, 8, 3, 8, 8, 5, 3, 8, 8, 8, 3, 7, 3, 3, 8, 8, 8,\n",
      "        8, 3, 3, 8, 8, 3, 3, 3, 3, 8, 8, 2, 8, 3, 2, 8, 8, 8, 8, 8, 8, 3, 3, 6,\n",
      "        3, 8, 6, 3, 6, 8, 3, 8, 3, 3, 8, 3, 8, 8, 3, 3, 8, 8, 8, 2, 8, 3, 3, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 2, 3, 8, 3, 3])\u001b[0m\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at finetuning_hugging_whitespace-finetuned-imdb/checkpoint-343500 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m14:31:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:12<00:00,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.17it/s]\n",
      "\u001b[34m14:31:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 25.688\u001b[0m\n",
      "\u001b[34m14:31:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 18.777\u001b[0m\n",
      "\u001b[34m14:31:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4759615361690521\u001b[0m\n",
      "\u001b[32m14:31:49 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:31:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.48it/s]\n",
      "\u001b[34m14:32:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 18.494\u001b[0m\n",
      "\u001b[34m14:32:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 18.382\u001b[0m\n",
      "\u001b[34m14:32:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4759615361690521\u001b[0m\n",
      "\u001b[32m14:32:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:32:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.32it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.27it/s]\n",
      "\u001b[34m14:32:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 18.258\u001b[0m\n",
      "\u001b[34m14:32:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 17.982\u001b[0m\n",
      "\u001b[34m14:32:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4711538553237915\u001b[0m\n",
      "\u001b[32m14:32:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:32:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.27it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.29it/s]\n",
      "\u001b[34m14:32:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 17.579\u001b[0m\n",
      "\u001b[34m14:32:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 17.463\u001b[0m\n",
      "\u001b[34m14:32:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.504807710647583\u001b[0m\n",
      "\u001b[32m14:32:31 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:32:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:12<00:00,  3.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  7.89it/s]\n",
      "\u001b[34m14:32:46 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 16.482\u001b[0m\n",
      "\u001b[34m14:32:46 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 17.295\u001b[0m\n",
      "\u001b[34m14:32:46 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5384615659713745\u001b[0m\n",
      "\u001b[32m14:32:46 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:32:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.32it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.49it/s]\n",
      "\u001b[34m14:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 14.681\u001b[0m\n",
      "\u001b[34m14:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 17.537\u001b[0m\n",
      "\u001b[34m14:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5480769276618958\u001b[0m\n",
      "\u001b[32m14:33:01 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.28it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.29it/s]\n",
      "\u001b[34m14:33:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.804\u001b[0m\n",
      "\u001b[34m14:33:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 19.999\u001b[0m\n",
      "\u001b[34m14:33:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.49038460850715637\u001b[0m\n",
      "\u001b[32m14:33:14 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:33:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.33it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.40it/s]\n",
      "\u001b[34m14:33:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.140\u001b[0m\n",
      "\u001b[34m14:33:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 17.909\u001b[0m\n",
      "\u001b[34m14:33:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.557692289352417\u001b[0m\n",
      "\u001b[32m14:33:27 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:33:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.51it/s]\n",
      "\u001b[34m14:33:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.879\u001b[0m\n",
      "\u001b[34m14:33:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 18.765\u001b[0m\n",
      "\u001b[34m14:33:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5865384340286255\u001b[0m\n",
      "\u001b[32m14:33:39 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:33:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00,  8.32it/s]\n",
      "\u001b[34m14:33:52 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.990\u001b[0m\n",
      "\u001b[34m14:33:52 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 18.964\u001b[0m\n",
      "\u001b[34m14:33:52 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5961538553237915\u001b[0m\n",
      "\u001b[32m14:33:52 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 train_binary_bert.py --model_filename \"trained_hugging_face_party_classification\" --base_model \"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-343500\" --data_path \"swerick_subsetdata_party_train.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
