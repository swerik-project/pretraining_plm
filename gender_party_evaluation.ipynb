{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import argparse\n",
    "import preprocessing\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"finetuning_hugging_python-finetuned-imdb/checkpoint-920384\")\n",
    "model_finetuned=model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_NaN(subset,example):\n",
    "    return example[subset] is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_label(dataset,nb_obs,label_name):\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Calculer le nombre d'observations pour chaque étiquette\n",
    "    grouped_data = df.groupby(label_name)\n",
    "\n",
    "    # Calculer le nombre d'observations par étiquette pour obtenir une répartition uniforme\n",
    "    total_samples = nb_obs\n",
    "    samples_per_label = total_samples // len(grouped_data.groups)\n",
    "\n",
    "    # Créer une liste pour stocker les observations échantillonnées\n",
    "    sampled_data = []\n",
    "\n",
    "    # Prélever aléatoirement les observations pour chaque groupe de label\n",
    "    for group_label, group_data in grouped_data.groups.items():\n",
    "        group_dataset=dataset.select(group_data)\n",
    "        label_data = group_dataset.shuffle(seed=np.random.randint(1, 1000)).select(range(min(len(group_data), samples_per_label)))\n",
    "        sampled_data.extend(label_data)\n",
    "\n",
    "    # Mélanger les observations pour obtenir un ordre aléatoire\n",
    "    np.random.shuffle(sampled_data)\n",
    "\n",
    "    # Créer un Dataset Hugging Face à partir des observations échantillonnées\n",
    "    sampled_dataset = Dataset.from_dict({key: [example[key] for example in sampled_data] for key in sampled_data[0]})\n",
    "    \n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_random(dataset,nb_ob):\n",
    "    dataset=dataset.shuffle()\n",
    "    echantillon_aleatoire = dataset.select(range(nb_ob))\n",
    "    return echantillon_aleatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"Note\"],padding=True, truncation=True,max_length=512)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, accuracy = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        output = model(input_ids,\n",
    "            token_type_ids=None, \n",
    "            attention_mask=input_mask, \n",
    "            labels=labels)\n",
    "        loss += output.loss.item()\n",
    "        preds_batch = torch.argmax(output.logits, axis=1)\n",
    "        batch_acc = torch.mean((preds_batch == labels).float())\n",
    "        accuracy.append(batch_acc)\n",
    "        \n",
    "    accuracy = torch.mean(torch.tensor(accuracy))\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(protocole):\n",
    "    match = re.search(r'/(\\d+)/', protocole)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        return int(year[:4])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"swerick_data_party_train.pkl\", \"test\": \"swerick_data_party_test.pkl\"}\n",
    "party_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [extract_date_from_filename(row['protocole']) for row in party_dataset['train']]\n",
    "dates_test = [extract_date_from_filename(row['protocole']) for row in party_dataset['test']]\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date', dates)\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date', dates_test)\n",
    "\n",
    "print(party_dataset[\"train\"][0])\n",
    "print(party_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"valid\": \"swerick_data_party_valid.pkl\"}\n",
    "party_valid_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_valid = [extract_date_from_filename(row['protocole']) for row in party_valid_dataset['valid']]\n",
    "party_valid_dataset['valid'] = party_valid_dataset['valid'].add_column('date', dates_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "date_scaler = StandardScaler()\n",
    "dates_train_2d = [[date] for date in party_dataset['train'][\"date\"]]\n",
    "dates_test_2d=[[date] for date in party_dataset['test'][\"date\"]]\n",
    "date_scaler.fit(dates_train_2d)\n",
    "dates_train =date_scaler.transform(dates_train_2d)\n",
    "dates_test =date_scaler.transform(dates_test_2d)\n",
    "print(dates_train)\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date_scaled', dates_train.squeeze())\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date_scaled', dates_test.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].filter(lambda x : filter_NaN(\"party\",x))\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(party_dataset[\"train\"][\"gender\"])\n",
    "label_names = label_encoder.classes_\n",
    "label_dict={ i : label_names[i] for i in  range(len(label_names))}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels_gender.pkl\", \"wb\") as fp:   \n",
    "   pickle.dump(label_names, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.pkl\",\"rb\") as f :\n",
    "    label_names=pickle.load(f)\n",
    "\n",
    "print(label_names.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_party_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_train_datasets = subset_label(party_dataset[\"train\"],5000,\"party\")\n",
    "party_test_datasets = subset_label(party_dataset[\"test\"],5000,\"party\")\n",
    "party_valid_datasets = subset_label(party_valid_dataset[\"valid\"],5000,\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1 = subset_random(party_dataset[\"train\"],100)\n",
    "train_set2 = subset_random(party_dataset[\"train\"],200)\n",
    "train_set3 = subset_random(party_dataset[\"train\"],500)\n",
    "train_set4= subset_random(party_dataset[\"train\"],1000)\n",
    "test_set = subset_random(party_dataset[\"test\"],10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(party_train_datasets)\n",
    "print(party_test_datasets)\n",
    "print(party_valid_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset = concatenate_datasets([train_set,test_set,valid_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets = train_set.map(tokenize_function,batched=True )\n",
    "tokenized_test_datasets = test_set.map(tokenize_function,batched=True )\n",
    "tokenized_valid_datasets = valid_set.map(tokenize_function,batched=True )\n",
    "tokenized_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_test_datasets=tokenized_test_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_test_datasets=tokenized_test_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_test_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_valid_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers=4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        tokenized_train_datasets,\n",
    "        shuffle=True,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "        tokenized_valid_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "# Not used atm\n",
    "test_loader = DataLoader(\n",
    "        tokenized_test_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs =10\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(label_dict),\n",
    "        id2label=label_dict).to(\"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "num_training_steps = len(train_loader) * n_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "\n",
    "# Linear warmup and step decay\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_pickle(\"swerick_data_party_train.pkl\")\n",
    "df = df.rename(columns={\"Note\":\"content\",\"party\" : \"tag\"})\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "print(type(df[\"tag\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.dropna(subset=\"tag\")\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test_set.to_pandas()\n",
    "print(df)\n",
    "df = df.rename(columns={\"Note\":\"content\", \"date_scaled\" : \"tag\"})\n",
    "print(df)\n",
    "df.to_csv(\"swerick_subsetdata_date_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_regression.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_regression.py --base_model \"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\" --model_filename \"trained/regression_date_hugging_face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 compare_models_regression.py --model_filename1 \"trained/regression_date\" --model_filename2 \"trained/regression_date_hugging_face\" --data_path \"swerick_subsetdata_date_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import regression_year\n",
    "\n",
    "regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-2919750\",\"swerick_subsetdata_date_train1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import regression_year\n",
    "with open(\"comparison_results.txt\", \"w\") :\n",
    "        pass\n",
    "for i in range (10):\n",
    "    train_set = subset_random(party_dataset[\"train\"],1000)\n",
    "    df=train_set.to_pandas()\n",
    "    df=df.rename(columns={\"Note\":\"content\", \"date_scaled\" : \"tag\"})\n",
    "    df.to_csv(\"swerick_subsetdata_date_train_robust.csv\")\n",
    "    regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-3148750\",\"swerick_subsetdata_date_train_robust.csv\")\n",
    "\n",
    "\n",
    "losses_model2, r2_model2 = [], []\n",
    "losses_model1, r2_model1 = [], []\n",
    "with open(\"comparison_results.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        loss_model1, r2_model1_val, loss_model2, r2_model2_val = line.strip().split(',')\n",
    "        losses_model1.append(float(loss_model1))\n",
    "        r2_model1.append(float(r2_model1_val))\n",
    "\n",
    "        losses_model2.append(float(loss_model2))\n",
    "        r2_model2.append(float(r2_model2_val))\n",
    "\n",
    "print(sum(losses_model1)/len(losses_model1))\n",
    "print(sum(losses_model2)/len(losses_model2))\n",
    "print(sum(r2_model1)/len(r2_model1))\n",
    "print(sum(r2_model2)/len(r2_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def r2_score(outputs, labels):\n",
    "    predictions = outputs.logits.squeeze()\n",
    "    labels_mean = torch.mean(labels.float())\n",
    "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
    "    ss_res = torch.sum((labels - predictions) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for ix, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['content'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['tag'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, valid_r2 = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].float().to(device)\n",
    "        output = model(input_ids,token_type_ids=None,attention_mask=input_mask,labels=labels)\n",
    "        loss +=output.loss.item()\n",
    "        r2 = r2_score(output, labels)\n",
    "        valid_r2.append(r2.item())\n",
    "        \n",
    "    r2 = torch.mean(torch.tensor(valid_r2))\n",
    "    return loss, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd \n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"trained/regression_date\",\n",
    "    num_labels=1,\n",
    ").to(device)\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"trained/regression_date_hugging_face\",\n",
    "    num_labels=1).to(device)\n",
    "\n",
    "df = pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "input_ids, attention_masks, labels = encode(df, tokenizer)\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "test_loader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=16,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "loss1,r2=evaluate(model1,test_loader)\n",
    "loss2,r22=evaluate(model2,test_loader)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"\\nLoss model 1:\", loss1 * 16/ len(test_loader))\n",
    "print(\"\\nR2 model1:\",torch.mean(torch.tensor(r2)))\n",
    "\n",
    "\n",
    "print(\"\\nLoss model 2:\", loss2* 16 / len(test_loader))\n",
    "print(\"\\nR2 model2:\",torch.mean(torch.tensor(r22)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --data_path \"swerick_subsetdata_party_train.csv\" --label_names $label_names_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --model_filename \"trained_hugging_face_party_classification\" --base_model \"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-343500\" --data_path \"swerick_subsetdata_party_train.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
