{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from data_swerick import create_dataset_swerick\n",
    "from evaluation import evaluation_task,regression_year\n",
    "import preprocessing\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch,data_collator):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model = preprocessing.create_model_MLM(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               protocole  \\\n",
      "0      data/1867/prot-1867--ak--0118.xml   \n",
      "1      data/1867/prot-1867--ak--0121.xml   \n",
      "2      data/1867/prot-1867--ak--0123.xml   \n",
      "3      data/1867/prot-1867--ak--0124.xml   \n",
      "4      data/1867/prot-1867--ak--0125.xml   \n",
      "...                                  ...   \n",
      "12394   data/202122/prot-202122--135.xml   \n",
      "12395   data/202122/prot-202122--137.xml   \n",
      "12396   data/202122/prot-202122--138.xml   \n",
      "12397   data/202122/prot-202122--141.xml   \n",
      "12398   data/202122/prot-202122--142.xml   \n",
      "\n",
      "                                                   texte  \n",
      "0      Sedan, i kraft af Rikets Regeringsform, lagtim...  \n",
      "1      Den 21 Januari. 25 mande af tiden för dessa va...  \n",
      "2      Den 23 Januari. - 55 Onsdagen den 23 Januari. ...  \n",
      "3      60 Den 24 Januari, f. m, Thorsdagen den 24 Jan...  \n",
      "4      66 Den 25 Januari. N:o 12, med delgifvande af ...  \n",
      "...                                                  ...  \n",
      "12394  § 1 Justering av protokoll Protokollet för den...  \n",
      "12395  § 1 Justering av protokoll Protokollen för den...  \n",
      "12396  § 1 Justering av protokoll Protokollet för den...  \n",
      "12397  § 1 Anmälan om återtagande av plats i riksdage...  \n",
      "12398  § 1 Anmälan om subsidiaritetsprövningar Talman...  \n",
      "\n",
      "[12399 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset1=pd.read_pickle(\"swerick_data_random_train.pkl\")\n",
    "dataset2=pd.read_pickle(\"swerick_data_random_valid.pkl\")\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50ccc6e64d048ed8009b5b08b7dd569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ac0eec4364c0c8bbf7d5e5b5547e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 12399\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 2673\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#datasest\n",
    "data_files = {\"train\": \"swerick_data_random_train.pkl\", \"test\": \"swerick_data_random_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets =preprocessing.tokenize_dataset(swerick_dataset,tokenizer)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tokenized_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets=tokenized_datasets.remove_columns(\"protocole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm_datasets \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mgrouping_dataset(tokenized_datasets,chunk_size)\n\u001b[1;32m      2\u001b[0m lm_datasets\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "lm_datasets = preprocessing.grouping_dataset(tokenized_datasets,chunk_size)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lm_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid={\"valid\":\"swerick_data_random_valid.pkl\"}\n",
    "valid_dataset = load_dataset(\"pandas\",data_files=data_valid) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtokenize_dataset(valid_dataset,tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "valid_dataset =preprocessing.tokenize_dataset(valid_dataset,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=preprocessing.grouping_dataset(valid_dataset,chunk_size)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"wb\") as f:\n",
    "     pickle.dump(valid_dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 762794\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\",\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial with a manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "    num_rows: 3663965\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"])\n",
    "\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset[\"valid\"],data_collator)\n",
    "\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)\n",
    "print(\"ok\")\n",
    "\n",
    "#for batch in train_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "#for batch in eval_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(eval_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataloader.dataset)\n",
    "print(eval_dataloader)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_ids\"].device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    train =DataLoader(\n",
    "    lm_dataset_bis[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n",
    "    train = [inputs.to(device) for inputs in train_dataloader]\n",
    "    return train\n",
    "\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bis = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_bis=model_bis.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bis.eval()\n",
    "\n",
    "total_loss = 0.0  # Variable to accumulate total loss\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bis(**batch)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()   # Accumulate the batch loss\n",
    "\n",
    "# Calculate the average loss\n",
    "average_loss = total_loss / len(eval_dataloader)\n",
    "\n",
    "print(f\"Initial Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model_bis.parameters(), lr=1.3e-5)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "losses_train=[]\n",
    "losses_test=[]\n",
    "#train_dataloader = get_dataloader()\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model_bis.train()\n",
    "    print(next(model_bis.parameters()).device)\n",
    "    print(epoch)\n",
    "    params_before_optimization = [param.data.clone() for param in model_bis.parameters()]\n",
    "    total_loss_train = 0.0 \n",
    "    train_dataloader = get_dataloader()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model_bis(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        params_after_optimization = [param.data for param in model_bis.parameters()]\n",
    "        parameters_changed = any((param_before != param_after).any() for param_before, param_after in zip(params_before_optimization, params_after_optimization))\n",
    "        #if parameters_changed==True :\n",
    "             # print(parameters_changed) \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    losses_train.append(total_loss_train/len(train_dataloader))\n",
    "    print(\"losses_train\",losses_train)\n",
    "\n",
    "    # Evaluation\n",
    "    model_bis.eval()\n",
    "    losses=[]\n",
    "    total_loss_eval=0.0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bis(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        total_loss_eval +=loss.item()\n",
    "\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "       perplexity = float(\"inf\")\n",
    "\n",
    "    losses_test.append(total_loss_eval/len(eval_dataloader))\n",
    "\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    print(\"losses_test\",losses_test)\n",
    "\n",
    "print(\"epoch\",num_train_epochs)\n",
    "plt.plot(range(num_train_epochs),losses_train,label=\"train Loss\")\n",
    "\n",
    "plt.plot(range(num_train_epochs),losses_test,label=\"test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_train)\n",
    "print(losses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"finetuning_manual\"\n",
    "model_bis.save_pretrained(file_path)\n",
    "tokenizer.save_pretrained(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"losses.pkl\"\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump({'losses_train': losses_train, 'losses_test': losses_test}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"./test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_long=AutoModelForMaskedLM.from_pretrained(\"./finetuning_hugging-finetuned-imdb/checkpoint-259384\")\n",
    "model_long=model_long.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(\"./test_model\")\n",
    "model=model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-4179250\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_kb=AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_task(model_hugging_face,valid_dataloader,\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-4179250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_task(model,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(0.13192342221736908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données à partir du fichier JSON\n",
    "with open(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-4179250/trainer_state.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epoch_train = []\n",
    "epoch_test=[]\n",
    "\n",
    "for entry in data['log_history']:\n",
    "    if 'loss' in entry:\n",
    "        train_loss.append(entry['loss'])\n",
    "        epoch_train.append((entry['epoch']))\n",
    "    elif 'eval_loss' in entry:\n",
    "        eval_loss.append(entry['eval_loss'])\n",
    "        epoch_test.append((entry['epoch']))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_train, train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epoch_test, eval_loss, label='Evaluation Loss', marker='o')\n",
    "plt.title('Training and Evaluation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id= tokenizer.pad_token_id\n",
    "sep_token_id = tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_token(model,dataloader):\n",
    "    model.eval()\n",
    "    correct_pred=[]\n",
    "    incorrect_pred=[]\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        with torch.no_grad():\n",
    "            outputs=model(**batch)\n",
    "        predictions=torch.argmax(outputs.logits,dim=-1)\n",
    "        indices_tokens_masked = torch.nonzero(batch[\"labels\"] != -100, as_tuple=False)\n",
    "        correct_indices=[]\n",
    "        incorrect_indices=[]\n",
    "        for id,label in enumerate(batch[\"labels\"][indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]]):\n",
    "            if label.item() == predictions[indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]][id]:\n",
    "                correct_indices.append(id)\n",
    "            else :\n",
    "                incorrect_indices.append(id)\n",
    "        correct_pred.extend(batch['labels'][indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]][correct_indices])\n",
    "        incorrect_pred.extend(batch['labels'][indices_tokens_masked[:, 0], indices_tokens_masked[:, 1]][incorrect_indices])\n",
    "\n",
    "    return correct_pred,incorrect_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_correct,hugging_face_incorrect = evaluate_model_token(model_hugging_face,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+',\n",
       " '6',\n",
       " '«',\n",
       " '?',\n",
       " 'J',\n",
       " ',',\n",
       " 'å',\n",
       " 'y',\n",
       " 'ë',\n",
       " '$',\n",
       " 'E',\n",
       " 'k',\n",
       " 'U',\n",
       " 'á',\n",
       " \"'\",\n",
       " 'Y',\n",
       " 'f',\n",
       " 'w',\n",
       " '>',\n",
       " 'm',\n",
       " 'G',\n",
       " '_',\n",
       " '2',\n",
       " '&',\n",
       " 'O',\n",
       " 'n',\n",
       " 'i',\n",
       " 'ö',\n",
       " 'ä',\n",
       " 'I',\n",
       " '£',\n",
       " ':',\n",
       " '-',\n",
       " '.',\n",
       " 'd',\n",
       " 'D',\n",
       " '*',\n",
       " 'V',\n",
       " '[',\n",
       " ']',\n",
       " 'ç',\n",
       " 'A',\n",
       " 'Q',\n",
       " '!',\n",
       " 'l',\n",
       " 'K',\n",
       " 'é',\n",
       " 'h',\n",
       " 'q',\n",
       " 'à',\n",
       " 'b',\n",
       " 'T',\n",
       " 'P',\n",
       " 'c',\n",
       " 'N',\n",
       " '8',\n",
       " 'g',\n",
       " 'ü',\n",
       " '§',\n",
       " '%',\n",
       " 'L',\n",
       " 'Z',\n",
       " '=',\n",
       " 'r',\n",
       " '4',\n",
       " 'a',\n",
       " '\"',\n",
       " '#',\n",
       " '—',\n",
       " 'ã',\n",
       " ';',\n",
       " '1',\n",
       " 'W',\n",
       " 'S',\n",
       " 'É',\n",
       " 'M',\n",
       " 'H',\n",
       " 'R',\n",
       " ')',\n",
       " 'j',\n",
       " 'F',\n",
       " 'Ö',\n",
       " 'Ä',\n",
       " 'z',\n",
       " 'ø',\n",
       " '|',\n",
       " 'Å',\n",
       " '€',\n",
       " '(',\n",
       " 'è',\n",
       " 'x',\n",
       " '»',\n",
       " 'B',\n",
       " 's',\n",
       " '0',\n",
       " 'v',\n",
       " 'Ó',\n",
       " 't',\n",
       " '5',\n",
       " '<',\n",
       " 'ó',\n",
       " ' ',\n",
       " 'X',\n",
       " 'e',\n",
       " 'u',\n",
       " '9',\n",
       " '3',\n",
       " 'p',\n",
       " 'o',\n",
       " 'C',\n",
       " '/',\n",
       " '7']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_face_incorrect_unique = list(set(hugging_face_incorrect))\n",
    "hugging_face_incorrect_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_correct,hugging_incorrect=evaluate_model_token(model_hugging_face,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_correct,base_incorrect=evaluate_model_token(model_kb,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def decoding_text(list):\n",
    "    counter = Counter(list)\n",
    "\n",
    "    # Trier les éléments par leur fréquence décroissante\n",
    "    sorted_numbers = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extraire les chiffres triés\n",
    "    unique_sorted_numbers = [num for num, _ in sorted_numbers]\n",
    "\n",
    "    decoded_texts = []\n",
    "    for tensor in unique_sorted_numbers:\n",
    "        decoded_text = tokenizer.decode(tensor.item())\n",
    "        decoded_texts.append(decoded_text)\n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_base_correct = decoding_text(base_correct)\n",
    "decoded_hugging_correct = decoding_text(hugging_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['flät',\n",
       " '##oarer',\n",
       " 'FAST',\n",
       " '##tain',\n",
       " 'köttbullar',\n",
       " 'Stadens',\n",
       " '##omter',\n",
       " 'gled',\n",
       " 'Smart',\n",
       " 'Längd',\n",
       " '##vita',\n",
       " 'lättad',\n",
       " 'Up',\n",
       " 'sankt',\n",
       " '##smaskin',\n",
       " '##etecken',\n",
       " 'Eget',\n",
       " '##stjärnor',\n",
       " '##icers',\n",
       " '##bero',\n",
       " '##IER',\n",
       " '##alom',\n",
       " '##frusna',\n",
       " '##butik',\n",
       " 'invester',\n",
       " 'avsiktligt',\n",
       " '##ANN',\n",
       " 'Europat',\n",
       " 'Måndag',\n",
       " 'vandringen',\n",
       " 'that',\n",
       " 'Örgryte',\n",
       " '##ändskt',\n",
       " 'Alex',\n",
       " 'nybyggt',\n",
       " '##växel',\n",
       " '##aton',\n",
       " 'Burton',\n",
       " '##YK',\n",
       " '##ugget',\n",
       " '##artan',\n",
       " 'östern',\n",
       " 'Globen',\n",
       " 'Mood',\n",
       " '##ward',\n",
       " 'avveck',\n",
       " '##ales',\n",
       " '##avd',\n",
       " '##såsen',\n",
       " '##miral',\n",
       " 'grytan',\n",
       " 'vänskapen',\n",
       " '##harmon',\n",
       " '##ssäsongen',\n",
       " '##embl',\n",
       " 'Raf',\n",
       " 'Stein',\n",
       " 'avhandlingen',\n",
       " 'uppn',\n",
       " 'musikdirektör',\n",
       " '##ografier',\n",
       " 'trampade',\n",
       " '##less',\n",
       " 'Cirk',\n",
       " '##igan',\n",
       " '##127',\n",
       " '##oplan',\n",
       " 'Teresa',\n",
       " '##ipa',\n",
       " 'skotten',\n",
       " 'sun',\n",
       " 'mästar',\n",
       " '##sutställningen',\n",
       " 'Emilia',\n",
       " 'Österb',\n",
       " '##ävlan',\n",
       " '##skiv',\n",
       " '668',\n",
       " 'uppen',\n",
       " '##merna',\n",
       " 'gamm',\n",
       " '##rukna',\n",
       " '##lekar',\n",
       " '##gnost',\n",
       " '##oning',\n",
       " '786',\n",
       " '##provinsen',\n",
       " '749',\n",
       " 'Hopkins',\n",
       " 'GL',\n",
       " '##RF',\n",
       " 'grädd',\n",
       " 'Ambul',\n",
       " '##ätare',\n",
       " '##fönster',\n",
       " '##uvan',\n",
       " '##löpande',\n",
       " '##utex',\n",
       " '##flöj',\n",
       " 'Järvs',\n",
       " 'publikt',\n",
       " '##hett',\n",
       " 'Augusta',\n",
       " '##köttet',\n",
       " 'enl',\n",
       " 'korsar',\n",
       " 'frodas',\n",
       " 'baken',\n",
       " 'Rust',\n",
       " 'egenhändigt',\n",
       " '050',\n",
       " '##sökte',\n",
       " 'motgång',\n",
       " 'Wret',\n",
       " '##moln',\n",
       " 'deporter',\n",
       " '##spänning',\n",
       " 'nyöpp',\n",
       " '##amet',\n",
       " '##ungan',\n",
       " 'Cec',\n",
       " 'ugg',\n",
       " '##sholmen',\n",
       " 'jättelika',\n",
       " '##cia',\n",
       " 'övergiven',\n",
       " 'svängarna',\n",
       " 'Chans',\n",
       " '##66',\n",
       " '##sborgs',\n",
       " '##artor',\n",
       " '##eci',\n",
       " 'tab',\n",
       " '##unch',\n",
       " 'samlare',\n",
       " '709',\n",
       " 'Wes',\n",
       " 'Monta',\n",
       " '##skaja',\n",
       " '##ynes',\n",
       " 'Endre',\n",
       " '##sven',\n",
       " '##glöm',\n",
       " 'arbet',\n",
       " 'käng',\n",
       " '##ÖK',\n",
       " 'Wen',\n",
       " 'Petit',\n",
       " 'mv',\n",
       " '##85',\n",
       " '##analysen',\n",
       " 'tjuvarna',\n",
       " '##haus',\n",
       " 'Tell',\n",
       " '##rakten',\n",
       " 'efterträda',\n",
       " 'foton',\n",
       " '##sprin',\n",
       " '##skylt',\n",
       " 'Clar',\n",
       " 'angel',\n",
       " 'astrona',\n",
       " '##reation',\n",
       " '##attar',\n",
       " 'Rydell',\n",
       " 'Kostar',\n",
       " '##nick',\n",
       " 'Nordqvist',\n",
       " '##ktis',\n",
       " 'Hirsch',\n",
       " 'egyptiska',\n",
       " 'Bonniers',\n",
       " 'galen',\n",
       " '##edöm',\n",
       " '##ologerna',\n",
       " '##italet',\n",
       " '##lagor',\n",
       " '##ussar',\n",
       " '##ADE',\n",
       " 'illustrationer',\n",
       " 'förkn',\n",
       " 'Aut',\n",
       " '##jten',\n",
       " 'Tf',\n",
       " 'tha',\n",
       " '##mir',\n",
       " 'Klipp',\n",
       " 'Said',\n",
       " 'Kro',\n",
       " '##ther',\n",
       " 'Matts',\n",
       " '##more',\n",
       " 'Gere',\n",
       " '##hjulet',\n",
       " 'Reyn',\n",
       " 'Eskilst',\n",
       " 'Philadelphia',\n",
       " 'klostret',\n",
       " '##järna',\n",
       " 'Ander',\n",
       " '##makaren',\n",
       " 'Friday',\n",
       " '##hos',\n",
       " 'äkten',\n",
       " 'Orm',\n",
       " '##nette',\n",
       " 'krigar',\n",
       " 'knän',\n",
       " '755',\n",
       " 'välplanerade',\n",
       " 'Db',\n",
       " '##skontakt',\n",
       " 'krama',\n",
       " 'misstänktes',\n",
       " 'Peters',\n",
       " '##kvällen',\n",
       " 'Stephen',\n",
       " 'kvarstod',\n",
       " '##rädgården',\n",
       " 'ingivas',\n",
       " '##ässan',\n",
       " 'daghemmet',\n",
       " '##rockar',\n",
       " '##ION',\n",
       " '##artjänsten',\n",
       " '##cirkel',\n",
       " 'backarna',\n",
       " 'Chansen',\n",
       " 'protestantiska',\n",
       " 'soul',\n",
       " 'AIK',\n",
       " 'made',\n",
       " '##fester',\n",
       " '##Sk',\n",
       " 'Valborg',\n",
       " '##minen',\n",
       " '##sgar',\n",
       " '##räffande',\n",
       " 'Vit',\n",
       " '##52',\n",
       " '##ofog',\n",
       " 'lättskött',\n",
       " '757',\n",
       " '##yrkom',\n",
       " 'Alexis',\n",
       " 'puff',\n",
       " 'insänd',\n",
       " 'Lang',\n",
       " 'hektiska',\n",
       " '##IEN',\n",
       " 'kista',\n",
       " '##ÅR',\n",
       " 'bister',\n",
       " '##saft',\n",
       " '##anch',\n",
       " '##smästaren',\n",
       " '##ippen',\n",
       " 'intermezz',\n",
       " 'tjugofem',\n",
       " 'aw',\n",
       " '##smatch',\n",
       " 'bokstä',\n",
       " 'Hedem',\n",
       " '##ankt',\n",
       " 'torra',\n",
       " '##ublic',\n",
       " 'lagets',\n",
       " 'BEK',\n",
       " 'brittiskt',\n",
       " '##ertig',\n",
       " 'Förresten',\n",
       " 'tav',\n",
       " '##estation',\n",
       " '##aså',\n",
       " '##icia',\n",
       " 'irländ',\n",
       " '##värr',\n",
       " 'Smör',\n",
       " '##edrag',\n",
       " 'bortglömda',\n",
       " 'administ',\n",
       " '##bloms',\n",
       " 'nedanstående',\n",
       " '##eley',\n",
       " '##åkare',\n",
       " 'käns',\n",
       " '##jerg',\n",
       " '##bbats',\n",
       " 'doc',\n",
       " 'formuleras',\n",
       " '##ronic',\n",
       " 'Google',\n",
       " 'font',\n",
       " '##leverantör',\n",
       " '##stadt',\n",
       " 'envis',\n",
       " '##agat',\n",
       " 'Bean',\n",
       " '##finnas',\n",
       " 'reag',\n",
       " 'aman',\n",
       " '##andagen',\n",
       " 'Palmer',\n",
       " 'braskamin',\n",
       " 'lår',\n",
       " 'befordrades',\n",
       " '##sker',\n",
       " '##eled',\n",
       " '##EET',\n",
       " '##ollar',\n",
       " 'Silv',\n",
       " '##ornet',\n",
       " '##sutställningar',\n",
       " 'Barcelona',\n",
       " 'kyr',\n",
       " '##hålet',\n",
       " 'Stenson',\n",
       " 'Takt',\n",
       " 'Freeman',\n",
       " 'Hallström',\n",
       " '##ÅNG',\n",
       " 'Cop',\n",
       " 'Ping',\n",
       " '##igenom',\n",
       " '##anas',\n",
       " 'andades',\n",
       " '##ifik',\n",
       " 'People',\n",
       " '##uel',\n",
       " '##öjt',\n",
       " 'Stenbeck',\n",
       " '##itteringen',\n",
       " 'tillbehör',\n",
       " '##skara',\n",
       " 'fle',\n",
       " 'Pool',\n",
       " '##eig',\n",
       " 'BAK',\n",
       " '##smiljön',\n",
       " 'Maja',\n",
       " 'Limhamn',\n",
       " 'Palmgren',\n",
       " '##fisken',\n",
       " 'äkt',\n",
       " 'Jes',\n",
       " 'sjunk',\n",
       " 'Corp',\n",
       " '##skommunikation',\n",
       " '##yle',\n",
       " 'Solv',\n",
       " '##ogie',\n",
       " '##östra',\n",
       " 'Kronprinsessan',\n",
       " 'Filmen',\n",
       " '##karlen',\n",
       " '##amo',\n",
       " 'skakande',\n",
       " 'tyget',\n",
       " '##ucka',\n",
       " 'katoliker',\n",
       " '##198',\n",
       " '##itos',\n",
       " 'häd',\n",
       " '##useets',\n",
       " 'Hew',\n",
       " '##rond',\n",
       " '##åverkan',\n",
       " '##imerat',\n",
       " '##repren',\n",
       " 'Eneby',\n",
       " 'präktig',\n",
       " '##eker',\n",
       " '##ures',\n",
       " 'plattan',\n",
       " '712',\n",
       " '##ÖS',\n",
       " '##plattor',\n",
       " 'Grat',\n",
       " '##rader',\n",
       " '##ettes',\n",
       " 'Ps',\n",
       " '##179',\n",
       " 'Tåget',\n",
       " '##ioter',\n",
       " '##spis',\n",
       " 'Rivière',\n",
       " 'Ces',\n",
       " 'Dana',\n",
       " 'Ond',\n",
       " 'barnvänligt',\n",
       " '##stidningen',\n",
       " 'Bjerk',\n",
       " 'Sat',\n",
       " '##iren',\n",
       " 'Osa',\n",
       " 'karri',\n",
       " 'försvunnen',\n",
       " '##ember',\n",
       " 'Häcken',\n",
       " '##uddar',\n",
       " '##olika',\n",
       " 'Fridhem',\n",
       " 'påpek',\n",
       " '##vänder',\n",
       " '##sleden',\n",
       " 'tillträd',\n",
       " '##lir',\n",
       " '##ejo',\n",
       " 'Lönneberga',\n",
       " '##ansök',\n",
       " '##andar',\n",
       " 'Out',\n",
       " 'Mort',\n",
       " '##iansen',\n",
       " '##liss',\n",
       " 'Häl',\n",
       " 'sekunden',\n",
       " '##talien',\n",
       " '##någ',\n",
       " '##lay',\n",
       " '##NING',\n",
       " 'EFTER',\n",
       " '##zzo',\n",
       " '##akes',\n",
       " 'Ladd',\n",
       " '##ply',\n",
       " '##veriet',\n",
       " 'uppsatser',\n",
       " 'Hipp',\n",
       " '##skytt',\n",
       " 'Hud',\n",
       " 'djär',\n",
       " '##ville',\n",
       " '##skjuten',\n",
       " 'vap',\n",
       " 'Beat',\n",
       " 'Skicka',\n",
       " 'Jones',\n",
       " '##gäst',\n",
       " '##lum',\n",
       " 'Snäll',\n",
       " 'Lagerbäck',\n",
       " 'Tjugo',\n",
       " '##43',\n",
       " 'Corn',\n",
       " 'Anläggningen',\n",
       " 'Stellan',\n",
       " '##Ne',\n",
       " '##spur',\n",
       " '##mart',\n",
       " 'Fart',\n",
       " '##segling',\n",
       " 'Dell',\n",
       " 'Bremen',\n",
       " '##150',\n",
       " '##aug',\n",
       " '##hyllor',\n",
       " '##steater',\n",
       " '##järt',\n",
       " '##svall',\n",
       " 'tänd',\n",
       " 'skutt',\n",
       " 'italienare',\n",
       " 'Kurserna',\n",
       " 'Hyre',\n",
       " 'välkom',\n",
       " '##ove',\n",
       " '##haw',\n",
       " 'kick',\n",
       " 'Argent',\n",
       " '##sfallet',\n",
       " 'Bent',\n",
       " '##rass',\n",
       " 'tredjepar',\n",
       " 'Ou',\n",
       " 'summerar',\n",
       " '##giro',\n",
       " '##lipper',\n",
       " '##lish',\n",
       " '##ardin',\n",
       " 'vetemjöl',\n",
       " '724',\n",
       " 'massage',\n",
       " '##umb',\n",
       " 'omdiskuterade',\n",
       " 'Kaf',\n",
       " '##emoni',\n",
       " 'rodd',\n",
       " 'ML',\n",
       " '##antas',\n",
       " '##geln',\n",
       " '##iaden',\n",
       " '##holt',\n",
       " '##128',\n",
       " '##Marie',\n",
       " '##musiker',\n",
       " '##grenska',\n",
       " 'Mimi',\n",
       " '##mässa',\n",
       " '##skyddade',\n",
       " 'månadskostnad',\n",
       " 'lagas',\n",
       " 'befint',\n",
       " '##ative',\n",
       " 'Dubb',\n",
       " '##producent',\n",
       " 'Nicar',\n",
       " 'olag',\n",
       " 'VÄST',\n",
       " 'handpenning',\n",
       " 'bätt',\n",
       " '##Ü',\n",
       " '##sponden',\n",
       " 'put',\n",
       " '##eper',\n",
       " 'Contin',\n",
       " 'illustr',\n",
       " 'arb',\n",
       " '##alagen',\n",
       " 'Reb',\n",
       " '##infar',\n",
       " 'Mild',\n",
       " 'Bomb',\n",
       " 'maskerad',\n",
       " 'lekfull',\n",
       " '##sbär',\n",
       " '##hol',\n",
       " '##atel',\n",
       " '##enc',\n",
       " '##arek',\n",
       " 'Jok',\n",
       " '##administrativa',\n",
       " 'express',\n",
       " '##efer',\n",
       " '##OCK',\n",
       " 'Zamb',\n",
       " 'Fut',\n",
       " 'Lap',\n",
       " 'förkunskaper',\n",
       " 'koma',\n",
       " 'Lennartsson',\n",
       " '##61',\n",
       " 'MODER',\n",
       " '##erman',\n",
       " 'soft',\n",
       " 'hits',\n",
       " 'spricker',\n",
       " '##säcken',\n",
       " '##emellan',\n",
       " 'Eger',\n",
       " '##orge',\n",
       " '##ningskort',\n",
       " 'motvind',\n",
       " 'livssyn',\n",
       " 'INGEN',\n",
       " 'Kerr',\n",
       " '##176',\n",
       " 'byrås',\n",
       " '##ivan',\n",
       " 'återfanns',\n",
       " 'ubåten',\n",
       " 'omöj',\n",
       " '##boden',\n",
       " '##RING',\n",
       " 'jubil',\n",
       " 'Lug',\n",
       " 'Mey',\n",
       " 'Dix',\n",
       " '##ulo',\n",
       " '##ustens',\n",
       " '##ATA',\n",
       " '##ylvan',\n",
       " 'Vågar',\n",
       " 'Toronto',\n",
       " 'Söndagar',\n",
       " '##onerande',\n",
       " '##skod',\n",
       " '##bytare',\n",
       " 'Södert',\n",
       " 'Jylland',\n",
       " '##burn',\n",
       " '##atu',\n",
       " '##OP',\n",
       " '##illo',\n",
       " 'avbröt',\n",
       " 'Wass',\n",
       " '##götlands',\n",
       " 'skärmen',\n",
       " 'sw',\n",
       " 'à',\n",
       " '##ugården',\n",
       " '##smottag',\n",
       " '##histor',\n",
       " '##deman',\n",
       " '##701',\n",
       " '##kamraten',\n",
       " '##111',\n",
       " '##klund',\n",
       " '##frik',\n",
       " '617',\n",
       " 'Mak',\n",
       " '##kir',\n",
       " '##ruvorna',\n",
       " 'Sura',\n",
       " 'Greg',\n",
       " 'Milles',\n",
       " 'Rus',\n",
       " '##oil',\n",
       " 'Vett',\n",
       " '##estra',\n",
       " '##SKOL',\n",
       " '##183',\n",
       " 'Hedvig',\n",
       " '595',\n",
       " 'FBI',\n",
       " 'segt',\n",
       " '##isto',\n",
       " 'rådgiv',\n",
       " '##pelet',\n",
       " '##samhäll',\n",
       " '##gärderna',\n",
       " 'musklerna',\n",
       " '##ierark',\n",
       " 'skålar',\n",
       " '##uvud',\n",
       " 'Long',\n",
       " '##ungarna',\n",
       " 'estetiskt',\n",
       " '##brö',\n",
       " 'Wag',\n",
       " '##bjudna',\n",
       " '##äckar',\n",
       " 'ÅLDER',\n",
       " 'kombin',\n",
       " 'Skytt',\n",
       " '##efull',\n",
       " 'dad',\n",
       " 'Love',\n",
       " 'potten',\n",
       " 'Inside',\n",
       " '##oule',\n",
       " 'Nordmakedonien',\n",
       " '##onius',\n",
       " '##imb',\n",
       " 'serve',\n",
       " 'Hid',\n",
       " 'berättelserna',\n",
       " 'slar',\n",
       " '##fota',\n",
       " 'Riss',\n",
       " '##ph',\n",
       " '##omy',\n",
       " 'kantonen',\n",
       " 'måltiden',\n",
       " '##204',\n",
       " 'Jokkm',\n",
       " '##engar',\n",
       " '##papperen',\n",
       " '##Ol',\n",
       " 'Haj',\n",
       " 'Deb',\n",
       " '##stick',\n",
       " '##IAL',\n",
       " '##skuponger',\n",
       " '##villa',\n",
       " 'Kontoret',\n",
       " 'Lyd',\n",
       " 'strip',\n",
       " 'Food',\n",
       " '##smäk',\n",
       " 'Sov',\n",
       " '##cl',\n",
       " '##gal',\n",
       " 'sås',\n",
       " 'Joachim',\n",
       " '##84',\n",
       " 'allmäng',\n",
       " '##omagasin',\n",
       " 'komedi',\n",
       " 'CNN',\n",
       " 'likvidator',\n",
       " '##sim',\n",
       " '##ellertid',\n",
       " '##tendenten',\n",
       " 'Trea',\n",
       " '##305',\n",
       " 'Steen',\n",
       " '##ertz',\n",
       " 'China',\n",
       " '##illen',\n",
       " '##imir',\n",
       " 'Nanny',\n",
       " '##såker',\n",
       " 'tul',\n",
       " 'kompisarna',\n",
       " '##kören',\n",
       " '##spapper',\n",
       " '##nöj',\n",
       " '##CKER',\n",
       " 'Sep',\n",
       " 'fans',\n",
       " '##ubbar',\n",
       " 'Sandhamn',\n",
       " '##cel',\n",
       " 'Jann',\n",
       " 'Broman',\n",
       " '##fire',\n",
       " '##fulle',\n",
       " 'BIL',\n",
       " 'Mari',\n",
       " '##urg',\n",
       " '##unkten',\n",
       " 'action',\n",
       " '##abs',\n",
       " 'Lagercrantz',\n",
       " 'Turt',\n",
       " '##hild',\n",
       " 'Bellman',\n",
       " '##åster',\n",
       " '##inom',\n",
       " 'inrikt',\n",
       " '##96',\n",
       " '##sfören',\n",
       " '##bads',\n",
       " 'Storg',\n",
       " '##republik',\n",
       " 'illustrerade',\n",
       " '##rövar',\n",
       " '##vällen',\n",
       " 'sydväst',\n",
       " '##nik',\n",
       " 'romantisk',\n",
       " 'konser',\n",
       " '##lägret',\n",
       " '##rain',\n",
       " 'Petrus',\n",
       " 'Sophia',\n",
       " 'soffan',\n",
       " 'festa',\n",
       " '##gänget',\n",
       " 'Dog',\n",
       " '##hästarna',\n",
       " 'hort',\n",
       " 'ip',\n",
       " '##SF',\n",
       " 'Stack',\n",
       " 'Lee',\n",
       " '##HJ',\n",
       " '##senter',\n",
       " '##laine',\n",
       " 'ostad',\n",
       " '##BERGS',\n",
       " '##UF',\n",
       " '##emaskin',\n",
       " 'Eventuella',\n",
       " 'Pear',\n",
       " 'MC',\n",
       " 'Gené',\n",
       " '##benen',\n",
       " '##avdelningens',\n",
       " 'Typ',\n",
       " 'inneh',\n",
       " 'giftet',\n",
       " '##idag',\n",
       " '754',\n",
       " '##bety',\n",
       " 'Herceg',\n",
       " 'Was',\n",
       " '##denna',\n",
       " 'uttagningen',\n",
       " '##hållna',\n",
       " '##installationer',\n",
       " 'Loh',\n",
       " 'präg',\n",
       " '##hoppet',\n",
       " '##56',\n",
       " '##ight',\n",
       " 'ledningsm',\n",
       " '##keberg',\n",
       " 'blund',\n",
       " '##hood',\n",
       " '##annen',\n",
       " 'Sein',\n",
       " 'Hustrun',\n",
       " 'Gård',\n",
       " 'Sle',\n",
       " '##heden',\n",
       " 'förkyld',\n",
       " '##enet',\n",
       " '##svagnar',\n",
       " '##snack',\n",
       " '##smaskiner',\n",
       " '##END',\n",
       " '##weiz',\n",
       " 'Mind',\n",
       " '##svägarna',\n",
       " 'jurid',\n",
       " 'kora',\n",
       " 'Gården',\n",
       " '##raste',\n",
       " 'pas',\n",
       " 'Musta',\n",
       " '##splanet',\n",
       " 'Gid',\n",
       " 'tår',\n",
       " 'Lend',\n",
       " 'råttor',\n",
       " 'Bollen',\n",
       " 'Bend',\n",
       " 'härlig',\n",
       " '##skärmar',\n",
       " '##sext',\n",
       " 'Jö',\n",
       " 'klingar',\n",
       " 'Cram',\n",
       " '##card',\n",
       " 'las',\n",
       " 'Great',\n",
       " '##ated',\n",
       " 'ound',\n",
       " 'Western',\n",
       " 'samhällskrit',\n",
       " 'dol',\n",
       " '##57',\n",
       " '##scenter',\n",
       " '##ÄD',\n",
       " 'Torn',\n",
       " 'Järvsö',\n",
       " 'Sha',\n",
       " '##dos',\n",
       " 'VIL',\n",
       " 'spär',\n",
       " 'minn',\n",
       " '##skoncernen',\n",
       " 'grenarna',\n",
       " 'Chil',\n",
       " 'grin',\n",
       " 'framlidne',\n",
       " '##46',\n",
       " 'välfört',\n",
       " 'Sop',\n",
       " '##orama',\n",
       " '##£',\n",
       " '##rique',\n",
       " 'grotta',\n",
       " '##ael',\n",
       " 'Bolibompa',\n",
       " 'Fordringar',\n",
       " 'distansen',\n",
       " 'Mich',\n",
       " '##ovskij',\n",
       " '##blandningen',\n",
       " 'dist',\n",
       " 'varumärket',\n",
       " 'José',\n",
       " 'Kille',\n",
       " 'färgad',\n",
       " 'Stol',\n",
       " '##wal',\n",
       " 'strateg',\n",
       " '##IX',\n",
       " 'sopp',\n",
       " '##uger',\n",
       " '##brän',\n",
       " 'prover',\n",
       " '##historier',\n",
       " 'Pur',\n",
       " '##ffin',\n",
       " 'Församlingen',\n",
       " '##göt',\n",
       " '##Q',\n",
       " 'Bianca',\n",
       " '##68',\n",
       " 'Pic',\n",
       " 'Alto',\n",
       " 'Ups',\n",
       " 'slal',\n",
       " '##91',\n",
       " 'föred',\n",
       " 'Vardags',\n",
       " '##fattare',\n",
       " 'Anfall',\n",
       " '##etti',\n",
       " '##chin',\n",
       " 'prem',\n",
       " '##iday',\n",
       " 'Vax',\n",
       " '##ynda',\n",
       " 'litter',\n",
       " 'proport',\n",
       " 'hipp',\n",
       " '##49',\n",
       " 'Betydligt',\n",
       " 'dödens',\n",
       " 'MEN',\n",
       " '##usar',\n",
       " '740',\n",
       " 'Pel',\n",
       " '##omer',\n",
       " 'Stadsbyggnad',\n",
       " '##emit',\n",
       " 'hörna',\n",
       " '##anmälan',\n",
       " '##aled',\n",
       " '##elsystem',\n",
       " 'gult',\n",
       " 'förhören',\n",
       " 'bölja',\n",
       " 'Gammel',\n",
       " '##urar',\n",
       " 'Lägenhet',\n",
       " 'Kajsa',\n",
       " '##sutrymmen',\n",
       " '##ilding',\n",
       " 'inarbetad',\n",
       " 'spöken',\n",
       " '##ries',\n",
       " '##skrafter',\n",
       " 'Utsikt',\n",
       " 'Aber',\n",
       " '##manland',\n",
       " '##huvudstaden',\n",
       " '##andas',\n",
       " '##rael',\n",
       " '##beln',\n",
       " '##iné',\n",
       " '##skägg',\n",
       " 'orgeln',\n",
       " 'Känd',\n",
       " '##fruar',\n",
       " '##ilas',\n",
       " 'äm',\n",
       " 'tackat',\n",
       " '##bd',\n",
       " 'Mellby',\n",
       " 'Kabel',\n",
       " 'Fleming',\n",
       " '##seln',\n",
       " 'nyförvär',\n",
       " 'markt',\n",
       " 'ordn',\n",
       " '##teckningar',\n",
       " '##rekvensen',\n",
       " '##uper',\n",
       " '##ersätt',\n",
       " '##irl',\n",
       " 'Abram',\n",
       " 'Reich',\n",
       " '##snatur',\n",
       " 'LEJON',\n",
       " 'TINGS',\n",
       " 'Lju',\n",
       " 'Indust',\n",
       " '##iblioteken',\n",
       " '##ädje',\n",
       " 'Boko',\n",
       " 'lagerb',\n",
       " '##aljer',\n",
       " 'rätter',\n",
       " 'vim',\n",
       " 'folkdans',\n",
       " 'Force',\n",
       " '##jobbet',\n",
       " '##301',\n",
       " '##qu',\n",
       " 'Allen',\n",
       " '##årsju',\n",
       " 'gass',\n",
       " '##artid',\n",
       " '##unken',\n",
       " '##haninge',\n",
       " 'Cruz',\n",
       " '##reporter',\n",
       " 'mart',\n",
       " 'förmed',\n",
       " 'illamående',\n",
       " '##ydde',\n",
       " 'Vänner',\n",
       " '##122',\n",
       " '##götland',\n",
       " 'Vass',\n",
       " '##ötte',\n",
       " '##ELSE',\n",
       " '##abliss',\n",
       " 'sjöutsikt',\n",
       " '##isar',\n",
       " 'årtion',\n",
       " 'skåde',\n",
       " 'RAD',\n",
       " 'king',\n",
       " 'Tam',\n",
       " '##ón',\n",
       " '##chn',\n",
       " '##sgat',\n",
       " 'Nyår',\n",
       " 'fingert',\n",
       " '##alender',\n",
       " 'källar',\n",
       " '##bow',\n",
       " 'dalen',\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_tokens = list(set(decoded_hugging_correct)-set(decoded_base_correct))\n",
    "print(len(good_tokens))\n",
    "good_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3231899"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking(batch,i):\n",
    "    word_ids=batch[\"word_ids\"]\n",
    " \n",
    "    masked_input_id=batch[\"input_ids\"].copy()\n",
    "    attention_mask=batch[\"attention_mask\"].copy()\n",
    " \n",
    "    labels=[[-100]*max_length]*len(batch[\"labels\"])\n",
    "    for z in range(len(masked_input_id)):\n",
    "        if batch[\"input_ids\"][z][i] ==tokenizer.pad_token_id or batch[\"input_ids\"][z][i] ==tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z][i]=batch[\"input_ids\"][z][i]\n",
    "        masked_input_id[z][i]=tokenizer.mask_token_id\n",
    "  \n",
    "        \n",
    "        word=tokenizer.decode(batch[\"input_ids\"][z][i])\n",
    "   \n",
    "        future_token=[j for j,_ in enumerate(word_ids[z]) if word_ids[z][j]==word_ids[z][i] and j>i]\n",
    "\n",
    "        for j in future_token:\n",
    "            labels[z][j]=batch[\"input_ids\"][z][j]\n",
    "    \n",
    "            masked_input_id[z][j]=tokenizer.mask_token_id\n",
    "           \n",
    "\n",
    "        masked_input_id[z]=np.array(masked_input_id[z])\n",
    "        attention_mask[z]=np.array(attention_mask[z])\n",
    "        labels[z]=np.array(labels[z])\n",
    "   \n",
    "    output_dict = {\"input_ids\": masked_input_id, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    return {k: v for k, v in output_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking_bis(batch, i):\n",
    "    word_ids = batch[\"word_ids\"]\n",
    "    masked_input_id = batch[\"input_ids\"].copy()\n",
    "    attention_mask = batch[\"attention_mask\"].copy()\n",
    "    \n",
    "    labels = np.full_like(masked_input_id, -100)\n",
    "    \n",
    "    for z, seq in enumerate(masked_input_id):\n",
    "        if seq[i] == tokenizer.pad_token_id or seq[i] == tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z, i] = seq[i]\n",
    "        masked_input_id[z][i] = tokenizer.mask_token_id\n",
    "        future_token = [j for j, _ in enumerate(word_ids[z]) if word_ids[z][j] == word_ids[z][i] and j > i]\n",
    "        \n",
    "        for j in future_token:\n",
    "            labels[z][j] = batch[\"input_ids\"][z][j]\n",
    "            masked_input_id[z][j] = tokenizer.mask_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": masked_input_id,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pll = 0\n",
    "batch_size=64\n",
    "for i in  range(max_length):\n",
    "    print(i)\n",
    "    losses=[]\n",
    "    eval_dataset_log = lm_datasets[\"test\"].map(\n",
    "        lambda examples: insert_special_masking_bis(examples,i),\n",
    "        batched=True,\n",
    "        remove_columns= lm_datasets[\"test\"].column_names\n",
    "    )\n",
    "    print(\"daatset\")\n",
    "    eval_dataloader = preprocessing.create_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
    "    print(\"dataloader\")\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch={key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output=model_hugging_face(**batch)\n",
    "        print(\"output\")\n",
    "        loss=output.loss\n",
    "        losses.append(loss.repeat(eval_dataloader.batch_size))\n",
    "        print(\"loss\")\n",
    "        break\n",
    "    losses = torch.cat(losses)\n",
    "    print(\"loss\")\n",
    "    #losses = losses[: len(eval_dataloader.dataset)]\n",
    "    pll += torch.mean(losses)\n",
    "\n",
    "\n",
    "pll /=max_length\n",
    "pll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_task(model_kb,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for ix, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['content'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    print(df[\"tag\"])\n",
    "    labels = torch.tensor(df['tag'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Create binary label where seg = 1\n",
    "df = df[df[\"content\"].notnull()]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "input_ids, attention_masks, labels = encode(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_long.config.name_or_path}-imdb\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    # Add other training arguments as needed\n",
    "    logging_steps=892,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True\n",
    ")\n",
    "print(training_args.device)\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model_long,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(result['eval_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
