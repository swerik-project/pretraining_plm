{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from data_swerick import create_dataset_swerick\n",
    "from evaluation import evaluation_task,regression_year\n",
    "import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch,data_collator):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model = preprocessing.create_model_MLM(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 12319\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 2683\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#datasest\n",
    "data_files = {\"train\": \"swerick_data_random_train.pkl\", \"test\": \"swerick_data_random_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242041d06ef440b090f781dfdf789bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets =preprocessing.tokenize_dataset(swerick_dataset,tokenizer)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tokenized_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name ['protocole'] not in the dataset. Current columns in the dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_datasets\u001b[38;5;241m=\u001b[39mtokenized_datasets\u001b[38;5;241m.\u001b[39mremove_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:366\u001b[0m, in \u001b[0;36mDatasetDict.remove_columns\u001b[0;34m(self, column_names)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: dataset\u001b[38;5;241m.\u001b[39mremove_columns(column_names\u001b[38;5;241m=\u001b[39mcolumn_names) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:366\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: dataset\u001b[38;5;241m.\u001b[39mremove_columns(column_names\u001b[38;5;241m=\u001b[39mcolumn_names) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m func(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2160\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2158\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2163\u001b[0m     )\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n",
      "\u001b[0;31mValueError\u001b[0m: Column name ['protocole'] not in the dataset. Current columns in the dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids']"
     ]
    }
   ],
   "source": [
    "tokenized_datasets=tokenized_datasets.remove_columns(\"protocole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm_datasets \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mgrouping_dataset(tokenized_datasets,chunk_size)\n\u001b[1;32m      2\u001b[0m lm_datasets\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "lm_datasets = preprocessing.grouping_dataset(tokenized_datasets,chunk_size)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lm_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid={\"valid\":\"swerick_data_random_valid.pkl\"}\n",
    "valid_dataset = load_dataset(\"pandas\",data_files=data_valid) \n",
    "valid_dataset =preprocessing.tokenize_dataset(valid_dataset,tokenizer)\n",
    "valid_dataset=preprocessing.grouping_dataset(valid_dataset,chunk_size)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"wb\") as f:\n",
    "     pickle.dump(valid_dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 800106\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\",\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial with a manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "    num_rows: 3663965\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"])\n",
    "\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset[\"valid\"],data_collator)\n",
    "\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(\"cpu\") for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)\n",
    "print(\"ok\")\n",
    "\n",
    "#for batch in train_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "#for batch in eval_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(eval_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3663965\n",
      "})\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x775620217590>\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader.dataset)\n",
    "print(eval_dataloader)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_ids\"].device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##försvar, industriellberedskap, ekonomiskberedskapo. s. v. Manharräknatmedolikakostnaderfördessaochsedanförsöktfog [MASK]hopdessabyggklotsartillenen [MASK] 90Nr10Onsdagenden23 [MASK]s1955Överbefälhavarensutredningrörandekrigsmaktensutvecklinghetligbyggnad. [UNK]. HerrHjalmarsonvari rest [MASK]åenfråga [MASK] somjagmednåg [MASK]ynpunktervillspinnavidarep [MASK]. Vilkenärsj [MASK]agrundenförÖB - utredningens [MASK] [MASK]? [MASK], minadamerochherrar, det [MASK], attdetta [MASK]representerardetminimumsom\n",
      "##inläggetskullekanskeintebehövanågotgenmäle, tyjagkonstaterarmedtillfreds [MASK], attherr [MASK]rhén [MASK]kommitmerapappersliknandevåglängdsomjagäridennas [MASK], [MASK]jagnufåruttryckamigså. Jagvilldockgöraetttillugerläggande. Manharpåvissthåll [MASK]tförlöjligavadsomskeddeiuniversitetsutredningengenomatts [MASK]attordföranden, d. v [MASK] [MASK]. statssekreterare [MASK]denman, skrevtillsigsjälv. Det [MASK]håller [MASK] [MASK]allspådets [MASK]t, utanvidenavdeförsta [MASK]gångarnaunderhöstens\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader():\n",
    "    train =DataLoader(\n",
    "    lm_dataset_bis[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n",
    "    train = [inputs.to(device) for inputs in train_dataloader]\n",
    "    return train\n",
    "\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_bis = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_bis=model_bis.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bis.eval()\n",
    "\n",
    "total_loss = 0.0  # Variable to accumulate total loss\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bis(**batch)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()   # Accumulate the batch loss\n",
    "\n",
    "# Calculate the average loss\n",
    "average_loss = total_loss / len(eval_dataloader)\n",
    "\n",
    "print(f\"Initial Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model_bis.parameters(), lr=1.3e-5)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "losses_train=[]\n",
    "losses_test=[]\n",
    "#train_dataloader = get_dataloader()\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model_bis.train()\n",
    "    print(next(model_bis.parameters()).device)\n",
    "    print(epoch)\n",
    "    params_before_optimization = [param.data.clone() for param in model_bis.parameters()]\n",
    "    total_loss_train = 0.0 \n",
    "    train_dataloader = get_dataloader()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model_bis(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        params_after_optimization = [param.data for param in model_bis.parameters()]\n",
    "        parameters_changed = any((param_before != param_after).any() for param_before, param_after in zip(params_before_optimization, params_after_optimization))\n",
    "        #if parameters_changed==True :\n",
    "             # print(parameters_changed) \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    losses_train.append(total_loss_train/len(train_dataloader))\n",
    "    print(\"losses_train\",losses_train)\n",
    "\n",
    "    # Evaluation\n",
    "    model_bis.eval()\n",
    "    losses=[]\n",
    "    total_loss_eval=0.0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bis(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        total_loss_eval +=loss.item()\n",
    "\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "       perplexity = float(\"inf\")\n",
    "\n",
    "    losses_test.append(total_loss_eval/len(eval_dataloader))\n",
    "\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    print(\"losses_test\",losses_test)\n",
    "\n",
    "print(\"epoch\",num_train_epochs)\n",
    "plt.plot(range(num_train_epochs),losses_train,label=\"train Loss\")\n",
    "\n",
    "plt.plot(range(num_train_epochs),losses_test,label=\"test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_train)\n",
    "print(losses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"finetuning_manual\"\n",
    "model_bis.save_pretrained(file_path)\n",
    "tokenizer.save_pretrained(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"losses.pkl\"\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump({'losses_train': losses_train, 'losses_test': losses_test}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"./test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_long=AutoModelForMaskedLM.from_pretrained(\"./finetuning_hugging-finetuned-imdb/checkpoint-259384\")\n",
    "model_long=model_long.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(\"./test_model\")\n",
    "model=model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_kb=AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n",
      "Manual perplexity...\n",
      " Perplexity: 2.905360638290348\n",
      "Accuracy...\n",
      "Accuracy: 0.7544896756314945\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model_hugging_face,valid_dataloader,\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846796b047fb47deb8391d2752a6f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2889373302459717, 'eval_runtime': 1670.4142, 'eval_samples_per_second': 478.987, 'eval_steps_per_second': 7.484}\n",
      ">>> Perplexity: 9.86\n",
      "Manual perplexity...\n",
      " Perplexity: 9.864508254198425\n",
      "Accuracy...\n",
      "Accuracy: 0.588802334176737\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    4,     4, 49795,  ..., 49795,  6742,     4],\n",
      "        [10996,   688, 12494,  ...,  3193, 13781,    19],\n",
      "        [18059,   183,  2261,  ...,    24, 48380, 48465],\n",
      "        ...,\n",
      "        [49816, 21033,    49,  ...,   469,  1314, 49830],\n",
      "        [21264,  1219,     4,  ..., 11583,   546, 20805],\n",
      "        [    7,     1,     4,  ...,     4,     4, 28662]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  109, 28534,  -100,  ...,  -100,  -100, 15873],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100, 38839,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,     7,  ...,    37,  2193,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1410209390681376"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(0.13192342221736908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données à partir du fichier JSON\n",
    "with open(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500/trainer_state.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epoch_train = []\n",
    "epoch_test=[]\n",
    "\n",
    "for entry in data['log_history']:\n",
    "    if 'loss' in entry:\n",
    "        train_loss.append(entry['loss'])\n",
    "        epoch_train.append((entry['epoch']))\n",
    "    elif 'eval_loss' in entry:\n",
    "        eval_loss.append(entry['eval_loss'])\n",
    "        epoch_test.append((entry['epoch']))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_train, train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epoch_test, eval_loss, label='Evaluation Loss', marker='o')\n",
    "plt.title('Training and Evaluation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id= tokenizer.pad_token_id\n",
    "sep_token_id = tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking(batch,i):\n",
    "    word_ids=batch[\"word_ids\"]\n",
    " \n",
    "    masked_input_id=batch[\"input_ids\"].copy()\n",
    "    attention_mask=batch[\"attention_mask\"].copy()\n",
    " \n",
    "    labels=[[-100]*max_length]*len(batch[\"labels\"])\n",
    "    for z in range(len(masked_input_id)):\n",
    "        if batch[\"input_ids\"][z][i] ==tokenizer.pad_token_id or batch[\"input_ids\"][z][i] ==tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z][i]=batch[\"input_ids\"][z][i]\n",
    "        masked_input_id[z][i]=tokenizer.mask_token_id\n",
    "  \n",
    "        \n",
    "        word=tokenizer.decode(batch[\"input_ids\"][z][i])\n",
    "   \n",
    "        future_token=[j for j,_ in enumerate(word_ids[z]) if word_ids[z][j]==word_ids[z][i] and j>i]\n",
    "\n",
    "        for j in future_token:\n",
    "            labels[z][j]=batch[\"input_ids\"][z][j]\n",
    "    \n",
    "            masked_input_id[z][j]=tokenizer.mask_token_id\n",
    "           \n",
    "\n",
    "        masked_input_id[z]=np.array(masked_input_id[z])\n",
    "        attention_mask[z]=np.array(attention_mask[z])\n",
    "        labels[z]=np.array(labels[z])\n",
    "   \n",
    "    output_dict = {\"input_ids\": masked_input_id, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    return {k: v for k, v in output_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking_bis(batch, i):\n",
    "    word_ids = batch[\"word_ids\"]\n",
    "    masked_input_id = batch[\"input_ids\"].copy()\n",
    "    attention_mask = batch[\"attention_mask\"].copy()\n",
    "    \n",
    "    labels = np.full_like(masked_input_id, -100)\n",
    "    \n",
    "    for z, seq in enumerate(masked_input_id):\n",
    "        if seq[i] == tokenizer.pad_token_id or seq[i] == tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z, i] = seq[i]\n",
    "        masked_input_id[z][i] = tokenizer.mask_token_id\n",
    "        future_token = [j for j, _ in enumerate(word_ids[z]) if word_ids[z][j] == word_ids[z][i] and j > i]\n",
    "        \n",
    "        for j in future_token:\n",
    "            labels[z][j] = batch[\"input_ids\"][z][j]\n",
    "            masked_input_id[z][j] = tokenizer.mask_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": masked_input_id,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "daatset\n",
      "dataloader\n",
      "output\n",
      "loss\n",
      "loss\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868fcd2f0a446ac8260aee3351fc56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/761125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daatset\n",
      "dataloader\n",
      "output\n",
      "loss\n",
      "loss\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5375b049bc084f5e9248be3ded3723e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/761125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      5\u001b[0m losses\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 6\u001b[0m eval_dataset_log \u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m examples: insert_special_masking_bis(examples,i),\n\u001b[1;32m      8\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaatset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mcreate_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3483\u001b[0m         batch,\n\u001b[1;32m   3484\u001b[0m         indices,\n\u001b[1;32m   3485\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   3486\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   3487\u001b[0m     )\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[120], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      5\u001b[0m losses\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      6\u001b[0m eval_dataset_log \u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m examples: insert_special_masking_bis(examples,i),\n\u001b[1;32m      8\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaatset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mcreate_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
      "Cell \u001b[0;32mIn[112], line 4\u001b[0m, in \u001b[0;36minsert_special_masking_bis\u001b[0;34m(batch, i)\u001b[0m\n\u001b[1;32m      2\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m masked_input_id \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 4\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull_like(masked_input_id, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(masked_input_id):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:272\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 272\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:375\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_column(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_table\u001b[38;5;241m.\u001b[39mselect([key]))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:441\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_column(pa_table)\n\u001b[1;32m    442\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:147\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pylist()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pll = 0\n",
    "batch_size=64\n",
    "for i in  range(max_length):\n",
    "    print(i)\n",
    "    losses=[]\n",
    "    eval_dataset_log = lm_datasets[\"test\"].map(\n",
    "        lambda examples: insert_special_masking_bis(examples,i),\n",
    "        batched=True,\n",
    "        remove_columns= lm_datasets[\"test\"].column_names\n",
    "    )\n",
    "    print(\"daatset\")\n",
    "    eval_dataloader = preprocessing.create_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
    "    print(\"dataloader\")\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch={key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output=model_hugging_face(**batch)\n",
    "        print(\"output\")\n",
    "        loss=output.loss\n",
    "        losses.append(loss.repeat(eval_dataloader.batch_size))\n",
    "        print(\"loss\")\n",
    "        break\n",
    "    losses = torch.cat(losses)\n",
    "    print(\"loss\")\n",
    "    #losses = losses[: len(eval_dataloader.dataset)]\n",
    "    pll += torch.mean(losses)\n",
    "\n",
    "\n",
    "pll /=max_length\n",
    "pll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d64e2999424512b94605be5fc3e3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 5813.28\n",
      "Manual perplexity...\n",
      " Perplexity: 5814.432395049958\n",
      "Accuracy...\n",
      "Accuracy: 0.18456963747366367\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model_kb,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year regression\n",
      "training\n",
      "stdout: \u001b[32m14:48:05 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m14:48:05 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m14:48:06 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([ 0.5352,  0.3230, -1.6105, -1.7756,  0.7710,  0.3937, -2.3650,  0.3701,\n",
      "         0.9596,  0.1108,  0.4880,  0.5352,  0.3937, -2.2000,  0.5352, -2.3886,\n",
      "         0.5588,  0.3465, -0.3372,  0.5116,  0.7945, -0.7381,  0.5588,  0.8889,\n",
      "         0.5116,  1.0303, -0.5259,  0.5588,  0.3937,  0.3937,  0.7238,  0.7238,\n",
      "         0.9596,  0.6295, -1.5869,  0.5116, -2.1057,  0.0400,  0.4173,  0.8889,\n",
      "        -0.5259, -1.0918,  0.9124,  0.6059,  1.0303, -2.0585,  0.6295, -1.4926,\n",
      "        -0.1722, -0.1015, -1.3983,  0.9596,  0.7002, -1.5633, -0.1486, -1.6812,\n",
      "         0.5588, -0.3372,  0.6059,  0.8417, -0.2193, -0.2193,  0.2287,  0.1343,\n",
      "         0.7238,  0.4644,  0.7238,  0.8181,  0.3937, -2.2235,  0.6295,  0.4173,\n",
      "         1.0068, -2.1528,  0.9360,  1.0303, -1.4690,  0.6059,  0.8181,  0.8653,\n",
      "        -1.9878,  0.3465,  0.4880,  1.0303, -1.3983,  0.9124,  0.9124,  0.5352,\n",
      "         1.0539,  1.0068, -0.1486,  0.3937,  0.7710, -0.5730, -1.9642,  1.0539,\n",
      "         0.3701,  0.5588, -1.3983,  0.1579,  0.2522, -0.8324,  0.4173, -0.5023,\n",
      "         0.3937,  0.3465,  0.8653,  0.5116,  0.8417,  0.1815,  0.7474, -1.2332,\n",
      "         0.7238,  1.0539,  0.4644,  0.5352,  0.0164, -0.4080, -1.1625, -2.4358,\n",
      "         0.3937,  0.9124,  0.6295, -0.7145,  0.5588,  0.3230,  0.4880,  0.8653,\n",
      "         0.7002,  0.1343,  0.6295,  0.5823,  0.9596, -0.1250, -1.8699,  0.4173,\n",
      "         0.9596,  0.5352,  0.9360,  0.7474,  0.8889, -0.2193, -1.4690, -1.4926,\n",
      "         0.4880,  1.0303,  0.7710,  0.5352,  0.4644,  0.0400,  0.3465,  1.0539,\n",
      "         0.5352,  0.8417, -2.4593, -0.5023,  0.4644, -1.6341, -0.8324, -1.3983,\n",
      "         0.5116,  0.3937,  1.0539,  0.3937,  0.7710, -0.1015,  0.4644,  0.3465,\n",
      "        -1.3511, -0.1722,  0.7002,  0.7945, -1.4926,  0.9124,  0.8417,  0.3937,\n",
      "         1.0068,  0.7238, -2.3414,  0.9124,  0.6295, -1.3983,  0.9832,  0.6059,\n",
      "         0.8653,  0.5116,  0.7710, -0.6202,  0.3701,  0.4644,  0.1815,  0.7238,\n",
      "         0.7238,  0.9832, -0.5966, -0.2901, -1.4454,  0.3465, -1.6812,  0.3701,\n",
      "         0.6531,  0.6295,  0.5823, -0.5023,  0.6767,  0.4880, -1.4219,  0.9832,\n",
      "         0.4173, -1.1153, -1.2097,  0.4409, -0.4316, -0.0307, -1.4690,  0.3230,\n",
      "        -0.1486, -1.2097, -1.7991,  0.6295, -1.5869,  0.8889,  0.3937,  0.9360,\n",
      "         0.8653, -0.4551, -1.9878,  0.7710,  0.9832,  0.8181, -0.6909,  0.5352,\n",
      "         0.3465,  0.9360,  0.5588,  0.7945,  0.7710,  0.7002,  0.9596, -1.1389,\n",
      "         0.2522,  0.7474,  0.9596,  0.5588,  0.8417, -1.7756,  0.9360,  0.9596,\n",
      "         0.6059, -0.3608, -1.5398,  0.5116,  0.8889,  0.5352, -2.3414,  0.5588,\n",
      "        -1.3983,  0.3465, -0.7617,  0.5116,  0.8889,  0.7002,  0.5352,  0.3230,\n",
      "        -0.1250,  0.4409,  0.6767,  0.8417, -0.4787,  0.2994,  0.4409,  0.6531,\n",
      "        -0.4316,  0.7945,  0.9124, -2.3886,  0.2994,  0.4409,  0.6295,  0.0872,\n",
      "         0.4173,  0.8181,  0.7002,  0.3937, -1.8463,  0.5352,  0.7238,  0.7710,\n",
      "        -1.4690, -1.5398, -2.3650,  0.9124,  0.5588,  1.0303,  0.4409,  0.6767,\n",
      "         0.8181, -2.5065,  0.7002,  0.5352, -1.4219,  0.3701, -1.7520,  0.6767,\n",
      "         0.4880,  0.9360, -0.8324, -1.7284,  0.4409, -1.3040,  0.8889,  0.9360,\n",
      "         0.4644, -1.8463,  0.8889,  0.6767, -1.3276,  0.6295,  0.6531, -2.1528,\n",
      "         0.8889, -1.5633,  0.4880, -0.2193,  0.9832,  0.7710,  0.8653,  0.7002,\n",
      "         0.6059,  0.7710, -1.6577,  0.4880,  0.6767, -0.2193,  1.0303,  0.4409,\n",
      "        -2.2235,  0.5823,  0.7710,  1.0303,  0.8889,  0.8417, -2.4358, -1.7048,\n",
      "         0.1108,  0.5588, -0.8324,  0.9832, -1.1389,  0.6295,  0.7238, -0.4316,\n",
      "         0.9124,  0.6059,  1.0303,  0.6295,  0.4409,  0.7474,  1.0539, -0.4551,\n",
      "         0.5116,  0.9832,  0.6531,  0.9832,  0.6531,  0.7002,  0.1343,  0.9360,\n",
      "        -0.6438,  0.6059, -0.5259,  1.0539,  0.7002,  0.8417, -1.7048,  0.2522,\n",
      "         0.9360,  0.7238,  0.4409, -2.3886, -1.3511, -1.3747,  0.5352,  0.3465,\n",
      "        -2.2471,  0.9596,  0.3701,  0.3701,  0.3701,  0.2287, -0.6673,  0.9596,\n",
      "         1.0303, -1.5633,  1.0303, -1.3747,  0.9360, -0.4316,  0.5352, -2.4829,\n",
      "         1.0303,  0.9360,  0.3701,  0.0636,  0.5823,  0.4173, -0.2901,  0.3937,\n",
      "         0.5116, -0.2429,  0.8417,  0.8653, -2.0821,  0.9124,  0.6531, -0.3608,\n",
      "         0.9124,  0.3701,  0.3230,  1.0068,  0.6767,  0.9360,  0.9124, -1.3983,\n",
      "         0.7238,  0.5352,  1.0303,  0.6295, -2.0349,  0.8889, -1.3983,  0.8889,\n",
      "         0.3465,  1.0539,  0.5823,  0.5823,  0.8653, -1.8463,  0.3230,  0.5352,\n",
      "         0.6767,  0.3230,  0.3701,  0.8653, -1.0210,  0.3230, -1.2804,  0.3465,\n",
      "         0.1815, -1.7520, -0.0779,  0.9360,  0.3701,  1.0068,  0.5823, -1.6105,\n",
      "         0.3701, -2.3179,  0.7002,  1.0068, -1.3747,  0.4644,  0.3701, -0.3372,\n",
      "        -1.5398,  0.5352,  0.7238,  1.0303,  0.5588,  0.4880,  1.0539,  0.6767,\n",
      "         0.7945,  0.9360, -1.3040,  0.3701,  0.3701,  0.5588,  0.7474,  0.7002,\n",
      "         0.7002,  0.5116, -1.3511,  1.0539,  0.7474, -1.7284, -1.3747,  0.0636,\n",
      "         0.4409,  0.8181,  1.0303, -0.3137,  0.7002, -1.6577,  0.9124,  0.6767,\n",
      "        -2.5772,  0.4173, -0.8560, -2.4358, -0.4080, -0.0779, -2.1292, -2.2000,\n",
      "         0.6059,  1.0068, -2.5301,  0.7238, -1.4926, -2.4829,  0.4173, -1.3511,\n",
      "         0.4173,  0.8417, -0.1958,  0.9596,  0.6295,  0.3701,  0.5588,  0.5588,\n",
      "        -2.0821,  1.0068,  0.9124, -0.6202, -0.6202, -1.0682,  0.4644, -1.7756,\n",
      "         0.8181,  0.2758, -1.1153, -0.2665,  0.6531,  0.4409,  0.3465,  0.4880,\n",
      "         0.8889, -0.4080,  0.8417,  0.8417,  0.5588,  0.9596, -2.4122,  0.7474,\n",
      "        -2.5537, -1.1389,  0.2287, -1.0918,  0.3465,  0.4880, -0.6202,  0.5352,\n",
      "         0.4880,  0.7474,  0.3937,  0.0164,  0.5116,  1.0539, -1.0682,  0.5823,\n",
      "        -1.5162, -1.7284,  0.3701,  0.5352,  0.7002,  0.9124,  0.3230, -0.1250,\n",
      "         0.8653,  0.5588, -1.6341, -1.4926,  0.5823, -2.3179,  0.3701,  0.3937,\n",
      "         0.6531,  0.4880,  0.8889,  0.5352,  0.5823, -0.1015,  0.8653,  0.7945,\n",
      "         1.0539,  0.4644,  0.3230,  0.8889, -1.2097, -2.1528,  0.9596, -1.3511,\n",
      "        -0.2193, -1.5633,  0.7002, -0.2429,  0.9360,  0.1815,  0.8889,  0.4880,\n",
      "        -1.9170,  0.8417,  0.6531,  0.6059, -2.2235,  0.4644,  0.3230,  0.7238,\n",
      "         0.3701, -1.5633,  0.2994, -0.8324,  0.4173, -1.3040,  0.3937,  0.9360,\n",
      "        -0.7852,  0.7710,  0.6059,  0.4409,  0.2758,  0.8889,  0.3701, -1.0918,\n",
      "        -1.7520, -2.2000,  0.6059,  0.6767,  0.7710,  0.8653,  0.0636, -0.2193,\n",
      "         0.6059, -1.0446,  0.8889, -1.3747,  1.0539, -0.3608,  0.7474,  0.6295,\n",
      "        -1.4926,  0.6767,  0.3701,  0.9596,  0.7238, -0.7381,  0.7710,  0.8889,\n",
      "        -0.6202,  0.5823,  1.0068,  0.7002,  0.9596, -0.7617,  0.4173,  0.5116,\n",
      "         0.8417,  0.6295, -0.7381,  0.9596, -0.5966,  0.7002, -0.6202,  0.4409,\n",
      "         0.4173,  0.9124, -1.0918,  0.2994,  0.7945,  0.6059,  0.4644,  0.6767,\n",
      "         0.6295, -0.4787, -1.1625, -1.7284, -0.0779,  0.2758,  0.5352,  0.6059,\n",
      "         0.3701, -0.3137,  0.4880,  0.5823,  0.3465,  0.4173, -0.8560,  0.7945,\n",
      "         0.7238, -1.5633, -0.9974,  0.7710, -0.3844,  0.4173, -0.9503,  0.8889,\n",
      "        -0.7852, -0.5259, -1.7520,  0.4173,  0.8653,  0.6059,  0.8889,  0.6767,\n",
      "         0.8653,  0.5588,  0.9832, -0.1722,  0.2994, -0.1958,  0.8889,  0.7710,\n",
      "         1.0539,  0.3937,  0.5823,  0.3465,  0.3937,  0.4644,  0.4409, -0.7145,\n",
      "         0.7002,  0.8653, -0.1486, -1.0918,  0.0164,  0.7002,  0.0636, -2.2235,\n",
      "        -1.3983,  0.8889,  0.7002, -0.0779,  1.0539,  0.6059, -1.8699,  0.4644,\n",
      "         0.8889, -1.0210, -1.5869,  0.7002,  0.2994,  1.0303,  0.9124,  0.5588,\n",
      "        -0.3844, -2.5537,  0.8417,  0.5116,  0.6059,  0.4409, -1.6577,  0.7238,\n",
      "        -0.0543,  0.9360,  0.5352,  0.4173,  0.8653, -0.1722, -0.2665,  0.1343,\n",
      "         0.7474, -1.5869, -1.7284,  0.8181, -0.3137,  0.3701, -0.7852,  1.0068,\n",
      "        -1.2804,  0.4644, -0.0779, -0.7381, -2.4593,  0.6059,  0.4880,  0.7238,\n",
      "        -2.0585,  1.0303,  0.7945, -1.7520,  0.9832, -2.5065, -1.2332,  0.6295,\n",
      "         0.3937, -0.5730,  0.5352,  0.4409, -2.4593,  0.3465,  0.9832, -1.8699,\n",
      "         0.7710,  0.7945,  0.9124,  1.0068,  1.0303,  0.5116, -0.2193, -2.0349,\n",
      "        -1.5162,  1.0303,  0.5588, -1.4454,  0.0636,  0.5823,  0.4644,  0.7002,\n",
      "         0.3701,  0.8653,  0.7238,  0.7238,  0.7474,  0.3701,  0.5588,  0.6531,\n",
      "        -0.1015,  0.9360,  0.8653, -2.4829,  0.7002,  0.9124,  0.6295,  0.9596,\n",
      "        -2.1057,  0.8417, -1.8934,  0.7710,  0.5352, -2.0585,  0.7474,  0.5116,\n",
      "         0.8889, -1.5869,  0.8417, -1.3747,  0.7002,  0.8653, -1.7520,  0.3937,\n",
      "         0.7945, -2.2707,  0.3465,  0.8653,  0.5823, -1.5162,  0.7002,  0.4880,\n",
      "        -0.5966, -0.4080, -0.5259, -0.2901,  0.8181, -0.0543,  0.2994,  0.8889,\n",
      "        -1.7284,  0.6767,  1.0539,  0.6059, -0.0779,  0.3230, -0.5966,  0.2522,\n",
      "         0.9124,  0.9360,  0.4409,  0.3701,  0.5116,  0.6531,  0.6531,  0.4644,\n",
      "         0.9832, -2.3414, -0.5023, -0.3137,  0.6295,  0.7002, -0.3372, -2.0349,\n",
      "         0.3465, -1.1153,  0.3701,  0.4644, -1.4454,  0.8889,  0.9596,  0.4880,\n",
      "        -0.0543,  0.3937,  0.7002, -0.3608, -1.4926,  0.9124, -0.7852,  0.4880,\n",
      "         0.2287,  0.8889,  0.9360,  0.9596,  0.8653, -2.3414,  0.7474,  0.3937,\n",
      "         0.9832, -0.5966,  0.3465, -1.3511,  0.5823,  0.4173,  0.1579,  0.7474,\n",
      "         0.6531, -1.6105,  0.4880,  0.8181,  0.3230,  0.9596,  0.4644, -0.6673,\n",
      "        -0.1015, -1.4926,  1.0068,  0.7474,  0.6295,  0.4173, -2.4358,  0.3230,\n",
      "         0.6295,  0.5588, -2.3886,  0.8889,  0.5823,  0.5352, -0.0307,  0.3230,\n",
      "         0.9124, -0.4316, -1.3747,  0.1579, -1.7048, -1.5633,  0.5823,  0.9832,\n",
      "         0.1108,  1.0303, -0.0307,  0.8181, -0.8796, -0.8324,  1.0068,  0.6531,\n",
      "        -2.5301,  0.5588,  1.0303, -0.6202, -0.0543,  0.6295, -0.5966,  0.5116,\n",
      "         0.2994,  0.4644,  0.2522,  0.7238,  0.0164,  0.7238,  0.3465,  0.7710,\n",
      "         0.4409,  0.9360,  1.0539,  0.7474,  0.7710, -1.7991,  0.8181, -0.1958,\n",
      "         0.7002,  0.3230,  0.2994, -1.6341,  0.4409,  0.5823,  0.7710, -0.1958,\n",
      "         0.7474,  0.6531,  0.7238,  0.5823,  0.5352,  0.5588, -0.9739,  0.8181,\n",
      "         0.7710,  0.4644, -0.7381,  0.7238, -2.0821,  0.1343,  0.4409,  0.0636])\u001b[0m\n",
      "\u001b[34m14:48:07 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m14:48:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.192\u001b[0m\n",
      "\u001b[34m14:48:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 2.386\u001b[0m\n",
      "\u001b[34m14:48:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8247222900390625\u001b[0m\n",
      "\u001b[32m14:48:22 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:48:23 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m14:48:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 2.033\u001b[0m\n",
      "\u001b[34m14:48:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.827\u001b[0m\n",
      "\u001b[34m14:48:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8589841723442078\u001b[0m\n",
      "\u001b[32m14:48:38 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:48:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m14:48:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 1.097\u001b[0m\n",
      "\u001b[34m14:48:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.445\u001b[0m\n",
      "\u001b[34m14:48:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8881311416625977\u001b[0m\n",
      "\u001b[32m14:48:54 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:48:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m14:49:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.723\u001b[0m\n",
      "\u001b[34m14:49:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.246\u001b[0m\n",
      "\u001b[34m14:49:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9019820094108582\u001b[0m\n",
      "\u001b[32m14:49:10 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:49:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "\u001b[34m14:49:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.511\u001b[0m\n",
      "\u001b[34m14:49:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.227\u001b[0m\n",
      "\u001b[34m14:49:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9051702618598938\u001b[0m\n",
      "\u001b[32m14:49:26 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:49:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "\u001b[34m14:49:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.434\u001b[0m\n",
      "\u001b[34m14:49:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.204\u001b[0m\n",
      "\u001b[34m14:49:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9062244296073914\u001b[0m\n",
      "\u001b[32m14:49:43 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:49:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "\u001b[34m14:49:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.372\u001b[0m\n",
      "\u001b[34m14:49:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.139\u001b[0m\n",
      "\u001b[34m14:49:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9111806750297546\u001b[0m\n",
      "\u001b[32m14:49:59 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:50:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "\u001b[34m14:50:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.377\u001b[0m\n",
      "\u001b[34m14:50:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.174\u001b[0m\n",
      "\u001b[34m14:50:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9072189331054688\u001b[0m\n",
      "\u001b[32m14:50:15 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:50:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "\u001b[34m14:50:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.329\u001b[0m\n",
      "\u001b[34m14:50:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.175\u001b[0m\n",
      "\u001b[34m14:50:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9081009030342102\u001b[0m\n",
      "\u001b[32m14:50:30 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:50:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "\u001b[34m14:50:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.335\u001b[0m\n",
      "\u001b[34m14:50:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.180\u001b[0m\n",
      "\u001b[34m14:50:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9086167216300964\u001b[0m\n",
      "\u001b[32m14:50:45 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 44/44 [00:14<00:00,  3.14it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.59it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.23it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.42it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.67it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.18it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.47it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.67it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.18it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.41it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.66it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.16it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.28it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.17it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.69it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.17it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.57it/s]\n",
      "\n",
      "Party classification\n",
      "training hugging face\n",
      "stdout: \u001b[32m14:50:47 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m14:50:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m14:50:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([ 0.5352,  0.3230, -1.6105, -1.7756,  0.7710,  0.3937, -2.3650,  0.3701,\n",
      "         0.9596,  0.1108,  0.4880,  0.5352,  0.3937, -2.2000,  0.5352, -2.3886,\n",
      "         0.5588,  0.3465, -0.3372,  0.5116,  0.7945, -0.7381,  0.5588,  0.8889,\n",
      "         0.5116,  1.0303, -0.5259,  0.5588,  0.3937,  0.3937,  0.7238,  0.7238,\n",
      "         0.9596,  0.6295, -1.5869,  0.5116, -2.1057,  0.0400,  0.4173,  0.8889,\n",
      "        -0.5259, -1.0918,  0.9124,  0.6059,  1.0303, -2.0585,  0.6295, -1.4926,\n",
      "        -0.1722, -0.1015, -1.3983,  0.9596,  0.7002, -1.5633, -0.1486, -1.6812,\n",
      "         0.5588, -0.3372,  0.6059,  0.8417, -0.2193, -0.2193,  0.2287,  0.1343,\n",
      "         0.7238,  0.4644,  0.7238,  0.8181,  0.3937, -2.2235,  0.6295,  0.4173,\n",
      "         1.0068, -2.1528,  0.9360,  1.0303, -1.4690,  0.6059,  0.8181,  0.8653,\n",
      "        -1.9878,  0.3465,  0.4880,  1.0303, -1.3983,  0.9124,  0.9124,  0.5352,\n",
      "         1.0539,  1.0068, -0.1486,  0.3937,  0.7710, -0.5730, -1.9642,  1.0539,\n",
      "         0.3701,  0.5588, -1.3983,  0.1579,  0.2522, -0.8324,  0.4173, -0.5023,\n",
      "         0.3937,  0.3465,  0.8653,  0.5116,  0.8417,  0.1815,  0.7474, -1.2332,\n",
      "         0.7238,  1.0539,  0.4644,  0.5352,  0.0164, -0.4080, -1.1625, -2.4358,\n",
      "         0.3937,  0.9124,  0.6295, -0.7145,  0.5588,  0.3230,  0.4880,  0.8653,\n",
      "         0.7002,  0.1343,  0.6295,  0.5823,  0.9596, -0.1250, -1.8699,  0.4173,\n",
      "         0.9596,  0.5352,  0.9360,  0.7474,  0.8889, -0.2193, -1.4690, -1.4926,\n",
      "         0.4880,  1.0303,  0.7710,  0.5352,  0.4644,  0.0400,  0.3465,  1.0539,\n",
      "         0.5352,  0.8417, -2.4593, -0.5023,  0.4644, -1.6341, -0.8324, -1.3983,\n",
      "         0.5116,  0.3937,  1.0539,  0.3937,  0.7710, -0.1015,  0.4644,  0.3465,\n",
      "        -1.3511, -0.1722,  0.7002,  0.7945, -1.4926,  0.9124,  0.8417,  0.3937,\n",
      "         1.0068,  0.7238, -2.3414,  0.9124,  0.6295, -1.3983,  0.9832,  0.6059,\n",
      "         0.8653,  0.5116,  0.7710, -0.6202,  0.3701,  0.4644,  0.1815,  0.7238,\n",
      "         0.7238,  0.9832, -0.5966, -0.2901, -1.4454,  0.3465, -1.6812,  0.3701,\n",
      "         0.6531,  0.6295,  0.5823, -0.5023,  0.6767,  0.4880, -1.4219,  0.9832,\n",
      "         0.4173, -1.1153, -1.2097,  0.4409, -0.4316, -0.0307, -1.4690,  0.3230,\n",
      "        -0.1486, -1.2097, -1.7991,  0.6295, -1.5869,  0.8889,  0.3937,  0.9360,\n",
      "         0.8653, -0.4551, -1.9878,  0.7710,  0.9832,  0.8181, -0.6909,  0.5352,\n",
      "         0.3465,  0.9360,  0.5588,  0.7945,  0.7710,  0.7002,  0.9596, -1.1389,\n",
      "         0.2522,  0.7474,  0.9596,  0.5588,  0.8417, -1.7756,  0.9360,  0.9596,\n",
      "         0.6059, -0.3608, -1.5398,  0.5116,  0.8889,  0.5352, -2.3414,  0.5588,\n",
      "        -1.3983,  0.3465, -0.7617,  0.5116,  0.8889,  0.7002,  0.5352,  0.3230,\n",
      "        -0.1250,  0.4409,  0.6767,  0.8417, -0.4787,  0.2994,  0.4409,  0.6531,\n",
      "        -0.4316,  0.7945,  0.9124, -2.3886,  0.2994,  0.4409,  0.6295,  0.0872,\n",
      "         0.4173,  0.8181,  0.7002,  0.3937, -1.8463,  0.5352,  0.7238,  0.7710,\n",
      "        -1.4690, -1.5398, -2.3650,  0.9124,  0.5588,  1.0303,  0.4409,  0.6767,\n",
      "         0.8181, -2.5065,  0.7002,  0.5352, -1.4219,  0.3701, -1.7520,  0.6767,\n",
      "         0.4880,  0.9360, -0.8324, -1.7284,  0.4409, -1.3040,  0.8889,  0.9360,\n",
      "         0.4644, -1.8463,  0.8889,  0.6767, -1.3276,  0.6295,  0.6531, -2.1528,\n",
      "         0.8889, -1.5633,  0.4880, -0.2193,  0.9832,  0.7710,  0.8653,  0.7002,\n",
      "         0.6059,  0.7710, -1.6577,  0.4880,  0.6767, -0.2193,  1.0303,  0.4409,\n",
      "        -2.2235,  0.5823,  0.7710,  1.0303,  0.8889,  0.8417, -2.4358, -1.7048,\n",
      "         0.1108,  0.5588, -0.8324,  0.9832, -1.1389,  0.6295,  0.7238, -0.4316,\n",
      "         0.9124,  0.6059,  1.0303,  0.6295,  0.4409,  0.7474,  1.0539, -0.4551,\n",
      "         0.5116,  0.9832,  0.6531,  0.9832,  0.6531,  0.7002,  0.1343,  0.9360,\n",
      "        -0.6438,  0.6059, -0.5259,  1.0539,  0.7002,  0.8417, -1.7048,  0.2522,\n",
      "         0.9360,  0.7238,  0.4409, -2.3886, -1.3511, -1.3747,  0.5352,  0.3465,\n",
      "        -2.2471,  0.9596,  0.3701,  0.3701,  0.3701,  0.2287, -0.6673,  0.9596,\n",
      "         1.0303, -1.5633,  1.0303, -1.3747,  0.9360, -0.4316,  0.5352, -2.4829,\n",
      "         1.0303,  0.9360,  0.3701,  0.0636,  0.5823,  0.4173, -0.2901,  0.3937,\n",
      "         0.5116, -0.2429,  0.8417,  0.8653, -2.0821,  0.9124,  0.6531, -0.3608,\n",
      "         0.9124,  0.3701,  0.3230,  1.0068,  0.6767,  0.9360,  0.9124, -1.3983,\n",
      "         0.7238,  0.5352,  1.0303,  0.6295, -2.0349,  0.8889, -1.3983,  0.8889,\n",
      "         0.3465,  1.0539,  0.5823,  0.5823,  0.8653, -1.8463,  0.3230,  0.5352,\n",
      "         0.6767,  0.3230,  0.3701,  0.8653, -1.0210,  0.3230, -1.2804,  0.3465,\n",
      "         0.1815, -1.7520, -0.0779,  0.9360,  0.3701,  1.0068,  0.5823, -1.6105,\n",
      "         0.3701, -2.3179,  0.7002,  1.0068, -1.3747,  0.4644,  0.3701, -0.3372,\n",
      "        -1.5398,  0.5352,  0.7238,  1.0303,  0.5588,  0.4880,  1.0539,  0.6767,\n",
      "         0.7945,  0.9360, -1.3040,  0.3701,  0.3701,  0.5588,  0.7474,  0.7002,\n",
      "         0.7002,  0.5116, -1.3511,  1.0539,  0.7474, -1.7284, -1.3747,  0.0636,\n",
      "         0.4409,  0.8181,  1.0303, -0.3137,  0.7002, -1.6577,  0.9124,  0.6767,\n",
      "        -2.5772,  0.4173, -0.8560, -2.4358, -0.4080, -0.0779, -2.1292, -2.2000,\n",
      "         0.6059,  1.0068, -2.5301,  0.7238, -1.4926, -2.4829,  0.4173, -1.3511,\n",
      "         0.4173,  0.8417, -0.1958,  0.9596,  0.6295,  0.3701,  0.5588,  0.5588,\n",
      "        -2.0821,  1.0068,  0.9124, -0.6202, -0.6202, -1.0682,  0.4644, -1.7756,\n",
      "         0.8181,  0.2758, -1.1153, -0.2665,  0.6531,  0.4409,  0.3465,  0.4880,\n",
      "         0.8889, -0.4080,  0.8417,  0.8417,  0.5588,  0.9596, -2.4122,  0.7474,\n",
      "        -2.5537, -1.1389,  0.2287, -1.0918,  0.3465,  0.4880, -0.6202,  0.5352,\n",
      "         0.4880,  0.7474,  0.3937,  0.0164,  0.5116,  1.0539, -1.0682,  0.5823,\n",
      "        -1.5162, -1.7284,  0.3701,  0.5352,  0.7002,  0.9124,  0.3230, -0.1250,\n",
      "         0.8653,  0.5588, -1.6341, -1.4926,  0.5823, -2.3179,  0.3701,  0.3937,\n",
      "         0.6531,  0.4880,  0.8889,  0.5352,  0.5823, -0.1015,  0.8653,  0.7945,\n",
      "         1.0539,  0.4644,  0.3230,  0.8889, -1.2097, -2.1528,  0.9596, -1.3511,\n",
      "        -0.2193, -1.5633,  0.7002, -0.2429,  0.9360,  0.1815,  0.8889,  0.4880,\n",
      "        -1.9170,  0.8417,  0.6531,  0.6059, -2.2235,  0.4644,  0.3230,  0.7238,\n",
      "         0.3701, -1.5633,  0.2994, -0.8324,  0.4173, -1.3040,  0.3937,  0.9360,\n",
      "        -0.7852,  0.7710,  0.6059,  0.4409,  0.2758,  0.8889,  0.3701, -1.0918,\n",
      "        -1.7520, -2.2000,  0.6059,  0.6767,  0.7710,  0.8653,  0.0636, -0.2193,\n",
      "         0.6059, -1.0446,  0.8889, -1.3747,  1.0539, -0.3608,  0.7474,  0.6295,\n",
      "        -1.4926,  0.6767,  0.3701,  0.9596,  0.7238, -0.7381,  0.7710,  0.8889,\n",
      "        -0.6202,  0.5823,  1.0068,  0.7002,  0.9596, -0.7617,  0.4173,  0.5116,\n",
      "         0.8417,  0.6295, -0.7381,  0.9596, -0.5966,  0.7002, -0.6202,  0.4409,\n",
      "         0.4173,  0.9124, -1.0918,  0.2994,  0.7945,  0.6059,  0.4644,  0.6767,\n",
      "         0.6295, -0.4787, -1.1625, -1.7284, -0.0779,  0.2758,  0.5352,  0.6059,\n",
      "         0.3701, -0.3137,  0.4880,  0.5823,  0.3465,  0.4173, -0.8560,  0.7945,\n",
      "         0.7238, -1.5633, -0.9974,  0.7710, -0.3844,  0.4173, -0.9503,  0.8889,\n",
      "        -0.7852, -0.5259, -1.7520,  0.4173,  0.8653,  0.6059,  0.8889,  0.6767,\n",
      "         0.8653,  0.5588,  0.9832, -0.1722,  0.2994, -0.1958,  0.8889,  0.7710,\n",
      "         1.0539,  0.3937,  0.5823,  0.3465,  0.3937,  0.4644,  0.4409, -0.7145,\n",
      "         0.7002,  0.8653, -0.1486, -1.0918,  0.0164,  0.7002,  0.0636, -2.2235,\n",
      "        -1.3983,  0.8889,  0.7002, -0.0779,  1.0539,  0.6059, -1.8699,  0.4644,\n",
      "         0.8889, -1.0210, -1.5869,  0.7002,  0.2994,  1.0303,  0.9124,  0.5588,\n",
      "        -0.3844, -2.5537,  0.8417,  0.5116,  0.6059,  0.4409, -1.6577,  0.7238,\n",
      "        -0.0543,  0.9360,  0.5352,  0.4173,  0.8653, -0.1722, -0.2665,  0.1343,\n",
      "         0.7474, -1.5869, -1.7284,  0.8181, -0.3137,  0.3701, -0.7852,  1.0068,\n",
      "        -1.2804,  0.4644, -0.0779, -0.7381, -2.4593,  0.6059,  0.4880,  0.7238,\n",
      "        -2.0585,  1.0303,  0.7945, -1.7520,  0.9832, -2.5065, -1.2332,  0.6295,\n",
      "         0.3937, -0.5730,  0.5352,  0.4409, -2.4593,  0.3465,  0.9832, -1.8699,\n",
      "         0.7710,  0.7945,  0.9124,  1.0068,  1.0303,  0.5116, -0.2193, -2.0349,\n",
      "        -1.5162,  1.0303,  0.5588, -1.4454,  0.0636,  0.5823,  0.4644,  0.7002,\n",
      "         0.3701,  0.8653,  0.7238,  0.7238,  0.7474,  0.3701,  0.5588,  0.6531,\n",
      "        -0.1015,  0.9360,  0.8653, -2.4829,  0.7002,  0.9124,  0.6295,  0.9596,\n",
      "        -2.1057,  0.8417, -1.8934,  0.7710,  0.5352, -2.0585,  0.7474,  0.5116,\n",
      "         0.8889, -1.5869,  0.8417, -1.3747,  0.7002,  0.8653, -1.7520,  0.3937,\n",
      "         0.7945, -2.2707,  0.3465,  0.8653,  0.5823, -1.5162,  0.7002,  0.4880,\n",
      "        -0.5966, -0.4080, -0.5259, -0.2901,  0.8181, -0.0543,  0.2994,  0.8889,\n",
      "        -1.7284,  0.6767,  1.0539,  0.6059, -0.0779,  0.3230, -0.5966,  0.2522,\n",
      "         0.9124,  0.9360,  0.4409,  0.3701,  0.5116,  0.6531,  0.6531,  0.4644,\n",
      "         0.9832, -2.3414, -0.5023, -0.3137,  0.6295,  0.7002, -0.3372, -2.0349,\n",
      "         0.3465, -1.1153,  0.3701,  0.4644, -1.4454,  0.8889,  0.9596,  0.4880,\n",
      "        -0.0543,  0.3937,  0.7002, -0.3608, -1.4926,  0.9124, -0.7852,  0.4880,\n",
      "         0.2287,  0.8889,  0.9360,  0.9596,  0.8653, -2.3414,  0.7474,  0.3937,\n",
      "         0.9832, -0.5966,  0.3465, -1.3511,  0.5823,  0.4173,  0.1579,  0.7474,\n",
      "         0.6531, -1.6105,  0.4880,  0.8181,  0.3230,  0.9596,  0.4644, -0.6673,\n",
      "        -0.1015, -1.4926,  1.0068,  0.7474,  0.6295,  0.4173, -2.4358,  0.3230,\n",
      "         0.6295,  0.5588, -2.3886,  0.8889,  0.5823,  0.5352, -0.0307,  0.3230,\n",
      "         0.9124, -0.4316, -1.3747,  0.1579, -1.7048, -1.5633,  0.5823,  0.9832,\n",
      "         0.1108,  1.0303, -0.0307,  0.8181, -0.8796, -0.8324,  1.0068,  0.6531,\n",
      "        -2.5301,  0.5588,  1.0303, -0.6202, -0.0543,  0.6295, -0.5966,  0.5116,\n",
      "         0.2994,  0.4644,  0.2522,  0.7238,  0.0164,  0.7238,  0.3465,  0.7710,\n",
      "         0.4409,  0.9360,  1.0539,  0.7474,  0.7710, -1.7991,  0.8181, -0.1958,\n",
      "         0.7002,  0.3230,  0.2994, -1.6341,  0.4409,  0.5823,  0.7710, -0.1958,\n",
      "         0.7474,  0.6531,  0.7238,  0.5823,  0.5352,  0.5588, -0.9739,  0.8181,\n",
      "         0.7710,  0.4644, -0.7381,  0.7238, -2.0821,  0.1343,  0.4409,  0.0636])\u001b[0m\n",
      "\u001b[34m14:50:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m14:51:05 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.362\u001b[0m\n",
      "\u001b[34m14:51:05 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 2.035\u001b[0m\n",
      "\u001b[34m14:51:05 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8649309873580933\u001b[0m\n",
      "\u001b[32m14:51:05 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:51:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m14:51:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 1.630\u001b[0m\n",
      "\u001b[34m14:51:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.428\u001b[0m\n",
      "\u001b[34m14:51:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9041872024536133\u001b[0m\n",
      "\u001b[32m14:51:21 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:51:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m14:51:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.891\u001b[0m\n",
      "\u001b[34m14:51:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.284\u001b[0m\n",
      "\u001b[34m14:51:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9157894849777222\u001b[0m\n",
      "\u001b[32m14:51:37 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:51:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m14:51:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.663\u001b[0m\n",
      "\u001b[34m14:51:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.152\u001b[0m\n",
      "\u001b[34m14:51:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9210456609725952\u001b[0m\n",
      "\u001b[32m14:51:54 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:51:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "\u001b[34m14:52:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.540\u001b[0m\n",
      "\u001b[34m14:52:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.166\u001b[0m\n",
      "\u001b[34m14:52:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9185675382614136\u001b[0m\n",
      "\u001b[32m14:52:10 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:52:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "\u001b[34m14:52:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.471\u001b[0m\n",
      "\u001b[34m14:52:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.028\u001b[0m\n",
      "\u001b[34m14:52:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9291154146194458\u001b[0m\n",
      "\u001b[32m14:52:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:52:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "\u001b[34m14:52:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.408\u001b[0m\n",
      "\u001b[34m14:52:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.129\u001b[0m\n",
      "\u001b[34m14:52:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9223154783248901\u001b[0m\n",
      "\u001b[32m14:52:41 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:52:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "\u001b[34m14:52:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.354\u001b[0m\n",
      "\u001b[34m14:52:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.009\u001b[0m\n",
      "\u001b[34m14:52:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.930173397064209\u001b[0m\n",
      "\u001b[32m14:52:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:52:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "\u001b[34m14:53:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.340\u001b[0m\n",
      "\u001b[34m14:53:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.070\u001b[0m\n",
      "\u001b[34m14:53:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.9254475831985474\u001b[0m\n",
      "\u001b[32m14:53:13 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m14:53:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "\u001b[34m14:53:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.347\u001b[0m\n",
      "\u001b[34m14:53:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.076\u001b[0m\n",
      "\u001b[34m14:53:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.925439715385437\u001b[0m\n",
      "\u001b[32m14:53:28 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 44/44 [00:14<00:00,  3.07it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.47it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.14it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.53it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.17it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.36it/s]\n",
      "100%|██████████| 44/44 [00:14<00:00,  3.13it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.32it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.18it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.58it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.75it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.16it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.25it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.17it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.63it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.15it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.67it/s]\n",
      "100%|██████████| 44/44 [00:13<00:00,  3.18it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.55it/s]\n",
      "\n",
      "comparing\n",
      "stdout: \n",
      "stderr: Traceback (most recent call last):\n",
      "  File \"/home/laurinemeier/swerick/compare_models_regression.py\", line 130, in <module>\n",
      "    main(args)\n",
      "  File \"/home/laurinemeier/swerick/compare_models_regression.py\", line 94, in main\n",
      "    input_ids, attention_masks, labels = encode(df, tokenizer)\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/swerick/compare_models_regression.py\", line 41, in encode\n",
      "    labels = torch.tensor(df['tag'].tolist())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many dimensions 'str'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regression_year(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-801500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for ix, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['content'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    print(df[\"tag\"])\n",
    "    labels = torch.tensor(df['tag'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.346546\n",
      "1       0.676650\n",
      "2       0.770966\n",
      "3      -2.270707\n",
      "4      -0.596608\n",
      "          ...   \n",
      "4995    0.205073\n",
      "4996   -2.341444\n",
      "4997    0.228652\n",
      "4998    0.228652\n",
      "4999   -0.313662\n",
      "Name: tag, Length: 5000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Create binary label where seg = 1\n",
    "df = df[df[\"content\"].notnull()]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "input_ids, attention_masks, labels = encode(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3465,  0.6767,  0.7710,  ...,  0.2287,  0.2287, -0.3137])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1399: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/home/laurinemeier/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f9eda7965b4212b1be4ee6874b1e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_long.config.name_or_path}-imdb\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    # Add other training arguments as needed\n",
    "    logging_steps=892,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True\n",
    ")\n",
    "print(training_args.device)\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model_long,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8190011382102966,\n",
       " 'eval_runtime': 1639.8287,\n",
       " 'eval_samples_per_second': 7.575,\n",
       " 'eval_steps_per_second': 0.119}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2682330542912186"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(result['eval_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
