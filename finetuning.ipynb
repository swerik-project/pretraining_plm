{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from data_swerick import create_dataset_swerick\n",
    "from evaluation import evaluation_task\n",
    "import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch,data_collator):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model = preprocessing.create_model_MLM(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =preprocessing.create_tokenizer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 12319\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 2683\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#datasest\n",
    "data_files = {\"train\": \"swerick_data_random_train.pkl\", \"test\": \"swerick_data_random_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242041d06ef440b090f781dfdf789bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets =preprocessing.tokenize_dataset(swerick_dataset,tokenizer)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tokenized_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name ['protocole'] not in the dataset. Current columns in the dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_datasets\u001b[38;5;241m=\u001b[39mtokenized_datasets\u001b[38;5;241m.\u001b[39mremove_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:366\u001b[0m, in \u001b[0;36mDatasetDict.remove_columns\u001b[0;34m(self, column_names)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: dataset\u001b[38;5;241m.\u001b[39mremove_columns(column_names\u001b[38;5;241m=\u001b[39mcolumn_names) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:366\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: dataset\u001b[38;5;241m.\u001b[39mremove_columns(column_names\u001b[38;5;241m=\u001b[39mcolumn_names) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m func(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2160\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2158\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2163\u001b[0m     )\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n",
      "\u001b[0;31mValueError\u001b[0m: Column name ['protocole'] not in the dataset. Current columns in the dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids']"
     ]
    }
   ],
   "source": [
    "tokenized_datasets=tokenized_datasets.remove_columns(\"protocole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm_datasets \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mgrouping_dataset(tokenized_datasets,chunk_size)\n\u001b[1;32m      2\u001b[0m lm_datasets\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "lm_datasets = preprocessing.grouping_dataset(tokenized_datasets,chunk_size)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lm_datasets,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm_dataset.pkl\",\"rb\") as f:\n",
    "    lm_datasets= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid={\"valid\":\"swerick_data_random_valid.pkl\"}\n",
    "valid_dataset = load_dataset(\"pandas\",data_files=data_valid) \n",
    "valid_dataset =preprocessing.tokenize_dataset(valid_dataset,tokenizer)\n",
    "valid_dataset=preprocessing.grouping_dataset(valid_dataset,chunk_size)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_dataset.pkl\",\"wb\") as f:\n",
    "     pickle.dump(valid_dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 800106\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"valid_dataset.pkl\",\"rb\") as f:\n",
    "    valid_dataset= pickle.load(f)\n",
    "\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset=valid_dataset.remove_columns([\"word_ids\",\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = preprocessing.data_collector_masking(tokenizer,0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial with a manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "    num_rows: 3663965\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column valid not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(lm_dataset_bis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mcreate_deterministic_eval_dataset(lm_dataset_bis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],data_collator)\n\u001b[0;32m----> 7\u001b[0m valid_dataset\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mcreate_deterministic_eval_dataset(valid_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m],data_collator)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_dataset)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2810\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2794\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2792\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m   2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2796\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2797\u001b[0m )\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 580\u001b[0m     _check_valid_column_key(key, table\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column valid not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']\""
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"])\n",
    "\n",
    "lm_dataset_bis = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "\n",
    "print(lm_dataset_bis[\"test\"])\n",
    "eval_dataset = preprocessing.create_deterministic_eval_dataset(lm_dataset_bis[\"test\"],data_collator)\n",
    "valid_dataset=preprocessing.create_deterministic_eval_dataset(valid_dataset[\"valid\"],data_collator)\n",
    "\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = preprocessing.create_dataloader(lm_dataset_bis[\"train\"],batch_size,data_collator)\n",
    "def to_device(batch):\n",
    "    return {key: value.to(\"cpu\") for key, value in batch.items()}\n",
    "\n",
    "print(\"ok\")\n",
    "eval_dataloader = preprocessing.create_dataloader(eval_dataset,batch_size,default_data_collator)\n",
    "valid_dataloader=preprocessing.create_dataloader(valid_dataset,batch_size,default_data_collator)\n",
    "print(\"ok\")\n",
    "\n",
    "#for batch in train_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "#for batch in eval_dataloader:\n",
    "    #batch = to_device(batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(eval_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3663965\n",
      "})\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x775620217590>\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader.dataset)\n",
    "print(eval_dataloader)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_ids\"].device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##försvar, industriellberedskap, ekonomiskberedskapo. s. v. Manharräknatmedolikakostnaderfördessaochsedanförsöktfog [MASK]hopdessabyggklotsartillenen [MASK] 90Nr10Onsdagenden23 [MASK]s1955Överbefälhavarensutredningrörandekrigsmaktensutvecklinghetligbyggnad. [UNK]. HerrHjalmarsonvari rest [MASK]åenfråga [MASK] somjagmednåg [MASK]ynpunktervillspinnavidarep [MASK]. Vilkenärsj [MASK]agrundenförÖB - utredningens [MASK] [MASK]? [MASK], minadamerochherrar, det [MASK], attdetta [MASK]representerardetminimumsom\n",
      "##inläggetskullekanskeintebehövanågotgenmäle, tyjagkonstaterarmedtillfreds [MASK], attherr [MASK]rhén [MASK]kommitmerapappersliknandevåglängdsomjagäridennas [MASK], [MASK]jagnufåruttryckamigså. Jagvilldockgöraetttillugerläggande. Manharpåvissthåll [MASK]tförlöjligavadsomskeddeiuniversitetsutredningengenomatts [MASK]attordföranden, d. v [MASK] [MASK]. statssekreterare [MASK]denman, skrevtillsigsjälv. Det [MASK]håller [MASK] [MASK]allspådets [MASK]t, utanvidenavdeförsta [MASK]gångarnaunderhöstens\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader():\n",
    "    train =DataLoader(\n",
    "    lm_dataset_bis[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n",
    "    train = [inputs.to(device) for inputs in train_dataloader]\n",
    "    return train\n",
    "\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n",
    "for step,batch in enumerate(get_dataloader()):\n",
    "    print(\n",
    "        tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_bis = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_bis=model_bis.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bis.eval()\n",
    "\n",
    "total_loss = 0.0  # Variable to accumulate total loss\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bis(**batch)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()   # Accumulate the batch loss\n",
    "\n",
    "# Calculate the average loss\n",
    "average_loss = total_loss / len(eval_dataloader)\n",
    "\n",
    "print(f\"Initial Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model_bis.parameters(), lr=1.3e-5)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "losses_train=[]\n",
    "losses_test=[]\n",
    "#train_dataloader = get_dataloader()\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model_bis.train()\n",
    "    print(next(model_bis.parameters()).device)\n",
    "    print(epoch)\n",
    "    params_before_optimization = [param.data.clone() for param in model_bis.parameters()]\n",
    "    total_loss_train = 0.0 \n",
    "    train_dataloader = get_dataloader()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model_bis(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        params_after_optimization = [param.data for param in model_bis.parameters()]\n",
    "        parameters_changed = any((param_before != param_after).any() for param_before, param_after in zip(params_before_optimization, params_after_optimization))\n",
    "        #if parameters_changed==True :\n",
    "             # print(parameters_changed) \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    losses_train.append(total_loss_train/len(train_dataloader))\n",
    "    print(\"losses_train\",losses_train)\n",
    "\n",
    "    # Evaluation\n",
    "    model_bis.eval()\n",
    "    losses=[]\n",
    "    total_loss_eval=0.0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bis(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        total_loss_eval +=loss.item()\n",
    "\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "       perplexity = float(\"inf\")\n",
    "\n",
    "    losses_test.append(total_loss_eval/len(eval_dataloader))\n",
    "\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    print(\"losses_test\",losses_test)\n",
    "\n",
    "print(\"epoch\",num_train_epochs)\n",
    "plt.plot(range(num_train_epochs),losses_train,label=\"train Loss\")\n",
    "\n",
    "plt.plot(range(num_train_epochs),losses_test,label=\"test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_train)\n",
    "print(losses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"finetuning_manual\"\n",
    "model_bis.save_pretrained(file_path)\n",
    "tokenizer.save_pretrained(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"losses.pkl\"\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump({'losses_train': losses_train, 'losses_test': losses_test}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"./test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_long=AutoModelForMaskedLM.from_pretrained(\"./finetuning_hugging-finetuned-imdb/checkpoint-259384\")\n",
    "model_long=model_long.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(\"./test_model\")\n",
    "model=model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hugging_face = AutoModelForMaskedLM.from_pretrained(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-572500\")\n",
    "model_hugging_face=model_hugging_face.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_kb=AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model_kb=model_kb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n",
      "Accuracy...\n",
      "Party classification\n",
      "training\n",
      "stdout: ['\"vänstern\"', 'Andra kammarens center', 'Andra kammarens frihandelsparti', 'Bondeförbundet', 'Centern (partigrupp 1873-1882)', 'Centern (partigrupp 1885-1887)', 'Centerpartiet', 'Det förenade högerpartiet', 'Ehrenheimska partiet', 'Folkpartiet', 'Folkpartiet (1895–1900)', 'Friesenska diskussionsklubben', 'Frihandelsvänliga centern', 'Frisinnade folkpartiet', 'Frisinnade försvarsvänner', 'Frisinnade landsföreningen', 'Första kammarens konservativa grupp', 'Första kammarens ministeriella grupp', 'Första kammarens minoritetsparti', 'Första kammarens moderata parti', 'Första kammarens nationella parti', 'Första kammarens protektionistiska parti', 'Gamla lantmannapartiet', 'Högerns riksdagsgrupp', 'Högerpartiet', 'Högerpartiet de konservativa', 'Jordbrukarnas fria grupp', 'Junkerpartiet', 'Kilbomspartiet', 'Kommunistiska partiet', 'Kristdemokraterna', 'Lantmanna- och borgarepartiet inom andrakammaren', 'Lantmannapartiet', 'Lantmannapartiets filial', 'Liberala riksdagspartiet', 'Liberala samlingspartiet', 'Liberalerna', 'Medborgerlig samling (1964–1968)', 'Miljöpartiet', 'Moderaterna', 'Nationella framstegspartiet', 'Ny demokrati', 'Nya centern (partigrupp 1883-1887)', 'Nya lantmannapartiet', 'Nyliberala partiet', 'Skånska partiet', 'Socialdemokraterna', 'Socialdemokratiska vänstergruppen', 'Socialistiska partiet', 'Stockholmsbänken', 'Sverigedemokraterna', 'Sveriges kommunistiska parti', 'Vänsterpartiet', 'borgmästarepartiet', 'frihandelsvänlig vilde', 'frisinnad vilde', 'högervilde', 'ministeriella partiet', 'partilös', 'politisk vilde', 'vänstervilde']\n",
      "{0: '\"vänstern\"', 1: 'Andra kammarens center', 2: 'Andra kammarens frihandelsparti', 3: 'Bondeförbundet', 4: 'Centern (partigrupp 1873-1882)', 5: 'Centern (partigrupp 1885-1887)', 6: 'Centerpartiet', 7: 'Det förenade högerpartiet', 8: 'Ehrenheimska partiet', 9: 'Folkpartiet', 10: 'Folkpartiet (1895–1900)', 11: 'Friesenska diskussionsklubben', 12: 'Frihandelsvänliga centern', 13: 'Frisinnade folkpartiet', 14: 'Frisinnade försvarsvänner', 15: 'Frisinnade landsföreningen', 16: 'Första kammarens konservativa grupp', 17: 'Första kammarens ministeriella grupp', 18: 'Första kammarens minoritetsparti', 19: 'Första kammarens moderata parti', 20: 'Första kammarens nationella parti', 21: 'Första kammarens protektionistiska parti', 22: 'Gamla lantmannapartiet', 23: 'Högerns riksdagsgrupp', 24: 'Högerpartiet', 25: 'Högerpartiet de konservativa', 26: 'Jordbrukarnas fria grupp', 27: 'Junkerpartiet', 28: 'Kilbomspartiet', 29: 'Kommunistiska partiet', 30: 'Kristdemokraterna', 31: 'Lantmanna- och borgarepartiet inom andrakammaren', 32: 'Lantmannapartiet', 33: 'Lantmannapartiets filial', 34: 'Liberala riksdagspartiet', 35: 'Liberala samlingspartiet', 36: 'Liberalerna', 37: 'Medborgerlig samling (1964–1968)', 38: 'Miljöpartiet', 39: 'Moderaterna', 40: 'Nationella framstegspartiet', 41: 'Ny demokrati', 42: 'Nya centern (partigrupp 1883-1887)', 43: 'Nya lantmannapartiet', 44: 'Nyliberala partiet', 45: 'Skånska partiet', 46: 'Socialdemokraterna', 47: 'Socialdemokratiska vänstergruppen', 48: 'Socialistiska partiet', 49: 'Stockholmsbänken', 50: 'Sverigedemokraterna', 51: 'Sveriges kommunistiska parti', 52: 'Vänsterpartiet', 53: 'borgmästarepartiet', 54: 'frihandelsvänlig vilde', 55: 'frisinnad vilde', 56: 'högervilde', 57: 'ministeriella partiet', 58: 'partilös', 59: 'politisk vilde', 60: 'vänstervilde'}\n",
      "61\n",
      "\u001b[32m16:16:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:16:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:16:51 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([58, 36, 55,  ...,  9, 21,  1])\u001b[0m\n",
      "\u001b[34m16:16:52 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:17:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 64.772\u001b[0m\n",
      "\u001b[34m16:17:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 60.577\u001b[0m\n",
      "\u001b[34m16:17:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.05287756398320198\u001b[0m\n",
      "\u001b[32m16:17:57 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:17:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:19:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 57.211\u001b[0m\n",
      "\u001b[34m16:19:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 55.534\u001b[0m\n",
      "\u001b[34m16:19:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.08467742055654526\u001b[0m\n",
      "\u001b[32m16:19:04 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:05 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:20:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 52.579\u001b[0m\n",
      "\u001b[34m16:20:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 53.322\u001b[0m\n",
      "\u001b[34m16:20:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.10575512796640396\u001b[0m\n",
      "\u001b[32m16:20:11 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:20:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:21:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 48.962\u001b[0m\n",
      "\u001b[34m16:21:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 52.248\u001b[0m\n",
      "\u001b[34m16:21:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.11840175837278366\u001b[0m\n",
      "\u001b[32m16:21:18 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:21:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "\u001b[34m16:22:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 45.137\u001b[0m\n",
      "\u001b[34m16:22:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.800\u001b[0m\n",
      "\u001b[34m16:22:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.13352271914482117\u001b[0m\n",
      "\u001b[32m16:22:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:22:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "\u001b[34m16:23:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 41.202\u001b[0m\n",
      "\u001b[34m16:23:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.569\u001b[0m\n",
      "\u001b[34m16:23:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.14406158030033112\u001b[0m\n",
      "\u001b[32m16:23:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:23:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "\u001b[34m16:24:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 37.459\u001b[0m\n",
      "\u001b[34m16:24:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.296\u001b[0m\n",
      "\u001b[34m16:24:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.14910189807415009\u001b[0m\n",
      "\u001b[32m16:24:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:24:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "\u001b[34m16:25:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 34.147\u001b[0m\n",
      "\u001b[34m16:25:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.212\u001b[0m\n",
      "\u001b[34m16:25:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.14910189807415009\u001b[0m\n",
      "\u001b[32m16:25:38 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:25:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "\u001b[34m16:26:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 31.515\u001b[0m\n",
      "\u001b[34m16:26:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.464\u001b[0m\n",
      "\u001b[34m16:26:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.15918254852294922\u001b[0m\n",
      "\u001b[32m16:26:41 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:26:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "\u001b[34m16:27:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 30.259\u001b[0m\n",
      "\u001b[34m16:27:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.157\u001b[0m\n",
      "\u001b[34m16:27:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.15918254852294922\u001b[0m\n",
      "\u001b[32m16:27:42 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.18it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.08it/s]\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.17it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.00it/s]\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.16it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  8.98it/s]\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.16it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.02it/s]\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.16it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.01it/s]\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.17it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.30it/s]\n",
      "100%|██████████| 186/186 [00:56<00:00,  3.31it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.66it/s]\n",
      "100%|██████████| 186/186 [00:55<00:00,  3.38it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.14it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.39it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.65it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.63it/s]\n",
      "\n",
      "stdout: ['\"vänstern\"', 'Andra kammarens center', 'Andra kammarens frihandelsparti', 'Bondeförbundet', 'Centern (partigrupp 1873-1882)', 'Centern (partigrupp 1885-1887)', 'Centerpartiet', 'Det förenade högerpartiet', 'Ehrenheimska partiet', 'Folkpartiet', 'Folkpartiet (1895–1900)', 'Friesenska diskussionsklubben', 'Frihandelsvänliga centern', 'Frisinnade folkpartiet', 'Frisinnade försvarsvänner', 'Frisinnade landsföreningen', 'Första kammarens konservativa grupp', 'Första kammarens ministeriella grupp', 'Första kammarens minoritetsparti', 'Första kammarens moderata parti', 'Första kammarens nationella parti', 'Första kammarens protektionistiska parti', 'Gamla lantmannapartiet', 'Högerns riksdagsgrupp', 'Högerpartiet', 'Högerpartiet de konservativa', 'Jordbrukarnas fria grupp', 'Junkerpartiet', 'Kilbomspartiet', 'Kommunistiska partiet', 'Kristdemokraterna', 'Lantmanna- och borgarepartiet inom andrakammaren', 'Lantmannapartiet', 'Lantmannapartiets filial', 'Liberala riksdagspartiet', 'Liberala samlingspartiet', 'Liberalerna', 'Medborgerlig samling (1964–1968)', 'Miljöpartiet', 'Moderaterna', 'Nationella framstegspartiet', 'Ny demokrati', 'Nya centern (partigrupp 1883-1887)', 'Nya lantmannapartiet', 'Nyliberala partiet', 'Skånska partiet', 'Socialdemokraterna', 'Socialdemokratiska vänstergruppen', 'Socialistiska partiet', 'Stockholmsbänken', 'Sverigedemokraterna', 'Sveriges kommunistiska parti', 'Vänsterpartiet', 'borgmästarepartiet', 'frihandelsvänlig vilde', 'frisinnad vilde', 'högervilde', 'ministeriella partiet', 'partilös', 'politisk vilde', 'vänstervilde']\n",
      "{0: '\"vänstern\"', 1: 'Andra kammarens center', 2: 'Andra kammarens frihandelsparti', 3: 'Bondeförbundet', 4: 'Centern (partigrupp 1873-1882)', 5: 'Centern (partigrupp 1885-1887)', 6: 'Centerpartiet', 7: 'Det förenade högerpartiet', 8: 'Ehrenheimska partiet', 9: 'Folkpartiet', 10: 'Folkpartiet (1895–1900)', 11: 'Friesenska diskussionsklubben', 12: 'Frihandelsvänliga centern', 13: 'Frisinnade folkpartiet', 14: 'Frisinnade försvarsvänner', 15: 'Frisinnade landsföreningen', 16: 'Första kammarens konservativa grupp', 17: 'Första kammarens ministeriella grupp', 18: 'Första kammarens minoritetsparti', 19: 'Första kammarens moderata parti', 20: 'Första kammarens nationella parti', 21: 'Första kammarens protektionistiska parti', 22: 'Gamla lantmannapartiet', 23: 'Högerns riksdagsgrupp', 24: 'Högerpartiet', 25: 'Högerpartiet de konservativa', 26: 'Jordbrukarnas fria grupp', 27: 'Junkerpartiet', 28: 'Kilbomspartiet', 29: 'Kommunistiska partiet', 30: 'Kristdemokraterna', 31: 'Lantmanna- och borgarepartiet inom andrakammaren', 32: 'Lantmannapartiet', 33: 'Lantmannapartiets filial', 34: 'Liberala riksdagspartiet', 35: 'Liberala samlingspartiet', 36: 'Liberalerna', 37: 'Medborgerlig samling (1964–1968)', 38: 'Miljöpartiet', 39: 'Moderaterna', 40: 'Nationella framstegspartiet', 41: 'Ny demokrati', 42: 'Nya centern (partigrupp 1883-1887)', 43: 'Nya lantmannapartiet', 44: 'Nyliberala partiet', 45: 'Skånska partiet', 46: 'Socialdemokraterna', 47: 'Socialdemokratiska vänstergruppen', 48: 'Socialistiska partiet', 49: 'Stockholmsbänken', 50: 'Sverigedemokraterna', 51: 'Sveriges kommunistiska parti', 52: 'Vänsterpartiet', 53: 'borgmästarepartiet', 54: 'frihandelsvänlig vilde', 55: 'frisinnad vilde', 56: 'högervilde', 57: 'ministeriella partiet', 58: 'partilös', 59: 'politisk vilde', 60: 'vänstervilde'}\n",
      "61\n",
      "\u001b[32m16:27:45 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:27:46 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:27:49 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([58, 36, 55,  ...,  9, 21,  1])\u001b[0m\n",
      "\u001b[34m16:27:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:28:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 65.231\u001b[0m\n",
      "\u001b[34m16:28:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 61.946\u001b[0m\n",
      "\u001b[34m16:28:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.052419353276491165\u001b[0m\n",
      "\u001b[32m16:28:51 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:28:52 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:29:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 56.806\u001b[0m\n",
      "\u001b[34m16:29:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 53.026\u001b[0m\n",
      "\u001b[34m16:29:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.08064515888690948\u001b[0m\n",
      "\u001b[32m16:29:53 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:29:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:30:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 50.761\u001b[0m\n",
      "\u001b[34m16:30:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 50.603\u001b[0m\n",
      "\u001b[34m16:30:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.12445014715194702\u001b[0m\n",
      "\u001b[32m16:30:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:30:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:31:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 46.579\u001b[0m\n",
      "\u001b[34m16:31:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 48.822\u001b[0m\n",
      "\u001b[34m16:31:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.1450696438550949\u001b[0m\n",
      "\u001b[32m16:31:58 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:31:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "\u001b[34m16:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 42.650\u001b[0m\n",
      "\u001b[34m16:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 48.129\u001b[0m\n",
      "\u001b[34m16:33:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.17127932608127594\u001b[0m\n",
      "\u001b[32m16:33:01 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:33:02 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "\u001b[34m16:34:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 38.948\u001b[0m\n",
      "\u001b[34m16:34:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 48.190\u001b[0m\n",
      "\u001b[34m16:34:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.18035189807415009\u001b[0m\n",
      "\u001b[32m16:34:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:34:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "\u001b[34m16:35:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 35.785\u001b[0m\n",
      "\u001b[34m16:35:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 47.698\u001b[0m\n",
      "\u001b[34m16:35:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.17576979100704193\u001b[0m\n",
      "\u001b[32m16:35:04 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:35:05 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "\u001b[34m16:36:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 33.088\u001b[0m\n",
      "\u001b[34m16:36:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 47.666\u001b[0m\n",
      "\u001b[34m16:36:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.18887463212013245\u001b[0m\n",
      "\u001b[32m16:36:06 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:36:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "\u001b[34m16:37:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 30.814\u001b[0m\n",
      "\u001b[34m16:37:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 47.716\u001b[0m\n",
      "\u001b[34m16:37:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.19189882278442383\u001b[0m\n",
      "\u001b[32m16:37:12 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:37:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "\u001b[34m16:38:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 29.669\u001b[0m\n",
      "\u001b[34m16:38:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 47.682\u001b[0m\n",
      "\u001b[34m16:38:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.18988269567489624\u001b[0m\n",
      "\u001b[32m16:38:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at finetuning_hugging_whitespace-finetuned-imdb/checkpoint-343500 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 186/186 [00:55<00:00,  3.37it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.62it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.39it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.67it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.67it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.66it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.67it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.65it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.64it/s]\n",
      "100%|██████████| 186/186 [00:54<00:00,  3.40it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.65it/s]\n",
      "100%|██████████| 186/186 [00:57<00:00,  3.25it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.03it/s]\n",
      "100%|██████████| 186/186 [00:58<00:00,  3.17it/s]\n",
      "100%|██████████| 62/62 [00:06<00:00,  9.00it/s]\n",
      "\n",
      "comparing\n",
      "stdout: ['\"vänstern\"', 'Andra kammarens center', 'Andra kammarens frihandelsparti', 'Bondeförbundet', 'Centern (partigrupp 1873-1882)', 'Centern (partigrupp 1885-1887)', 'Centerpartiet', 'Det förenade högerpartiet', 'Ehrenheimska partiet', 'Folkpartiet', 'Folkpartiet (1895–1900)', 'Friesenska diskussionsklubben', 'Frihandelsvänliga centern', 'Frisinnade folkpartiet', 'Frisinnade försvarsvänner', 'Frisinnade landsföreningen', 'Första kammarens konservativa grupp', 'Första kammarens ministeriella grupp', 'Första kammarens minoritetsparti', 'Första kammarens moderata parti', 'Första kammarens nationella parti', 'Första kammarens protektionistiska parti', 'Gamla lantmannapartiet', 'Högerns riksdagsgrupp', 'Högerpartiet', 'Högerpartiet de konservativa', 'Jordbrukarnas fria grupp', 'Junkerpartiet', 'Kilbomspartiet', 'Kommunistiska partiet', 'Kristdemokraterna', 'Lantmanna- och borgarepartiet inom andrakammaren', 'Lantmannapartiet', 'Lantmannapartiets filial', 'Liberala riksdagspartiet', 'Liberala samlingspartiet', 'Liberalerna', 'Medborgerlig samling (1964–1968)', 'Miljöpartiet', 'Moderaterna', 'Nationella framstegspartiet', 'Ny demokrati', 'Nya centern (partigrupp 1883-1887)', 'Nya lantmannapartiet', 'Nyliberala partiet', 'Skånska partiet', 'Socialdemokraterna', 'Socialdemokratiska vänstergruppen', 'Socialistiska partiet', 'Stockholmsbänken', 'Sverigedemokraterna', 'Sveriges kommunistiska parti', 'Vänsterpartiet', 'borgmästarepartiet', 'frihandelsvänlig vilde', 'frisinnad vilde', 'högervilde', 'ministeriella partiet', 'partilös', 'politisk vilde', 'vänstervilde']\n",
      "61\n",
      "{0: '\"vänstern\"', 1: 'Andra kammarens center', 2: 'Andra kammarens frihandelsparti', 3: 'Bondeförbundet', 4: 'Centern (partigrupp 1873-1882)', 5: 'Centern (partigrupp 1885-1887)', 6: 'Centerpartiet', 7: 'Det förenade högerpartiet', 8: 'Ehrenheimska partiet', 9: 'Folkpartiet', 10: 'Folkpartiet (1895–1900)', 11: 'Friesenska diskussionsklubben', 12: 'Frihandelsvänliga centern', 13: 'Frisinnade folkpartiet', 14: 'Frisinnade försvarsvänner', 15: 'Frisinnade landsföreningen', 16: 'Första kammarens konservativa grupp', 17: 'Första kammarens ministeriella grupp', 18: 'Första kammarens minoritetsparti', 19: 'Första kammarens moderata parti', 20: 'Första kammarens nationella parti', 21: 'Första kammarens protektionistiska parti', 22: 'Gamla lantmannapartiet', 23: 'Högerns riksdagsgrupp', 24: 'Högerpartiet', 25: 'Högerpartiet de konservativa', 26: 'Jordbrukarnas fria grupp', 27: 'Junkerpartiet', 28: 'Kilbomspartiet', 29: 'Kommunistiska partiet', 30: 'Kristdemokraterna', 31: 'Lantmanna- och borgarepartiet inom andrakammaren', 32: 'Lantmannapartiet', 33: 'Lantmannapartiets filial', 34: 'Liberala riksdagspartiet', 35: 'Liberala samlingspartiet', 36: 'Liberalerna', 37: 'Medborgerlig samling (1964–1968)', 38: 'Miljöpartiet', 39: 'Moderaterna', 40: 'Nationella framstegspartiet', 41: 'Ny demokrati', 42: 'Nya centern (partigrupp 1883-1887)', 43: 'Nya lantmannapartiet', 44: 'Nyliberala partiet', 45: 'Skånska partiet', 46: 'Socialdemokraterna', 47: 'Socialdemokratiska vänstergruppen', 48: 'Socialistiska partiet', 49: 'Stockholmsbänken', 50: 'Sverigedemokraterna', 51: 'Sveriges kommunistiska parti', 52: 'Vänsterpartiet', 53: 'borgmästarepartiet', 54: 'frihandelsvänlig vilde', 55: 'frisinnad vilde', 56: 'högervilde', 57: 'ministeriella partiet', 58: 'partilös', 59: 'politisk vilde', 60: 'vänstervilde'}\n",
      "\n",
      "Accuracy model 1: 0.516919959473151\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.27      0.38        81\n",
      "           1       0.45      0.47      0.46        81\n",
      "           2       0.73      0.46      0.56        81\n",
      "           3       0.45      0.69      0.55        81\n",
      "           4       0.32      0.59      0.42        81\n",
      "           5       0.44      0.65      0.53        81\n",
      "           6       0.53      0.73      0.61        81\n",
      "           7       0.68      0.42      0.52        81\n",
      "           8       0.42      0.63      0.50        81\n",
      "           9       0.51      0.78      0.62        81\n",
      "          10       0.47      0.56      0.51        81\n",
      "          11       0.85      0.21      0.34        81\n",
      "          12       0.36      0.52      0.43        81\n",
      "          13       0.64      0.46      0.53        81\n",
      "          14       0.32      0.75      0.45        81\n",
      "          15       0.44      0.69      0.54        81\n",
      "          16       0.59      0.40      0.47        81\n",
      "          17       0.48      0.60      0.53        81\n",
      "          18       0.93      0.17      0.29        81\n",
      "          19       0.37      0.59      0.45        81\n",
      "          20       0.53      0.62      0.57        81\n",
      "          21       0.46      0.37      0.41        81\n",
      "          22       0.89      0.20      0.32        81\n",
      "          23       0.60      0.64      0.62        81\n",
      "          24       0.61      0.56      0.58        81\n",
      "          25       0.60      0.73      0.66        75\n",
      "          26       0.48      0.70      0.57        81\n",
      "          27       0.71      0.81      0.76        81\n",
      "          28       0.68      0.83      0.75        81\n",
      "          29       0.73      0.60      0.66        81\n",
      "          30       0.62      0.57      0.59        81\n",
      "          31       0.81      0.27      0.41        81\n",
      "          32       0.49      0.21      0.29        81\n",
      "          33       0.77      0.12      0.21        81\n",
      "          34       0.53      0.49      0.51        81\n",
      "          35       0.58      0.53      0.55        81\n",
      "          36       0.55      0.59      0.57        81\n",
      "          37       0.73      0.89      0.80        81\n",
      "          38       0.57      0.77      0.65        81\n",
      "          39       0.70      0.54      0.61        81\n",
      "          40       0.39      0.59      0.47        81\n",
      "          41       0.60      0.78      0.68        81\n",
      "          42       1.00      0.17      0.29        81\n",
      "          43       0.80      0.35      0.48        81\n",
      "          44       0.73      0.41      0.52        81\n",
      "          45       0.42      0.59      0.49        81\n",
      "          46       1.00      0.10      0.18        81\n",
      "          47       0.61      0.59      0.60        81\n",
      "          48       0.50      0.73      0.60        81\n",
      "          49       0.48      0.36      0.41        81\n",
      "          50       0.64      0.80      0.71        81\n",
      "          51       0.73      0.37      0.49        81\n",
      "          52       0.72      0.28      0.41        81\n",
      "          53       0.27      0.78      0.40        81\n",
      "          54       0.46      0.72      0.56        81\n",
      "          55       0.59      0.30      0.39        81\n",
      "          56       0.89      0.30      0.44        81\n",
      "          57       0.79      0.14      0.23        81\n",
      "          58       0.62      0.56      0.59        81\n",
      "          59       0.70      0.20      0.31        81\n",
      "          60       0.34      0.75      0.46        81\n",
      "\n",
      "    accuracy                           0.52      4935\n",
      "   macro avg       0.60      0.52      0.50      4935\n",
      "weighted avg       0.60      0.52      0.50      4935\n",
      "\n",
      "\n",
      "Accuracy model 2: 0.4703140830800405\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.51      0.44        81\n",
      "           1       0.65      0.14      0.22        81\n",
      "           2       0.61      0.53      0.57        81\n",
      "           3       0.46      0.81      0.58        81\n",
      "           4       0.60      0.32      0.42        81\n",
      "           5       0.49      0.59      0.54        81\n",
      "           6       0.52      0.59      0.55        81\n",
      "           7       0.76      0.31      0.44        81\n",
      "           8       0.35      0.64      0.45        81\n",
      "           9       0.65      0.62      0.63        81\n",
      "          10       0.29      0.52      0.38        81\n",
      "          11       0.50      0.42      0.46        81\n",
      "          12       0.39      0.41      0.40        81\n",
      "          13       0.69      0.27      0.39        81\n",
      "          14       0.35      0.74      0.47        81\n",
      "          15       0.45      0.63      0.52        81\n",
      "          16       0.35      0.31      0.33        81\n",
      "          17       0.57      0.21      0.31        81\n",
      "          18       0.39      0.37      0.38        81\n",
      "          19       0.42      0.33      0.37        81\n",
      "          20       0.63      0.15      0.24        81\n",
      "          21       0.72      0.16      0.26        81\n",
      "          22       0.28      0.42      0.34        81\n",
      "          23       0.42      0.31      0.35        81\n",
      "          24       0.74      0.42      0.54        81\n",
      "          25       0.52      0.83      0.64        75\n",
      "          26       0.24      0.53      0.33        81\n",
      "          27       0.80      0.80      0.80        81\n",
      "          28       0.59      0.89      0.71        81\n",
      "          29       0.70      0.68      0.69        81\n",
      "          30       0.43      0.49      0.46        81\n",
      "          31       0.92      0.14      0.24        81\n",
      "          32       1.00      0.06      0.12        81\n",
      "          33       0.39      0.36      0.37        81\n",
      "          34       0.38      0.48      0.42        81\n",
      "          35       0.49      0.25      0.33        81\n",
      "          36       0.37      0.75      0.50        81\n",
      "          37       0.72      0.90      0.80        81\n",
      "          38       0.63      0.48      0.55        81\n",
      "          39       0.54      0.32      0.40        81\n",
      "          40       0.50      0.40      0.44        81\n",
      "          41       0.63      0.72      0.67        81\n",
      "          42       0.33      0.41      0.36        81\n",
      "          43       1.00      0.06      0.12        81\n",
      "          44       0.52      0.56      0.54        81\n",
      "          45       0.53      0.21      0.30        81\n",
      "          46       0.88      0.17      0.29        81\n",
      "          47       0.37      0.62      0.47        81\n",
      "          48       0.64      0.69      0.67        81\n",
      "          49       0.28      0.70      0.40        81\n",
      "          50       0.65      0.83      0.73        81\n",
      "          51       0.71      0.44      0.55        81\n",
      "          52       0.71      0.31      0.43        81\n",
      "          53       0.33      0.68      0.45        81\n",
      "          54       0.60      0.84      0.70        81\n",
      "          55       0.41      0.36      0.38        81\n",
      "          56       0.73      0.20      0.31        81\n",
      "          57       0.37      0.49      0.42        81\n",
      "          58       0.50      0.57      0.53        81\n",
      "          59       0.54      0.26      0.35        81\n",
      "          60       0.53      0.52      0.53        81\n",
      "\n",
      "    accuracy                           0.47      4935\n",
      "   macro avg       0.54      0.47      0.45      4935\n",
      "weighted avg       0.54      0.47      0.45      4935\n",
      "\n",
      "\n",
      "  0%|          | 0/309 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 309/309 [01:07<00:00,  4.58it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model_hugging_face,valid_dataloader,\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-572500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846796b047fb47deb8391d2752a6f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2889373302459717, 'eval_runtime': 1670.4142, 'eval_samples_per_second': 478.987, 'eval_steps_per_second': 7.484}\n",
      ">>> Perplexity: 9.86\n",
      "Manual perplexity...\n",
      " Perplexity: 9.864508254198425\n",
      "Accuracy...\n",
      "Accuracy: 0.588802334176737\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    4,     4, 49795,  ..., 49795,  6742,     4],\n",
      "        [10996,   688, 12494,  ...,  3193, 13781,    19],\n",
      "        [18059,   183,  2261,  ...,    24, 48380, 48465],\n",
      "        ...,\n",
      "        [49816, 21033,    49,  ...,   469,  1314, 49830],\n",
      "        [21264,  1219,     4,  ..., 11583,   546, 20805],\n",
      "        [    7,     1,     4,  ...,     4,     4, 28662]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  109, 28534,  -100,  ...,  -100,  -100, 15873],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100, 38839,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,     7,  ...,    37,  2193,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1410209390681376"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(0.13192342221736908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données à partir du fichier JSON\n",
    "with open(\"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-572500/trainer_state.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epoch_train = []\n",
    "epoch_test=[]\n",
    "\n",
    "for entry in data['log_history']:\n",
    "    if 'loss' in entry:\n",
    "        train_loss.append(entry['loss'])\n",
    "        epoch_train.append((entry['epoch']))\n",
    "    elif 'eval_loss' in entry:\n",
    "        eval_loss.append(entry['eval_loss'])\n",
    "        epoch_test.append((entry['epoch']))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_train, train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epoch_test, eval_loss, label='Evaluation Loss', marker='o')\n",
    "plt.title('Training and Evaluation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id= tokenizer.pad_token_id\n",
    "sep_token_id = tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking(batch,i):\n",
    "    word_ids=batch[\"word_ids\"]\n",
    " \n",
    "    masked_input_id=batch[\"input_ids\"].copy()\n",
    "    attention_mask=batch[\"attention_mask\"].copy()\n",
    " \n",
    "    labels=[[-100]*max_length]*len(batch[\"labels\"])\n",
    "    for z in range(len(masked_input_id)):\n",
    "        if batch[\"input_ids\"][z][i] ==tokenizer.pad_token_id or batch[\"input_ids\"][z][i] ==tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z][i]=batch[\"input_ids\"][z][i]\n",
    "        masked_input_id[z][i]=tokenizer.mask_token_id\n",
    "  \n",
    "        \n",
    "        word=tokenizer.decode(batch[\"input_ids\"][z][i])\n",
    "   \n",
    "        future_token=[j for j,_ in enumerate(word_ids[z]) if word_ids[z][j]==word_ids[z][i] and j>i]\n",
    "\n",
    "        for j in future_token:\n",
    "            labels[z][j]=batch[\"input_ids\"][z][j]\n",
    "    \n",
    "            masked_input_id[z][j]=tokenizer.mask_token_id\n",
    "           \n",
    "\n",
    "        masked_input_id[z]=np.array(masked_input_id[z])\n",
    "        attention_mask[z]=np.array(attention_mask[z])\n",
    "        labels[z]=np.array(labels[z])\n",
    "   \n",
    "    output_dict = {\"input_ids\": masked_input_id, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    return {k: v for k, v in output_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_special_masking_bis(batch, i):\n",
    "    word_ids = batch[\"word_ids\"]\n",
    "    masked_input_id = batch[\"input_ids\"].copy()\n",
    "    attention_mask = batch[\"attention_mask\"].copy()\n",
    "    \n",
    "    labels = np.full_like(masked_input_id, -100)\n",
    "    \n",
    "    for z, seq in enumerate(masked_input_id):\n",
    "        if seq[i] == tokenizer.pad_token_id or seq[i] == tokenizer.sep_token_id:\n",
    "            continue\n",
    "        \n",
    "        labels[z, i] = seq[i]\n",
    "        masked_input_id[z][i] = tokenizer.mask_token_id\n",
    "        future_token = [j for j, _ in enumerate(word_ids[z]) if word_ids[z][j] == word_ids[z][i] and j > i]\n",
    "        \n",
    "        for j in future_token:\n",
    "            labels[z][j] = batch[\"input_ids\"][z][j]\n",
    "            masked_input_id[z][j] = tokenizer.mask_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": masked_input_id,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "daatset\n",
      "dataloader\n",
      "output\n",
      "loss\n",
      "loss\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868fcd2f0a446ac8260aee3351fc56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/761125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daatset\n",
      "dataloader\n",
      "output\n",
      "loss\n",
      "loss\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5375b049bc084f5e9248be3ded3723e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/761125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      5\u001b[0m losses\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 6\u001b[0m eval_dataset_log \u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m examples: insert_special_masking_bis(examples,i),\n\u001b[1;32m      8\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaatset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mcreate_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3483\u001b[0m         batch,\n\u001b[1;32m   3484\u001b[0m         indices,\n\u001b[1;32m   3485\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   3486\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   3487\u001b[0m     )\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[120], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      5\u001b[0m losses\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      6\u001b[0m eval_dataset_log \u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m examples: insert_special_masking_bis(examples,i),\n\u001b[1;32m      8\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m lm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaatset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mcreate_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
      "Cell \u001b[0;32mIn[112], line 4\u001b[0m, in \u001b[0;36minsert_special_masking_bis\u001b[0;34m(batch, i)\u001b[0m\n\u001b[1;32m      2\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m masked_input_id \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 4\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull_like(masked_input_id, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(masked_input_id):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:272\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 272\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:375\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_column(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_table\u001b[38;5;241m.\u001b[39mselect([key]))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:441\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_column(pa_table)\n\u001b[1;32m    442\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:147\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pylist()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pll = 0\n",
    "batch_size=64\n",
    "for i in  range(max_length):\n",
    "    print(i)\n",
    "    losses=[]\n",
    "    eval_dataset_log = lm_datasets[\"test\"].map(\n",
    "        lambda examples: insert_special_masking_bis(examples,i),\n",
    "        batched=True,\n",
    "        remove_columns= lm_datasets[\"test\"].column_names\n",
    "    )\n",
    "    print(\"daatset\")\n",
    "    eval_dataloader = preprocessing.create_dataloader(eval_dataset_log,batch_size,default_data_collator)\n",
    "    print(\"dataloader\")\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch={key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output=model_hugging_face(**batch)\n",
    "        print(\"output\")\n",
    "        loss=output.loss\n",
    "        losses.append(loss.repeat(eval_dataloader.batch_size))\n",
    "        print(\"loss\")\n",
    "        break\n",
    "    losses = torch.cat(losses)\n",
    "    print(\"loss\")\n",
    "    #losses = losses[: len(eval_dataloader.dataset)]\n",
    "    pll += torch.mean(losses)\n",
    "\n",
    "\n",
    "pll /=max_length\n",
    "pll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d64e2999424512b94605be5fc3e3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 5813.28\n",
      "Manual perplexity...\n",
      " Perplexity: 5814.432395049958\n",
      "Accuracy...\n",
      "Accuracy: 0.18456963747366367\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model_kb,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer evaluation....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432cfa4389024def8ce59a63d369962a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 5363.19\n",
      "Manual perplexity...\n",
      " Perplexity: 5364.880164258874\n",
      "Accuracy...\n",
      "Accuracy: 0.23634023447799532\n"
     ]
    }
   ],
   "source": [
    "evaluation_task(model,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurinemeier/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1399: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/home/laurinemeier/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f9eda7965b4212b1be4ee6874b1e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_long.config.name_or_path}-imdb\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    # Add other training arguments as needed\n",
    "    logging_steps=892,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True\n",
    ")\n",
    "print(training_args.device)\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model_long,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8190011382102966,\n",
       " 'eval_runtime': 1639.8287,\n",
       " 'eval_samples_per_second': 7.575,\n",
       " 'eval_steps_per_second': 0.119}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2682330542912186"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(result['eval_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
