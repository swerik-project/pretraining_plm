{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, BertTokenizer, pipeline\n",
    "from transformers import BertTokenizer, BertConfig,AutoConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples,tokenizer):\n",
    "    result = tokenizer(examples[\"texte\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples,chunk_size):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cc488212804952b54e8f0e7a634891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89d78877d4741c1bdef6fcef6cc22ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac50feaa41949fc8a24e87d7cabd861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3653008b484da29572e5068e91bf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model =  AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model.load_state_dict(torch.load(\"finetuning_manual.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x74a00c22cba0>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte', '__index_level_0__'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte', '__index_level_0__'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#datasest\n",
    "data_files = {\"train\": \"swerick_data_train.pkl\", \"test\": \"swerick_data_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9a3cbb9e2c4824a1c45bb63e4284c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26821 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c48410bc2646719dec0e658ed03e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = swerick_dataset.map(\n",
    "      lambda examples: tokenize_function(examples, tokenizer), batched=True, remove_columns=[\"texte\", \"protocole\",'__index_level_0__']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e5a60472f84bf78f147f7c05d8b563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a3012a7f5b45d58fba3e57cbcf825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 102248\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 22431\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "lm_datasets = tokenized_datasets.map( lambda examples: group_texts(examples,chunk_size), batched=True) #dataset with chunk\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accf1d6a7f084775b3ec6252c274a5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"mosaicml/mosaic-bert-base\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=chunk_size,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte', '__index_level_0__'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte', '__index_level_0__'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(swerick_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)#add the MASK term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "train_dataloader = [\n",
    "    inputs.to(device) for inputs in train_dataloader\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102248"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurinemeier/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"tokenizer_swerick.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_local': 'swerick_mosaic', 'data_remote': None, 'max_seq_len': 128, 'tokenizer_name': 'swerick_tokenizer', 'mlm_probability': 0.3, 'run_name': 'mosaic-bert-base-uncased', 'model': {'name': 'mosaic_bert', 'pretrained_model_name': None, 'tokenizer_name': '${tokenizer_name}', 'model_config': {'num_attention_heads': 12, 'num_hidden_layers': 12, 'attention_probs_dropout_prob': 0.0}}, 'train_loader': {'name': 'text', 'dataset': {'local': '${data_local}', 'remote': '${data_remote}', 'split': 'swerick_data_sentence_train', 'tokenizer_name': '${tokenizer_name}', 'max_seq_len': '${max_seq_len}', 'shuffle': True, 'mlm_probability': '${mlm_probability}'}, 'drop_last': True, 'num_workers': 1}, 'eval_loader': {'name': 'text', 'dataset': {'local': '${data_local}', 'remote': '${data_remote}', 'split': 'swerick_data_sentence_test', 'tokenizer_name': '${tokenizer_name}', 'max_seq_len': '${max_seq_len}', 'shuffle': False, 'mlm_probability': 0.15}, 'drop_last': False, 'num_workers': 8}, 'scheduler': {'name': 'linear_decay_with_warmup', 't_warmup': '0.06dur', 'alpha_f': 0.02}, 'optimizer': {'name': 'decoupled_adamw', 'lr': 0.0005, 'betas': [0.9, 0.98], 'eps': 1e-06, 'weight_decay': 1e-05}, 'algorithms': {'fused_layernorm': {}}, 'max_duration': '286720000sp', 'eval_interval': '2000ba', 'global_train_batch_size': 8, 'seed': 17, 'device_eval_batch_size': 128, 'device_train_microbatch_size': 128, 'precision': 'amp_bf16', 'progress_bar': True, 'log_to_console': True, 'console_log_interval': '1ba', 'callbacks': {'speed_monitor': {'window_size': 500}, 'lr_monitor': {}}}\n",
      "{'num_attention_heads': 12, 'num_hidden_layers': 12, 'attention_probs_dropout_prob': 0.0}\n",
      "BertConfig {\n",
      "  \"alibi_starting_size\": 512,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50325\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurinemeier/swerick/bert_layers.py:180: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50328, 768, padding_idx=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig as TransformersBertConfig\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional, cast\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "\n",
    "class BertConfig(TransformersBertConfig):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alibi_starting_size: int = 512,\n",
    "        attention_probs_dropout_prob: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Configuration class for MosaicBert.\n",
    "\n",
    "        Args:\n",
    "            alibi_starting_size (int): Use `alibi_starting_size` to determine how large of an alibi tensor to\n",
    "                create when initializing the model. You should be able to ignore this parameter in most cases.\n",
    "                Defaults to 512.\n",
    "            attention_probs_dropout_prob (float): By default, turn off attention dropout in Mosaic BERT\n",
    "                (otherwise, Flash Attention will be off by default). Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob, **kwargs)\n",
    "        self.alibi_starting_size = alibi_starting_size\n",
    "\n",
    "with open(\"examples/examples/benchmarks/bert/yamls/main/mosaic-bert-base-uncased.yaml\") as f:\n",
    "        yaml_cfg = om.load(f)\n",
    "cfg = cast(DictConfig, yaml_cfg)  \n",
    "print(cfg)  \n",
    "        \n",
    "\n",
    "pretrained_model_name = \"KBLab/bert-base-swedish-cased\"\n",
    "model_config=cfg.model.get('model_config', None)\n",
    "print(model_config)\n",
    "config = BertConfig.from_pretrained(\n",
    "        pretrained_model_name, **model_config)\n",
    "print(config)\n",
    "if config.vocab_size % 8 != 0:\n",
    "        config.vocab_size += 8 - (config.vocab_size % 8)\n",
    "\n",
    "import bert_layers as bert_layers_module\n",
    "from composer.metrics.nlp import (BinaryF1Score, LanguageCrossEntropy,\n",
    "                                  MaskedAccuracy)\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "model = bert_layers_module.BertForMaskedLM(config)\n",
    "\n",
    "    # We have to do it again here because wrapping by HuggingFaceModel changes it\n",
    "if config.vocab_size % 8 != 0:\n",
    "    config.vocab_size += 8 - (config.vocab_size % 8)\n",
    "model.resize_token_embeddings(config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "mlm_probability = 0.30\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "        tokenizer=wrapped_tokenizer,\n",
    "        mlm=mlm_probability is not None,\n",
    "        mlm_probability=mlm_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"from_scratc_dataset\",\"rb\") as f:\n",
    "    tokenized_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"],collate_fn=collate_fn,batch_size=64,num_workers=4)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"],collate_fn=collate_fn,batch_size=64,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52588\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import preprocessing\n",
    "batch_size = 64\n",
    "num_epochs=100\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "print(len(tokenized_datasets[\"train\"]) // batch_size)\n",
    "model_name =\"scratch\"\n",
    "\n",
    "trainer = preprocessing.create_trainer(model,model_name,batch_size,logging_steps,train_dataset=tokenized_datasets[\"train\"],eval_dataset=tokenized_datasets[\"test\"],data_collator=collate_fn,tokenizer=wrapped_tokenizer,num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurinemeier/.local/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlaurinemeier\u001b[0m (\u001b[33muppsala_ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/laurinemeier/swerick/wandb/run-20240507_155804-8pwnuh8f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uppsala_ml/huggingface/runs/8pwnuh8f' target=\"_blank\">amber-disco-3</a></strong> to <a href='https://wandb.ai/uppsala_ml/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uppsala_ml/huggingface' target=\"_blank\">https://wandb.ai/uppsala_ml/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uppsala_ml/huggingface/runs/8pwnuh8f' target=\"_blank\">https://wandb.ai/uppsala_ml/huggingface/runs/8pwnuh8f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5258900 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 23.67 GiB total capacity; 8.89 GiB already allocated; 409.38 MiB free; 8.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2699\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2702\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2730\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/swerick/bert_layers.py:888\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoding error; please open an issue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    886\u001b[0m     batch, seqlen \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    887\u001b[0m     prediction_scores \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mbert_padding_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_put_first_axis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_token_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(b s) d -> b s d\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    891\u001b[0m         b\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    894\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/swerick/bert_padding.py:74\u001b[0m, in \u001b[0;36mIndexPutFirstAxis.forward\u001b[0;34m(ctx, values, indices, first_axis_dim)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_axis_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m output[indices] \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 23.67 GiB total capacity; 8.89 GiB already allocated; 409.38 MiB free; 8.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f7dddfed710>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f7dddfbbc10, execution_count=7 error_before_exec=None error_in_exec=CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 23.67 GiB total capacity; 8.89 GiB already allocated; 409.38 MiB free; 8.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF info=<ExecutionInfo object at 7f7da7e1bd90, raw_cell=\"trainer.train()\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/laurinemeier/swerick/from_scratch_model.ipynb#X41sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scripting with torch.jit.script failed and sample inputs are not provided for tracing with torch.jit.trace\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/composer/utils/inference.py\", line 207, in export_for_inference\n",
      "    export_model = torch.jit.script(model)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/_script.py\", line 1284, in script\n",
      "    return torch.jit._recursive.create_script_module(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/_recursive.py\", line 480, in create_script_module\n",
      "    return create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/_recursive.py\", line 492, in create_script_module_impl\n",
      "    method_stubs = stubs_fn(nn_module)\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/_recursive.py\", line 761, in infer_methods_to_compile\n",
      "    stubs.append(make_stub_from_method(nn_module, method))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/_recursive.py\", line 73, in make_stub_from_method\n",
      "    return make_stub(func, method_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/_recursive.py\", line 58, in make_stub\n",
      "    ast = get_jit_def(func, name, self_name=\"RecursiveScriptModule\")\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/frontend.py\", line 297, in get_jit_def\n",
      "    return build_def(parsed_def.ctx, fn_def, type_line, def_name, self_name=self_name, pdt_arg_types=pdt_arg_types)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/laurinemeier/.local/lib/python3.11/site-packages/torch/jit/frontend.py\", line 343, in build_def\n",
      "    type_comment_decl = torch._C.parse_type_comment(type_line)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: expected type comment but found 'ident' here:\n",
      "output = self.model(**batch)  # type: ignore (thirdparty)\n",
      "~~~~~~ <--- HERE\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Scritping and tracing failed! No model is getting exported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m working_dir\u001b[38;5;241m=\u001b[39mtempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory()\n\u001b[1;32m      7\u001b[0m model_save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(working_dir\u001b[38;5;241m.\u001b[39mname,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mexport_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/composer/utils/inference.py:221\u001b[0m, in \u001b[0;36mexport_for_inference\u001b[0;34m(model, save_format, save_path, save_object_store, sample_input, dynamic_axes, surgery_algs, transforms, onnx_opset_version, load_path, load_object_store, load_strict)\u001b[0m\n\u001b[1;32m    219\u001b[0m         torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39msave(export_model, local_save_path)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScritping and tracing failed! No model is getting exported.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m ExportFormat\u001b[38;5;241m.\u001b[39mONNX:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Scritping and tracing failed! No model is getting exported."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from composer.utils import export_for_inference\n",
    "\n",
    "save_format=\"torchscript\"\n",
    "working_dir=tempfile.TemporaryDirectory()\n",
    "model_save_path=os.path.join(working_dir.name,\"model.pt\")\n",
    "\n",
    "export_for_inference(model=model,save_format=save_format,save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of HuggingFaceModel(\n",
       "  (model): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(50328, 768, padding_idx=0)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertUnpadAttention(\n",
       "              (self): BertUnpadSelfAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp): BertGatedLinearUnitMLP(\n",
       "              (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=50328, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading checkpoint from From_scratch_train/ep0-ba15356-rank0.pt -> /tmp/tmp70xilx7w/local-composer-checkpoint.pt\n",
      "Loading checkpoint into CPU RAM...\n",
      "##############################\n",
      "Saving HF Model Config...\n",
      "{'alibi_starting_size': 512, 'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': 0.0, 'classifier_dropout': None, 'gradient_checkpointing': False, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'layer_norm_eps': 1e-12, 'max_position_embeddings': 512, 'model_type': 'bert', 'num_attention_heads': 12, 'num_hidden_layers': 12, 'output_past': True, 'pad_token_id': 0, 'position_embedding_type': 'absolute', 'transformers_version': '4.28.1', 'type_vocab_size': 2, 'use_cache': True, 'vocab_size': 50328}\n",
      "BertConfig {\n",
      "  \"alibi_starting_size\": 512,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50328\n",
      "}\n",
      "\n",
      "##############################\n",
      "Saving HF Tokenizer...\n",
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=50325, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "##############################\n",
      "Saving HF Model Weights...\n",
      "##############################\n",
      "HF checkpoint folder successfully created at model_0/.\n",
      "Done.\n",
      "##############################\n",
      "Loading model from model_0/\n",
      "You are using a model of type bert to instantiate a model of type mpt. This is not supported for all configurations of models and can yield errors.\n",
      "Instantiating an MPTForCausalLM model from /home/laurinemeier/swerick/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Some weights of the model checkpoint at model_0/ were not used when initializing MPTForCausalLM: ['bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.wo.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.11.mlp.layernorm.bias']\n",
      "- This IS expected if you are initializing MPTForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPTForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPTForCausalLM were not initialized from the model checkpoint at model_0/ and are newly initialized: ['transformer.blocks.1.ffn.down_proj.weight', 'transformer.blocks.17.attn.Wqkv.bias', 'transformer.blocks.2.ffn.down_proj.weight', 'transformer.blocks.7.norm_2.weight', 'transformer.blocks.11.ffn.down_proj.weight', 'transformer.blocks.5.norm_1.weight', 'transformer.blocks.5.attn.out_proj.bias', 'transformer.blocks.15.ffn.up_proj.weight', 'transformer.blocks.5.attn.Wqkv.bias', 'transformer.blocks.13.attn.out_proj.bias', 'transformer.blocks.7.attn.Wqkv.bias', 'transformer.blocks.20.attn.Wqkv.weight', 'transformer.blocks.12.ffn.up_proj.bias', 'transformer.blocks.0.ffn.down_proj.bias', 'transformer.blocks.5.ffn.up_proj.bias', 'transformer.blocks.8.attn.out_proj.bias', 'transformer.blocks.1.attn.out_proj.weight', 'transformer.blocks.4.ffn.up_proj.bias', 'transformer.wte.weight', 'transformer.blocks.15.ffn.down_proj.bias', 'transformer.blocks.21.norm_1.weight', 'transformer.blocks.16.attn.out_proj.bias', 'transformer.blocks.14.ffn.up_proj.weight', 'transformer.blocks.14.attn.out_proj.weight', 'transformer.blocks.13.norm_1.weight', 'transformer.blocks.4.norm_1.weight', 'transformer.blocks.10.ffn.up_proj.weight', 'transformer.blocks.1.norm_2.weight', 'transformer.blocks.22.ffn.up_proj.weight', 'transformer.blocks.9.attn.out_proj.weight', 'transformer.blocks.9.ffn.down_proj.weight', 'transformer.blocks.10.norm_2.weight', 'transformer.blocks.19.ffn.up_proj.bias', 'transformer.blocks.9.ffn.down_proj.bias', 'transformer.blocks.8.norm_1.weight', 'transformer.blocks.17.norm_1.weight', 'transformer.blocks.14.attn.Wqkv.weight', 'transformer.blocks.1.attn.Wqkv.weight', 'transformer.blocks.13.attn.Wqkv.weight', 'transformer.blocks.15.norm_1.weight', 'transformer.blocks.19.norm_1.weight', 'transformer.blocks.9.norm_1.weight', 'transformer.blocks.15.attn.out_proj.weight', 'transformer.blocks.5.norm_1.bias', 'transformer.blocks.2.norm_1.weight', 'transformer.blocks.22.ffn.down_proj.bias', 'transformer.blocks.10.attn.Wqkv.bias', 'transformer.blocks.19.norm_2.weight', 'transformer.blocks.19.ffn.up_proj.weight', 'transformer.blocks.20.ffn.up_proj.weight', 'transformer.blocks.15.attn.Wqkv.bias', 'transformer.blocks.0.norm_1.weight', 'transformer.blocks.22.norm_1.bias', 'transformer.blocks.12.attn.Wqkv.weight', 'transformer.blocks.21.ffn.down_proj.weight', 'transformer.blocks.6.norm_1.weight', 'transformer.blocks.18.attn.out_proj.bias', 'transformer.blocks.16.norm_2.bias', 'transformer.blocks.16.ffn.down_proj.bias', 'transformer.blocks.6.ffn.down_proj.weight', 'transformer.blocks.8.attn.Wqkv.bias', 'transformer.blocks.3.norm_1.bias', 'transformer.blocks.8.norm_1.bias', 'transformer.blocks.11.attn.out_proj.bias', 'transformer.blocks.14.norm_1.bias', 'transformer.blocks.23.ffn.down_proj.weight', 'transformer.blocks.12.attn.Wqkv.bias', 'transformer.blocks.22.attn.out_proj.weight', 'transformer.blocks.3.norm_1.weight', 'transformer.blocks.20.attn.Wqkv.bias', 'transformer.blocks.1.norm_1.bias', 'transformer.blocks.3.attn.out_proj.weight', 'transformer.blocks.22.ffn.up_proj.bias', 'transformer.blocks.14.ffn.down_proj.bias', 'transformer.blocks.1.ffn.up_proj.bias', 'transformer.blocks.4.ffn.down_proj.weight', 'transformer.blocks.19.attn.out_proj.weight', 'transformer.blocks.20.ffn.up_proj.bias', 'transformer.blocks.23.norm_1.weight', 'transformer.blocks.9.attn.out_proj.bias', 'transformer.blocks.2.attn.Wqkv.weight', 'transformer.blocks.14.norm_1.weight', 'transformer.blocks.22.attn.Wqkv.weight', 'transformer.blocks.8.ffn.up_proj.weight', 'transformer.blocks.10.norm_1.bias', 'transformer.blocks.21.norm_1.bias', 'transformer.blocks.19.ffn.down_proj.bias', 'transformer.blocks.14.norm_2.weight', 'transformer.blocks.2.norm_2.bias', 'transformer.blocks.10.ffn.down_proj.bias', 'transformer.blocks.12.norm_2.weight', 'transformer.blocks.7.norm_2.bias', 'transformer.blocks.4.ffn.up_proj.weight', 'transformer.blocks.1.norm_2.bias', 'transformer.blocks.2.ffn.up_proj.bias', 'transformer.blocks.21.attn.out_proj.weight', 'transformer.blocks.12.ffn.up_proj.weight', 'transformer.blocks.20.ffn.down_proj.bias', 'transformer.blocks.0.norm_2.weight', 'transformer.norm_f.weight', 'transformer.blocks.14.norm_2.bias', 'transformer.blocks.9.ffn.up_proj.bias', 'transformer.blocks.3.attn.Wqkv.weight', 'transformer.blocks.15.attn.Wqkv.weight', 'transformer.blocks.21.norm_2.bias', 'transformer.blocks.6.norm_1.bias', 'transformer.blocks.9.norm_2.bias', 'transformer.blocks.18.attn.out_proj.weight', 'transformer.blocks.18.norm_2.weight', 'transformer.blocks.20.norm_1.bias', 'transformer.blocks.18.norm_1.bias', 'transformer.blocks.8.norm_2.bias', 'transformer.wpe.weight', 'transformer.blocks.12.norm_1.weight', 'transformer.blocks.10.norm_2.bias', 'transformer.blocks.13.ffn.up_proj.bias', 'transformer.blocks.18.attn.Wqkv.weight', 'transformer.blocks.13.norm_2.weight', 'transformer.blocks.21.attn.out_proj.bias', 'transformer.blocks.4.attn.out_proj.weight', 'transformer.blocks.21.attn.Wqkv.weight', 'transformer.blocks.15.attn.out_proj.bias', 'transformer.blocks.22.attn.out_proj.bias', 'transformer.blocks.12.attn.out_proj.weight', 'transformer.blocks.19.norm_1.bias', 'transformer.blocks.4.norm_1.bias', 'transformer.blocks.2.attn.out_proj.bias', 'transformer.blocks.16.ffn.up_proj.weight', 'transformer.blocks.19.attn.Wqkv.weight', 'transformer.blocks.15.norm_2.weight', 'transformer.blocks.11.norm_1.bias', 'transformer.blocks.0.norm_1.bias', 'transformer.blocks.6.attn.Wqkv.weight', 'transformer.blocks.11.norm_2.bias', 'transformer.blocks.17.ffn.up_proj.bias', 'transformer.blocks.6.attn.out_proj.weight', 'transformer.blocks.18.ffn.up_proj.weight', 'transformer.blocks.3.attn.out_proj.bias', 'transformer.blocks.22.norm_2.weight', 'transformer.blocks.4.norm_2.weight', 'transformer.blocks.6.ffn.up_proj.bias', 'transformer.blocks.15.norm_1.bias', 'transformer.blocks.12.norm_1.bias', 'transformer.blocks.11.attn.Wqkv.bias', 'transformer.blocks.20.attn.out_proj.bias', 'transformer.blocks.13.ffn.up_proj.weight', 'transformer.blocks.6.norm_2.bias', 'transformer.blocks.18.norm_2.bias', 'transformer.blocks.20.norm_2.bias', 'transformer.blocks.19.attn.Wqkv.bias', 'transformer.blocks.5.ffn.down_proj.weight', 'transformer.blocks.23.norm_1.bias', 'transformer.blocks.6.norm_2.weight', 'transformer.blocks.10.norm_1.weight', 'transformer.blocks.4.ffn.down_proj.bias', 'transformer.blocks.7.norm_1.bias', 'transformer.blocks.9.norm_2.weight', 'transformer.blocks.16.ffn.up_proj.bias', 'transformer.blocks.6.ffn.down_proj.bias', 'transformer.blocks.21.ffn.up_proj.bias', 'transformer.blocks.13.attn.Wqkv.bias', 'transformer.blocks.6.ffn.up_proj.weight', 'transformer.blocks.23.ffn.down_proj.bias', 'transformer.blocks.10.attn.out_proj.weight', 'transformer.blocks.21.norm_2.weight', 'transformer.blocks.6.attn.Wqkv.bias', 'transformer.blocks.0.ffn.up_proj.bias', 'transformer.blocks.19.ffn.down_proj.weight', 'transformer.blocks.3.ffn.down_proj.weight', 'transformer.blocks.8.ffn.down_proj.bias', 'transformer.blocks.21.attn.Wqkv.bias', 'transformer.blocks.22.norm_1.weight', 'transformer.blocks.14.ffn.up_proj.bias', 'transformer.blocks.12.norm_2.bias', 'transformer.blocks.18.ffn.down_proj.weight', 'transformer.blocks.12.attn.out_proj.bias', 'transformer.blocks.13.ffn.down_proj.weight', 'transformer.blocks.8.norm_2.weight', 'transformer.blocks.19.attn.out_proj.bias', 'transformer.blocks.1.ffn.down_proj.bias', 'transformer.blocks.5.attn.out_proj.weight', 'transformer.blocks.18.ffn.down_proj.bias', 'transformer.blocks.8.ffn.down_proj.weight', 'transformer.blocks.17.attn.out_proj.bias', 'transformer.blocks.13.attn.out_proj.weight', 'transformer.blocks.13.norm_2.bias', 'transformer.blocks.23.attn.Wqkv.bias', 'transformer.blocks.3.attn.Wqkv.bias', 'transformer.blocks.3.ffn.up_proj.weight', 'transformer.blocks.4.attn.Wqkv.weight', 'transformer.blocks.0.attn.Wqkv.bias', 'transformer.blocks.4.attn.out_proj.bias', 'transformer.blocks.1.ffn.up_proj.weight', 'transformer.blocks.4.attn.Wqkv.bias', 'transformer.blocks.13.norm_1.bias', 'transformer.blocks.15.norm_2.bias', 'transformer.blocks.3.norm_2.weight', 'transformer.blocks.3.ffn.down_proj.bias', 'transformer.blocks.0.attn.out_proj.weight', 'transformer.blocks.9.norm_1.bias', 'transformer.blocks.17.attn.Wqkv.weight', 'transformer.blocks.20.norm_2.weight', 'transformer.blocks.7.ffn.up_proj.bias', 'transformer.blocks.10.attn.Wqkv.weight', 'transformer.blocks.7.ffn.down_proj.bias', 'transformer.blocks.23.norm_2.weight', 'transformer.blocks.10.ffn.up_proj.bias', 'transformer.blocks.11.norm_2.weight', 'transformer.blocks.2.ffn.up_proj.weight', 'transformer.blocks.16.norm_1.bias', 'transformer.blocks.18.norm_1.weight', 'transformer.blocks.11.attn.Wqkv.weight', 'transformer.blocks.23.attn.Wqkv.weight', 'transformer.blocks.10.attn.out_proj.bias', 'transformer.blocks.3.norm_2.bias', 'transformer.blocks.15.ffn.up_proj.bias', 'transformer.blocks.7.attn.Wqkv.weight', 'transformer.blocks.17.norm_2.bias', 'transformer.blocks.0.attn.out_proj.bias', 'transformer.blocks.1.attn.Wqkv.bias', 'transformer.blocks.22.attn.Wqkv.bias', 'transformer.blocks.23.attn.out_proj.bias', 'transformer.blocks.17.attn.out_proj.weight', 'transformer.blocks.7.ffn.down_proj.weight', 'transformer.blocks.9.attn.Wqkv.bias', 'transformer.blocks.4.norm_2.bias', 'transformer.blocks.2.norm_2.weight', 'transformer.blocks.17.ffn.down_proj.bias', 'transformer.blocks.12.ffn.down_proj.weight', 'transformer.blocks.17.ffn.up_proj.weight', 'transformer.blocks.18.ffn.up_proj.bias', 'transformer.blocks.9.attn.Wqkv.weight', 'transformer.blocks.14.attn.Wqkv.bias', 'transformer.blocks.6.attn.out_proj.bias', 'transformer.blocks.5.attn.Wqkv.weight', 'transformer.blocks.14.ffn.down_proj.weight', 'transformer.blocks.16.ffn.down_proj.weight', 'transformer.blocks.14.attn.out_proj.bias', 'transformer.blocks.17.norm_1.bias', 'transformer.blocks.21.ffn.up_proj.weight', 'transformer.blocks.20.norm_1.weight', 'transformer.blocks.3.ffn.up_proj.bias', 'transformer.blocks.2.attn.out_proj.weight', 'transformer.blocks.16.attn.Wqkv.weight', 'transformer.blocks.17.ffn.down_proj.weight', 'transformer.blocks.2.ffn.down_proj.bias', 'transformer.blocks.7.attn.out_proj.bias', 'transformer.blocks.21.ffn.down_proj.bias', 'transformer.blocks.0.ffn.up_proj.weight', 'transformer.blocks.5.norm_2.weight', 'transformer.blocks.7.norm_1.weight', 'transformer.blocks.8.attn.Wqkv.weight', 'transformer.blocks.11.ffn.up_proj.weight', 'transformer.blocks.7.ffn.up_proj.weight', 'transformer.blocks.12.ffn.down_proj.bias', 'transformer.blocks.18.attn.Wqkv.bias', 'transformer.blocks.16.attn.out_proj.weight', 'transformer.blocks.19.norm_2.bias', 'transformer.blocks.23.ffn.up_proj.bias', 'transformer.blocks.16.attn.Wqkv.bias', 'transformer.blocks.1.norm_1.weight', 'transformer.blocks.20.attn.out_proj.weight', 'transformer.blocks.2.norm_1.bias', 'transformer.blocks.9.ffn.up_proj.weight', 'transformer.blocks.8.attn.out_proj.weight', 'transformer.blocks.5.ffn.up_proj.weight', 'transformer.blocks.20.ffn.down_proj.weight', 'transformer.blocks.22.ffn.down_proj.weight', 'transformer.blocks.0.ffn.down_proj.weight', 'transformer.blocks.11.ffn.up_proj.bias', 'transformer.blocks.5.ffn.down_proj.bias', 'transformer.blocks.0.norm_2.bias', 'transformer.blocks.22.norm_2.bias', 'transformer.blocks.5.norm_2.bias', 'transformer.blocks.0.attn.Wqkv.weight', 'transformer.blocks.23.attn.out_proj.weight', 'transformer.blocks.7.attn.out_proj.weight', 'transformer.blocks.11.norm_1.weight', 'transformer.blocks.16.norm_1.weight', 'transformer.blocks.8.ffn.up_proj.bias', 'transformer.blocks.11.attn.out_proj.weight', 'transformer.blocks.16.norm_2.weight', 'transformer.blocks.13.ffn.down_proj.bias', 'transformer.blocks.23.ffn.up_proj.weight', 'transformer.norm_f.bias', 'transformer.blocks.2.attn.Wqkv.bias', 'transformer.blocks.23.norm_2.bias', 'transformer.blocks.10.ffn.down_proj.weight', 'transformer.blocks.15.ffn.down_proj.weight', 'transformer.blocks.11.ffn.down_proj.bias', 'transformer.blocks.1.attn.out_proj.bias', 'transformer.blocks.17.norm_2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading tokenizer from model_0/\n",
      "Editing files for HF compatibility...\n"
     ]
    }
   ],
   "source": [
    "!python3 convert_composer_to_hf.py --composer_path From_scratch_train/ep0-ba15356-rank0.pt --hf_output model_0/ --output_precision bf16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.model1.configuration_mpt.MPTConfig'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      6\u001b[0m fill_mask \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mwrapped_tokenizer)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:474\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    473\u001b[0m     )\n\u001b[0;32m--> 474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.model1.configuration_mpt.MPTConfig'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"model1\",trust_remote_code=True)\n",
    "\n",
    "from transformers import pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=wrapped_tokenizer)\n",
    "fill_mask(\"hey [MASK]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
