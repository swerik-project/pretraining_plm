{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyriksdagen in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: SPARQLWrapper in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (2.0.0)\n",
      "Requirement already satisfied: alto-xml in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (0.0.5)\n",
      "Requirement already satisfied: base58 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (2.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (4.12.2)\n",
      "Requirement already satisfied: dateparser in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (1.2.0)\n",
      "Requirement already satisfied: importlib_resources in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (6.4.0)\n",
      "Requirement already satisfied: kblab-client in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (0.0.16a0)\n",
      "Requirement already satisfied: lxml in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (4.9.3)\n",
      "Requirement already satisfied: matplotlib in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (3.8.0)\n",
      "Requirement already satisfied: nltk in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (2.1.4)\n",
      "Requirement already satisfied: progressbar2 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (4.4.2)\n",
      "Requirement already satisfied: py-markdown-table in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (0.4.0)\n",
      "Requirement already satisfied: pyparlaclarin in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (0.9.0)\n",
      "Requirement already satisfied: requests in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (2.31.0)\n",
      "Requirement already satisfied: textdistance in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (4.2.1)\n",
      "Requirement already satisfied: tqdm in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (4.65.0)\n",
      "Requirement already satisfied: trainerlog in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (0.3.0)\n",
      "Requirement already satisfied: unidecode in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (1.2.0)\n",
      "Requirement already satisfied: xmlschema in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pyriksdagen) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from alto-xml->pyriksdagen) (4.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->pyriksdagen) (2.5)\n",
      "Requirement already satisfied: python-dateutil in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from dateparser->pyriksdagen) (2.8.2)\n",
      "Requirement already satisfied: pytz in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from dateparser->pyriksdagen) (2023.3.post1)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from dateparser->pyriksdagen) (2023.10.3)\n",
      "Requirement already satisfied: tzlocal in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from dateparser->pyriksdagen) (2.1)\n",
      "Requirement already satisfied: pyyaml in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from kblab-client->pyriksdagen) (6.0.1)\n",
      "Requirement already satisfied: htfile in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from kblab-client->pyriksdagen) (0.0.1a0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from matplotlib->pyriksdagen) (3.0.9)\n",
      "Requirement already satisfied: click in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from nltk->pyriksdagen) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from nltk->pyriksdagen) (1.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from pandas->pyriksdagen) (2023.3)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from progressbar2->pyriksdagen) (3.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from requests->pyriksdagen) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from requests->pyriksdagen) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from requests->pyriksdagen) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from requests->pyriksdagen) (2024.2.2)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from SPARQLWrapper->pyriksdagen) (7.0.0)\n",
      "Requirement already satisfied: colorlog in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from trainerlog->pyriksdagen) (6.8.2)\n",
      "Requirement already satisfied: elementpath<5.0.0,>=4.4.0 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from xmlschema->pyriksdagen) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from python-dateutil->dateparser->pyriksdagen) (1.16.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /home/laurinemeier/anaconda3/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper->pyriksdagen) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyriksdagen\n",
    "from lxml import etree\n",
    "import progressbar\n",
    "from pyparlaclarin.read import paragraph_iterator, speeches_with_name\n",
    "from pyriksdagen.utils import protocol_iterators, download_corpus\n",
    "import pyriksdagen\n",
    "# We need a parser for reading in XML data\n",
    "parser = etree.XMLParser(remove_blank_text=True)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleSpec(name='pyriksdagen', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7473401ad5d0>, origin='/home/laurinemeier/anaconda3/lib/python3.11/site-packages/pyriksdagen/__init__.py', submodule_search_locations=['/home/laurinemeier/anaconda3/lib/python3.11/site-packages/pyriksdagen'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "politicians.zip: 100%|██████████| 2.20M/2.20M [00:00<00:00, 28.9MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m11:11:34 [WARNING] \u001b[37m(pyriksdagen)\u001b[0m: data already exists at the path 'data'. It will be overwritten once the download is finished.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "records.zip: 100%|██████████| 1.60G/1.60G [00:20<00:00, 84.8MiB/s]\n"
     ]
    }
   ],
   "source": [
    "print(pyriksdagen.__spec__)\n",
    "download_corpus(partitions=[\"politicians\", \"records\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = list(protocol_iterators(corpus_root=\"data/\", start=1867, end=202122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oppna_data_to_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Load protocols with the new XML / HTML structure (from 2013 onwards)\n",
    "    and convert it to a python dict with contents.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    data[\"paragraphs\"] = []\n",
    "\n",
    "    # Metadata\n",
    "    session = input_dict[\"dokumentstatus\"][\"dokument\"][\"rm\"]\n",
    "    session = session.replace(\"/\", \"\")\n",
    "    pid = input_dict[\"dokumentstatus\"][\"dokument\"][\"nummer\"]\n",
    "    date = input_dict[\"dokumentstatus\"][\"dokument\"][\"datum\"]\n",
    "    html = input_dict[\"dokumentstatus\"][\"dokument\"][\"html\"]\n",
    "    html_tree = clean_html(html)\n",
    "    year = int(date.split(\"-\")[0])\n",
    "    protocol_id = f\"prot-{session}--{pid}\"\n",
    "\n",
    "    data[\"protocol_id\"] = protocol_id\n",
    "    data[\"date\"] = date.split(\" \")[0]\n",
    "    data[\"session\"] = session\n",
    "\n",
    "    # New HTML structure with div[@class='Section1']\n",
    "    section1 = html_tree.xpath(\".//div[@class='Section1']\")\n",
    "    for elements in section1:\n",
    "        for elem in elements:\n",
    "            if elem.tag in [\"p\", \"h1\", \"h2\"]:\n",
    "                elemtext = \"\".join(elem.itertext())\n",
    "                linebreak = elemtext.strip() == \"\" and \"\\n\" in elemtext\n",
    "                if linebreak:\n",
    "                    pass\n",
    "                else:\n",
    "                    paragraph = elemtext.strip()\n",
    "                    paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "                    paragraph = re.sub(\"\\\\s+\", \" \", paragraph)\n",
    "                    data[\"paragraphs\"].append(paragraph)\n",
    "\n",
    "    if len(data[\"paragraphs\"]) == 0:\n",
    "        tree = html_tree\n",
    "\n",
    "        # Old data structure 1990-2003\n",
    "        pres = tree.findall(\".//pre\")\n",
    "        if len(pres) > 0:\n",
    "            for pre in pres:\n",
    "                if pre.text is not None:\n",
    "                    tblocks = re.sub(\"([a-zß-ÿ,])- ?\\n ?([a-zß-ÿ])\", \"\\\\1\\\\2\", pre.text)\n",
    "                    tblocks = re.sub(\"([a-zß-ÿ,]) ?\\n ?([a-zß-ÿ])\", \"\\\\1 \\\\2\", tblocks)\n",
    "                    for paragraph in tblocks.split(\"\\n\"):\n",
    "                        paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "                        paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "                        data[\"paragraphs\"].append(paragraph)\n",
    "\n",
    "        # Standard HTML structure, roughly 2003-2013\n",
    "        elif len(tree.xpath(\"//div[@class='indrag']\")) > 0:\n",
    "            tree = tree.xpath(\"//body\")[0]\n",
    "            for elem in tree:\n",
    "                elemtext = \"\".join(elem.itertext())\n",
    "                linebreak = elemtext.strip() == \"\" and \"\\n\" in elemtext\n",
    "                if elem.tag == \"br\" or linebreak:\n",
    "                    pass\n",
    "                else:\n",
    "                    paragraph = elemtext.strip()\n",
    "                    paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "                    paragraph = re.sub(\"\\\\s+\", \" \", paragraph)\n",
    "                    data[\"paragraphs\"].append(paragraph)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_in_question = protocols[12]\n",
    "root = etree.parse(protocol_in_question, parser).getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "          RIKSDAGENS Ar PROTOKOLL\n",
      "        \n",
      "\n",
      "          1955 ANDRA KAMMAREN Nr 13\n",
      "        \n",
      "\n",
      "          13—15 april\n",
      "        \n",
      "\n",
      "          Debatter m. m.\n",
      "        \n",
      "\n",
      "          Onsdagen den 13 april Sid.\n",
      "        \n",
      "\n",
      "          Familjerådgivning «... 5 Interpellation av herr Ericsson : i\n",
      "          Näs ang. de minskade perioderna\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for elem in list(paragraph_iterator(root, output=\"lxml\"))[:7]:\n",
    "  print(\" \".join(elem.itertext()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       protocole                                              texte\n",
      "0              0  Sedan,ikraftafRiketsRegeringsform,lagtimaRiksd...\n",
      "1              1  24Den19Januari.Lördagenden19Januari,KL!/,11f.m...\n",
      "2              2  Den21Januari.25mandeaftidenfördessaval,erhålli...\n",
      "3              3  Den22Januari.41Tisdagenden22Januari.Kl.10£m.g1...\n",
      "4              4  Den23Januari.-55Onsdagenden23Januari.Kl.10f.m....\n",
      "...          ...                                                ...\n",
      "17637      17637  §1JusteringavprotokollProtokolletförden7juniju...\n",
      "17638      17638  §1JusteringavprotokollProtokolletförden8juniju...\n",
      "17639      17639  §1JusteringavprotokollProtokollenförden9,10,13...\n",
      "17640      17640  §1AnmälanomåtertagandeavplatsiriksdagenTalmann...\n",
      "17641      17641  §1AnmälanomsubsidiaritetsprövningarTalmannenan...\n",
      "\n",
      "[17642 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "\n",
    "for i in range(len(protocols)):\n",
    "  protocol_in_question = protocols[i]\n",
    "  root = etree.parse(protocol_in_question, parser).getroot()\n",
    "  element_str=\"\"\n",
    "  for elem in list(paragraph_iterator(root, output=\"lxml\")):\n",
    "    element_str += \" \".join(elem.itertext()).replace(\"\\n\",\"\")\n",
    "    \n",
    "  data.append({\"protocole\": i,\"texte\": \"\".join(element_str.split())})\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"swerick_data_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ddaac0b4e94c489e42a3392a2fa6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=\"swerick_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte'],\n",
      "        num_rows: 130\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(swerick_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df,test_size=0.2,random_state=42)\n",
    "df_train.to_pickle(\"swerick_data_long_train.pkl\")\n",
    "df_test.to_pickle(\"swerick_data_long_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples,tokenizer):\n",
    "    result = tokenizer(examples[\"texte\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples,chunk_size):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'texte', '__index_level_0__'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'texte', '__index_level_0__'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#datasest\n",
    "data_files = {\"train\": \"swerick_data_train.pkl\", \"test\": \"swerick_data_test.pkl\"}\n",
    "swerick_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(swerick_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)#add the MASK term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = swerick_dataset.map(\n",
    "      lambda examples: tokenize_function(examples, tokenizer), batched=True, remove_columns=[\"texte\", \"protocole\",'__index_level_0__']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 57211\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 12561\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size =128\n",
    "lm_datasets = tokenized_datasets.map( lambda examples: group_texts(examples,chunk_size), batched=True) #dataset with chunk\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\",\"token_type_ids\"])\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "train_dataloader = [\n",
    "    inputs.to(\"cpu\") for inputs in train_dataloader\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
