{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer\n",
    "from transformers import default_data_collator\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import  BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import preprocessing\n",
    "import preprocessing\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from evaluation import reform_scratch_classfication\n",
    "from evaluation import regression_year\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "model_finetuned=model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exbert_tokenizer = AutoTokenizer.from_pretrained(\"/home/laurinemeier/swerick/exbert_tokenizer\")\n",
    "model_exbert=  AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swerick_tokenizer= AutoTokenizer.from_pretrained(\"/home/laurinemeier/swerick/swerick_tokenizer\")\n",
    "config = transformers.BertConfig.from_pretrained(\"/home/laurinemeier/swerick/pretraining_scratch/checkpoint-5258900\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"/home/laurinemeier/swerick/pretraining_scratch/checkpoint-5258900\",config=config,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_NaN(subset,example):\n",
    "    return example[subset] is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_label(dataset,nb_obs,label_name):\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Calculer le nombre d'observations pour chaque étiquette\n",
    "    grouped_data = df.groupby(label_name)\n",
    "\n",
    "    # Calculer le nombre d'observations par étiquette pour obtenir une répartition uniforme\n",
    "    total_samples = nb_obs\n",
    "    samples_per_label = total_samples // len(grouped_data.groups)\n",
    "\n",
    "    # Créer une liste pour stocker les observations échantillonnées\n",
    "    sampled_data = []\n",
    "\n",
    "    # Prélever aléatoirement les observations pour chaque groupe de label\n",
    "    for group_label, group_data in grouped_data.groups.items():\n",
    "        group_dataset=dataset.select(group_data)\n",
    "        label_data = group_dataset.shuffle(seed=np.random.randint(1, 1000)).select(range(min(len(group_data), samples_per_label)))\n",
    "        sampled_data.extend(label_data)\n",
    "\n",
    "    # Mélanger les observations pour obtenir un ordre aléatoire\n",
    "    np.random.shuffle(sampled_data)\n",
    "\n",
    "    # Créer un Dataset Hugging Face à partir des observations échantillonnées\n",
    "    sampled_dataset = Dataset.from_dict({key: [example[key] for example in sampled_data] for key in sampled_data[0]})\n",
    "    \n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_random(dataset,nb_ob):\n",
    "    dataset=dataset.shuffle()\n",
    "    echantillon_aleatoire = dataset.select(range(nb_ob))\n",
    "    return echantillon_aleatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"Note\"],padding=True, truncation=True,max_length=512)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, accuracy = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        output = model(input_ids,\n",
    "            token_type_ids=None, \n",
    "            attention_mask=input_mask, \n",
    "            labels=labels)\n",
    "        loss += output.loss.item()\n",
    "        preds_batch = torch.argmax(output.logits, axis=1)\n",
    "        batch_acc = torch.mean((preds_batch == labels).float())\n",
    "        accuracy.append(batch_acc)\n",
    "        \n",
    "    accuracy = torch.mean(torch.tensor(accuracy))\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(protocole):\n",
    "    match = re.search(r'/(\\d+)/', protocole)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        return int(year[:4])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_reform(example):\n",
    "    example['reform'] = 'pre' if example['date'] <= 1912 else 'post'\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_filter(example, year):\n",
    "    print(\"hey\")\n",
    "    return example['date'] >= year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_notes(example):\n",
    "    note = example['Note']\n",
    "    return ('Herr' in note or\n",
    "            'Fru'  in note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_size(example):\n",
    "    note = example['Note']\n",
    "    word_count = len(note.split())\n",
    "    return word_count > 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 3378877\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 725974\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"swerick_data_party_train.pkl\", \"test\": \"swerick_data_party_test.pkl\"}\n",
    "party_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': ['statsverkspropositionen gjorda framställningar rörande utgifterna\\n          för budgetåret 1969/70 inom försvarsdepartementets verksamhetsområde\\n          jämte motioner.', 'Lördagen den 1 april e. m. Nr 29. 87', 'Herr Restadius, som yttrade:', 'Nr 65.', 'Regeringens agerande i husläkarfrågan och i propositionen om\\n            fri etablering är sådant som har lett fram till politiska låsningar\\n            i utformningen av välfärden. Men som jag ser det gäller motsättningarna\\n            främst långtgående nedskärningar och privatiseringar i välfärdssamhället.'], 'Note/seg': ['note', 'note', 'note', 'note', 'seg'], '__index_level_0__': [6608920, 3521581, 1119166, 2126533, 10261505]}\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"/home/laurinemeier/swerick/evaluation/swerick_data_seg_train.pkl\", \"test\": \"/home/laurinemeier/swerick/evaluation/swerick_data_seg_test.pkl\"}\n",
    "seg_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(seg_dataset[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the date of the protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'protocole': 'data/1867/prot-1867--ak--0119.xml', 'Note': 'För min del kan jag ej se något hinder för företagande af dessa val ännu något tidigare än Herr Talmannen föreslagit. Jag har föreställt mig, att vi så mycket som möjligt borde taga vara på den dyrbara tiden, och att nu ifrågakomna val kunde förrättas t. ex. på Tisdagen, och valen till suppleanter i Utskotten på Thorsdagen.', 'id': 'i-PyY1Vo1W6WaajphhpnKHN8', 'party': None, 'gender': 'man', 'date': 1867}\n",
      "{'protocole': 'data/1867/prot-1867--ak--0118.xml', 'Note': 'Mine Herrar! Sannolikt är det för många bland Eder oväntadt att se en prest intaga talmansstolen i Riksdagens Andra Kammare. Det sker också icke efter min egen önskan. Jag har lika litet eftersträfvat detta höga och ansvarsfulla förtroende, som jag sökt sjelfva riksdagsmannakallets hedrande uppdrag. Men då vahnännen inom den krets jag tillhör lemnat mig det sednare, och Kongl. Maj:t sedermera behagat förläna mig det förra, har jag ej haft giltiga skäl att undandraga mig någotdera. Det är nemligen min öfvertygelse, att hvarje Svensk medborgare, i hvilken samhällsställning han än må befinna sig, är skyldig att gå dit han kallas, så snart kallelsen för honom innebär en uppmaning att, på den plats honom anvisas, efter bästa förstånd bidraga till fäderneslandets gagn. Då I, Mine Herrar, utan tvitvel alla hafven samma öfvertygelse, vågar jag hoppas, att af Eder blifva emottagen med den välvilja, som en ärlig afsigt förtjenar, och att derjemte få påräkna det öfverseende, som jag säkert vid många tillfällen nödgas taga i anspråk. Jag behöfver icke här beskrifva de svårigheter, som åtfölja talmansbefattningen och förorsaka bryderi äfven för en långt större förmåga än den, som blifvit mig beskärd, ej heller erinra, att dessa svårigheter ingalunda blifvit förminskade utan snarare förökade genom den nya Riksdags-ordningen, så länge hon ännu saknar den gamlas traditioner. En antydning härom torde vara tillräcklig för att framkalla den endrägt, det trofasta inbördes tillstånd, hvarförutan vår gemensamma verksamhet omöjligt kan vinna erforderlig reda och önskad framgång.', 'id': 'i-QW5W2LNiETVn7C6pxUPTUU', 'party': None, 'gender': 'man', 'date': 1867}\n"
     ]
    }
   ],
   "source": [
    "dates = [extract_date_from_filename(row['protocole']) for row in party_dataset['train']]\n",
    "dates_test = [extract_date_from_filename(row['protocole']) for row in party_dataset['test']]\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date', dates)\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date', dates_test)\n",
    "\n",
    "print(party_dataset[\"train\"][0])\n",
    "print(party_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'determine_reform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Label spellign reformp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m party_dataset \u001b[38;5;241m=\u001b[39m party_dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[43mdetermine_reform\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'determine_reform' is not defined"
     ]
    }
   ],
   "source": [
    "#Label spellign reformp\n",
    "party_dataset = party_dataset.map(determine_reform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Year filter \n",
    "party_dataset = party_dataset.filter(lambda x: year_filter(x, 2000))\n",
    "party_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.57723263]\n",
      " [-2.57723263]\n",
      " [-2.57723263]\n",
      " ...\n",
      " [ 1.05391207]\n",
      " [ 1.05391207]\n",
      " [ 1.05391207]]\n"
     ]
    }
   ],
   "source": [
    "#Standarlized the date\n",
    "\n",
    "date_scaler = StandardScaler()\n",
    "dates_train_2d = [[date] for date in party_dataset['train'][\"date\"]]\n",
    "dates_test_2d=[[date] for date in party_dataset['test'][\"date\"]]\n",
    "date_scaler.fit(dates_train_2d)\n",
    "dates_train =date_scaler.transform(dates_train_2d)\n",
    "dates_test =date_scaler.transform(dates_test_2d)\n",
    "print(dates_train)\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date_scaled', dates_train.squeeze())\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date_scaled', dates_test.squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].filter(lambda x : filter_NaN(\"gender\",x))\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].filter(lambda x : filter_NaN(\"gender\",x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Text for party Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset_note_train=party_dataset['train'].filter(filter_notes)\n",
    "party_dataset_note_test=party_dataset['test'].filter(filter_notes)\n",
    "party_dataset_note_train=party_dataset_note_train.filter(filter_size)\n",
    "party_dataset_note_test=party_dataset_note_test.filter(filter_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a subset for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced subset \n",
    "party_train_datasets = subset_label(party_dataset_year[\"train\"],10000,\"party\")\n",
    "party_test_datasets = subset_label(party_dataset[\"test\"],5000,\"party\")\n",
    "#party_valid_datasets = subset_label(party_valid_dataset[\"valid\"],5000,\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random subset\n",
    "train_set= subset_random(party_dataset[\"train\"],1000)\n",
    "test_set = subset_random(party_dataset[\"test\"],10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content   tag  __index_level_0__\n",
      "0                           Torsdagen den Gjuni 1985  note            8950568\n",
      "1  Avkastningen av den hälftenandel som faller på...   seg            8239669\n",
      "2  Beträffande herr Kilboms invändningar så talad...   seg            3530266\n",
      "3  Transportstyrelsen har givit SMHI i uppdrag at...   seg           12627748\n",
      "4   Enligt uppgift i pressen, IDAG den 25 april 1991  note            9785354\n"
     ]
    }
   ],
   "source": [
    "#Converting them into csv file\n",
    "df =train_set.to_pandas()\n",
    "df = df.rename(columns={\"Text\":\"content\",\"Note/seg\" : \"tag\"})\n",
    "print(df.head())\n",
    "df.to_csv(\"swerick_subsetdata_note_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Party alignement classification\n",
      "training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m party_gender_detection\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_binary_bert,compare_model\n\u001b[0;32m----> 9\u001b[0m \u001b[43mreform_scratch_classfication\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrained_gender_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4611166\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtokenizer2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/laurinemeier/swerick/exbert_tokenizer_60k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m  \u001b[38;5;66;03m#party_gender_detection(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#regression_year(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_date_train100.csv\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#reform_classification(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_test.csv\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#reform_scratch_classfication(\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-3511320\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_test.csv\",\"/home/laurinemeier/swerick/exbert_tokenizer\")\u001b[39;00m\n",
      "File \u001b[0;32m~/swerick/evaluation/evaluation.py:532\u001b[0m, in \u001b[0;36mreform_scratch_classfication\u001b[0;34m(model_filename, base_model1, base_model2, data_path_train, data_path_test, tokenizer1, tokenizer2)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Exécuter la commande\u001b[39;00m\n\u001b[1;32m    531\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m--> 532\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Print the output\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stdout\u001b[38;5;241m.\u001b[39mdecode())\n",
      "File \u001b[0;32m~/anaconda3/envs/swerick/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/swerick/lib/python3.12/subprocess.py:2113\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2107\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2108\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2110\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2111\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2113\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/swerick/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training for a specific task\n",
    "reform_scratch_classfication(\"trained_gender_classification\",\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000\",\"/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4611166\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_test.csv\",tokenizer2=\"/home/laurinemeier/swerick/exbert_tokenizer_60k\")\n",
    "#regression_year(\"/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4806004\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_date_train100.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First round\n",
      "Year regression\n",
      "training\n",
      "stdout: \u001b[32m12:06:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m12:06:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m12:06:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([ 0.2522,  0.3465, -1.5869,  0.8653, -1.5398, -1.5162,  1.0539, -0.1015,\n",
      "        -1.7520,  0.5823, -0.5495, -2.4829,  0.5352, -1.5633,  0.8181, -0.3372,\n",
      "         0.3937,  1.0068,  0.6767, -0.4316, -0.2193, -1.3747,  0.9360,  0.6059,\n",
      "        -0.0543,  0.8889,  0.5588,  0.9596, -1.5633,  0.9360,  1.0068,  0.2994,\n",
      "         0.4409,  0.5588,  0.3701,  0.2994, -2.3650, -1.4926,  0.8889,  0.4173,\n",
      "        -1.1389,  0.4644, -1.3983,  0.6531,  0.3701, -0.7617,  0.7238,  0.9596,\n",
      "         0.7474, -2.1764,  0.9596, -1.8934, -0.6909,  1.0068,  0.9596, -2.5065,\n",
      "        -1.2568, -2.5537, -0.0779,  0.2758,  0.5116, -0.2193,  0.3465,  0.4880,\n",
      "         0.7945,  0.7945,  0.8889,  0.5352,  0.4644,  1.0539, -1.4926,  1.0068,\n",
      "         1.0539, -1.0918, -0.3608,  0.6531,  0.7002, -0.3137,  1.0303,  0.3465,\n",
      "         0.8417, -1.1389,  0.8889,  0.2758,  0.4880,  0.7945,  0.6531,  0.4409,\n",
      "         0.7002,  0.5823,  0.4880, -1.3511, -0.2429,  0.5823,  0.8653,  0.9124,\n",
      "         0.7474,  1.0068,  0.7710,  0.5116,  0.5116,  0.3701,  0.4409,  0.8889,\n",
      "         0.7238,  0.8653,  0.7238,  0.7238,  0.4644,  0.6059, -1.4219, -0.2665,\n",
      "         0.7474,  0.4409, -1.8699,  0.2287,  0.5588,  0.5116,  0.9124,  0.3230,\n",
      "        -0.9739,  0.3230, -1.9642,  1.0068,  0.3937,  0.6295, -2.3179, -0.2901,\n",
      "         0.9360, -1.5869,  0.5823,  0.6531,  0.9596, -0.5966, -0.3844,  0.5823,\n",
      "        -1.0682,  0.5116,  0.1343, -1.7756])\u001b[0m\n",
      "length train loader : 140\n",
      "length valid loader : 30\n",
      "length train loader : 140\n",
      "KBLab/bert-base-swedish-cased\n",
      "/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4806004\n",
      "\u001b[34m12:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m12:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.945\u001b[0m\n",
      "\u001b[34m12:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.205\u001b[0m\n",
      "\u001b[34m12:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.3598676919937134\u001b[0m\n",
      "\u001b[32m12:06:30 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m12:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 4.967\u001b[0m\n",
      "\u001b[34m12:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.299\u001b[0m\n",
      "\u001b[34m12:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.339255690574646\u001b[0m\n",
      "\u001b[32m12:06:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m12:06:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 1.952\u001b[0m\n",
      "\u001b[34m12:06:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 4.172\u001b[0m\n",
      "\u001b[34m12:06:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.5704621076583862\u001b[0m\n",
      "\u001b[32m12:06:37 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m12:06:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 1.196\u001b[0m\n",
      "\u001b[34m12:06:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.826\u001b[0m\n",
      "\u001b[34m12:06:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.599692165851593\u001b[0m\n",
      "\u001b[32m12:06:41 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "\u001b[34m12:06:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.845\u001b[0m\n",
      "\u001b[34m12:06:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.746\u001b[0m\n",
      "\u001b[34m12:06:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.6061856746673584\u001b[0m\n",
      "\u001b[32m12:06:45 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "\u001b[34m12:06:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.590\u001b[0m\n",
      "\u001b[34m12:06:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.910\u001b[0m\n",
      "\u001b[34m12:06:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.5863520503044128\u001b[0m\n",
      "\u001b[32m12:06:50 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "\u001b[34m12:06:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.525\u001b[0m\n",
      "\u001b[34m12:06:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.770\u001b[0m\n",
      "\u001b[34m12:06:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.6015129089355469\u001b[0m\n",
      "\u001b[32m12:06:53 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "\u001b[34m12:06:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.613\u001b[0m\n",
      "\u001b[34m12:06:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.808\u001b[0m\n",
      "\u001b[34m12:06:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.5956789255142212\u001b[0m\n",
      "\u001b[32m12:06:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m12:06:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "\u001b[34m12:06:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.675\u001b[0m\n",
      "\u001b[34m12:06:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.720\u001b[0m\n",
      "\u001b[34m12:06:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.605078399181366\u001b[0m\n",
      "\u001b[32m12:06:59 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "\u001b[34m12:07:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.470\u001b[0m\n",
      "\u001b[34m12:07:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.648\u001b[0m\n",
      "\u001b[34m12:07:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.6133259534835815\u001b[0m\n",
      "\u001b[32m12:07:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "new model\n",
      "/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4806004\n",
      "\u001b[34m12:07:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m12:07:07 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 13.797\u001b[0m\n",
      "\u001b[34m12:07:07 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.154\u001b[0m\n",
      "\u001b[34m12:07:07 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.28833243250846863\u001b[0m\n",
      "\u001b[32m12:07:07 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m12:07:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.163\u001b[0m\n",
      "\u001b[34m12:07:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 3.484\u001b[0m\n",
      "\u001b[34m12:07:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.6627269983291626\u001b[0m\n",
      "\u001b[32m12:07:12 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m12:07:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 2.100\u001b[0m\n",
      "\u001b[34m12:07:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 2.433\u001b[0m\n",
      "\u001b[34m12:07:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.7600497007369995\u001b[0m\n",
      "\u001b[32m12:07:16 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m12:07:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 1.155\u001b[0m\n",
      "\u001b[34m12:07:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 2.408\u001b[0m\n",
      "\u001b[34m12:07:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.7725224494934082\u001b[0m\n",
      "\u001b[32m12:07:20 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:22 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 4 starts!\u001b[0m\n",
      "\u001b[34m12:07:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.950\u001b[0m\n",
      "\u001b[34m12:07:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 2.085\u001b[0m\n",
      "\u001b[34m12:07:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8036563396453857\u001b[0m\n",
      "\u001b[32m12:07:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 5 starts!\u001b[0m\n",
      "\u001b[34m12:07:29 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.664\u001b[0m\n",
      "\u001b[34m12:07:29 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.425\u001b[0m\n",
      "\u001b[34m12:07:29 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8646344542503357\u001b[0m\n",
      "\u001b[32m12:07:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 6 starts!\u001b[0m\n",
      "\u001b[34m12:07:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.516\u001b[0m\n",
      "\u001b[34m12:07:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.381\u001b[0m\n",
      "\u001b[34m12:07:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8675885200500488\u001b[0m\n",
      "\u001b[32m12:07:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 7 starts!\u001b[0m\n",
      "\u001b[34m12:07:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.428\u001b[0m\n",
      "\u001b[34m12:07:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.369\u001b[0m\n",
      "\u001b[34m12:07:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8697558641433716\u001b[0m\n",
      "\u001b[32m12:07:38 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 8 starts!\u001b[0m\n",
      "\u001b[34m12:07:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.539\u001b[0m\n",
      "\u001b[34m12:07:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.422\u001b[0m\n",
      "\u001b[34m12:07:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8652600049972534\u001b[0m\n",
      "\u001b[32m12:07:42 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m12:07:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 9 starts!\u001b[0m\n",
      "\u001b[34m12:07:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.552\u001b[0m\n",
      "\u001b[34m12:07:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.463\u001b[0m\n",
      "\u001b[34m12:07:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation r2: 0.8615443110466003\u001b[0m\n",
      "\u001b[32m12:07:45 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4806004 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.75it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.75it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.80it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.89it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.00it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.71it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.84it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.32it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.02it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.85it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.67it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.65it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.98it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.67it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.32it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.66it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.77it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.78it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.29it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.77it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.76it/s]\n",
      "\n",
      "comparing\n",
      "stdout: size test set 10000\n",
      "\n",
      "Loss model 1: 2.3718385427474975\n",
      "\n",
      "R2 model1: tensor(0.8448)\n",
      "\n",
      "Loss model 2: 1.627277031135559\n",
      "\n",
      "R2 model2: tensor(0.8961)\n",
      "2.3718385427474975\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 625/625 [01:03<00:00,  9.82it/s]\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 625/625 [01:03<00:00,  9.80it/s]\n",
      "/home/laurinemeier/swerick/evaluation/compare_models_regression.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"\\nR2 model1:\",torch.mean(torch.tensor(r21)))\n",
      "/home/laurinemeier/swerick/evaluation/compare_models_regression.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"\\nR2 model2:\",torch.mean(torch.tensor(r22)))\n",
      "\n",
      "First round\n",
      "Year regression\n",
      "training\n"
     ]
    }
   ],
   "source": [
    "#TRaining mutltiple times with random training dataset\n",
    "\n",
    "\n",
    "for i in range (5):\n",
    "        train_set = subset_random(party_dataset[\"train\"],200)\n",
    "        df=train_set.to_pandas()\n",
    "        df=df.rename(columns={\"Note\":\"content\", \"date_scaled\" : \"tag\"})\n",
    "\n",
    "        df.to_csv(\"swerick_subsetdata_date_train200.csv\")\n",
    "        print(\"First round\")\n",
    "        #reform_scratch_classfication(\"trained_gender_classification\",\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000\",\"/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4611166\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_test.csv\",tokenizer2=\"/home/laurinemeier/swerick/exbert_tokenizer_60k\")\n",
    "        regression_year(\"/home/laurinemeier/swerick/exbert_60k-finetuned-imdb/checkpoint-4806004\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_date_train200.csv\",\"/home/laurinemeier/swerick/exbert_tokenizer_60k\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining classficaition : LIME EXplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting dataset\n",
    "# \n",
    "df = pd.read_csv('/home/laurinemeier/swerick/evaluation/trained_note_classification054000/correct_exbert_notKBBERT.csv')\n",
    "MAX_TEXT_LENGTH = 1000 # if not the explainer crash\n",
    "df['content'] = df['content'].apply(lambda x: x[:MAX_TEXT_LENGTH] if isinstance(x, str) else x)\n",
    "df=df.sample(n=10,random_state=42)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_dicts():\n",
    "    return defaultdict(int), defaultdict(int)\n",
    "\n",
    "\n",
    "# Knwoing the token resposible for classification\n",
    "models = {\n",
    "   'kbbert': ('/home/laurinemeier/swerick/evaluation/trained_reform_classification',tokenizer),\n",
    "    'sbertex': ('/home/laurinemeier/swerick/evaluation/trained_reform_classification054000', exbert_tokenizer),\n",
    "    'daptbert': ('/home/laurinemeier/swerick/evaluation/trained_reform_classification061000',tokenizer),\n",
    "   # 'sparlbert': ('/home/laurinemeier/swerick/evaluation/trained_reform_classification258900', swerick_tokenizer),\n",
    "}\n",
    "\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=[0, 1])\n",
    "\n",
    "token_freqs = {model_name: create_token_dicts() for model_name in models}\n",
    "\n",
    "for model_name, (model_path, tokenizer) in models.items():\n",
    "    print(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    pre_tokens_freq, post_tokens_freq = token_freqs[model_name]\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['content']\n",
    "        print(row['tag'])\n",
    "\n",
    "        exp = explainer.explain_instance(text, lambda x: predict_proba(x, model, tokenizer), num_features=2)\n",
    "        exp.show_in_notebook(text=True, labels=(exp.available_labels()[0],))\n",
    "\n",
    "         # Predict the label\n",
    "        prediction = model(**tokenizer(text, return_tensors='pt')).logits.argmax(dim=1).item()\n",
    "        predicted_label = 'pre' if prediction == 1 else 'post'\n",
    "        \n",
    "        sorted_list=sorted(exp.as_list(), key=lambda x:x[1], reverse=True)\n",
    "        important_tokens = [word for word, importance in sorted_list][:3]\n",
    "\n",
    "        if predicted_label == 'pre':\n",
    "            for token in important_tokens:\n",
    "                pre_tokens_freq[token] += 1\n",
    "        elif predicted_label == 'post':\n",
    "            for token in important_tokens:\n",
    "                post_tokens_freq[token] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
