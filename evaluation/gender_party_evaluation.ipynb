{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import preprocessing\n",
    "import preprocessing\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"KBLab/bert-base-swedish-cased\"\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "model_finetuned=model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_exbert=  AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000\")\n",
    "model_exbert=model_exbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "exbert_tokenizer = AutoTokenizer.from_pretrained(\"/home/laurinemeier/swerick/exbert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "swerick_tokenizer= PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/home/laurinemeier/swerick/alvis_project/pretraining_from_scratch/tokenizer_swerick.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('swerick_tokenizer/tokenizer_config.json',\n",
       " 'swerick_tokenizer/special_tokens_map.json',\n",
       " 'swerick_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swerick_tokenizer.save_pretrained(\"swerick_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "config = transformers.BertConfig.from_pretrained(\"/home/laurinemeier/swerick/alvis_project/pretraining_from_scratch/checkpoint-5258900\")\n",
    "mosaicBert = AutoModelForMaskedLM.from_pretrained(\"/home/laurinemeier/swerick/alvis_project/pretraining_from_scratch/checkpoint-5258900\",config=config,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_NaN(subset,example):\n",
    "    return example[subset] is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_label(dataset,nb_obs,label_name):\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Calculer le nombre d'observations pour chaque étiquette\n",
    "    grouped_data = df.groupby(label_name)\n",
    "\n",
    "    # Calculer le nombre d'observations par étiquette pour obtenir une répartition uniforme\n",
    "    total_samples = nb_obs\n",
    "    samples_per_label = total_samples // len(grouped_data.groups)\n",
    "\n",
    "    # Créer une liste pour stocker les observations échantillonnées\n",
    "    sampled_data = []\n",
    "\n",
    "    # Prélever aléatoirement les observations pour chaque groupe de label\n",
    "    for group_label, group_data in grouped_data.groups.items():\n",
    "        group_dataset=dataset.select(group_data)\n",
    "        label_data = group_dataset.shuffle(seed=np.random.randint(1, 1000)).select(range(min(len(group_data), samples_per_label)))\n",
    "        sampled_data.extend(label_data)\n",
    "\n",
    "    # Mélanger les observations pour obtenir un ordre aléatoire\n",
    "    np.random.shuffle(sampled_data)\n",
    "\n",
    "    # Créer un Dataset Hugging Face à partir des observations échantillonnées\n",
    "    sampled_dataset = Dataset.from_dict({key: [example[key] for example in sampled_data] for key in sampled_data[0]})\n",
    "    \n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_random(dataset,nb_ob):\n",
    "    dataset=dataset.shuffle()\n",
    "    echantillon_aleatoire = dataset.select(range(nb_ob))\n",
    "    return echantillon_aleatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"Note\"],padding=True, truncation=True,max_length=512)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, accuracy = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        output = model(input_ids,\n",
    "            token_type_ids=None, \n",
    "            attention_mask=input_mask, \n",
    "            labels=labels)\n",
    "        loss += output.loss.item()\n",
    "        preds_batch = torch.argmax(output.logits, axis=1)\n",
    "        batch_acc = torch.mean((preds_batch == labels).float())\n",
    "        accuracy.append(batch_acc)\n",
    "        \n",
    "    accuracy = torch.mean(torch.tensor(accuracy))\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(protocole):\n",
    "    match = re.search(r'/(\\d+)/', protocole)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        return int(year[:4])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 3378877\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['protocole', 'Note', 'id', 'party', 'gender'],\n",
      "        num_rows: 725974\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"swerick_data_party_train.pkl\", \"test\": \"swerick_data_party_test.pkl\"}\n",
    "party_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"swerick_data_party_train.pkl\")\n",
    "df= df.sample(n=100, random_state=42)\n",
    "df.to_csv(\"swerick_data_party_train_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Fonction pour mettre à jour le Counter basé sur le champ 'party'\n",
    "def add_party_counts(batch, counts):\n",
    "    print(\"hpp\")\n",
    "    counts.update(batch['party'])\n",
    "    return batch  \n",
    "\n",
    "\n",
    "counts = Counter()\n",
    "party_dataset[\"train\"].map(add_party_counts, fn_kwargs={'counts': counts}, batched=True)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties_to_keep = {party for party, count in counts.items() if count >=100 and party !='None'}\n",
    "print(parties_to_keep)\n",
    "\n",
    "party_dataset = party_dataset.filter(lambda example: example['party'] in parties_to_keep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(party_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_alignment = {\n",
    "    '\"vänstern\"':'left',\n",
    "    'Folkpartiet':'left',\n",
    "    'Folkpartiet (1895–1900)':'left',\n",
    "    'Frisinnade folkpartiet':'left',\n",
    "    'Kommunistiska partiet':'left',\n",
    "    'Socialdemokraterna':'left',\n",
    "    'Socialdemokratiska vänstergruppen':'left',\n",
    "    'Socialistiska partiet':'left',\n",
    "    'Miljöpartiet':'agro',\n",
    "    'Vänsterpartiet' :'left',\n",
    "    'Sveriges kommunistiska parti': 'left',\n",
    "    'Andra kammarens frihandelsparti' :'liberal',\n",
    "    'Första kammarens minoritetsparti':'right',\n",
    "    'Ehrenheimska partiet':'right',\n",
    "    'Första kammarens moderata parti':'liberal',\n",
    "    'Bondeförbundet':'agro',\n",
    "    'Centerpartiet' :'liberal',\n",
    "    'Det förenade högerpartiet':'right',\n",
    "    'Första kammarens ministeriella grupp':'right',\n",
    "    'Första kammarens nationella parti':'right',\n",
    "    'Första kammarens protektionistiska parti':'right',\n",
    "    'högervilde':'right',\n",
    "    'Högerpartiet':'right',\n",
    "    'Högerpartiet de konservativa':'right',\n",
    "    'Kilbomspartiet':'left',\n",
    "    'Lantmannapartiets filial' :'agro',\n",
    "    'Nya lantmannapartiet':'agro',\n",
    "    'Skånska partiet':'agro',\n",
    "    'Högerns riksdagsgrupp':'right',\n",
    "    'Högerpartiet ':'right',\n",
    "    'Jordbrukarnas fria grupp':'agro',\n",
    "    'borgmästarepartiet':'right',\n",
    "    'Junkerpartiet':'right',\n",
    "    'Kristdemokraterna' :'right',\n",
    "    'Moderaterna' :'right',\n",
    "    'Sverigedemokraterna' :'right',\n",
    "    'Nationella framstegspartiet':'right',\n",
    "    'Ny demokrati' :'right',\n",
    "    'Gamla lantmannapartiet':'agro',\n",
    "    'Lantmannapartiet':'agro',\n",
    "    'Andra kammarens center':'liberal',\n",
    "    'frihandelsvänlig vilde':'liberal',\n",
    "    'frisinnad vilde':'liberal',\n",
    "    'Stockholmsbänken':'liberal',\n",
    "    'Frihandelsvänliga centern':'liberal',\n",
    "    'ministeriella partiet':'liberal',\n",
    "    'Centern (partigrupp 1873-1882)':'liberal',\n",
    "    'Centern (partigrupp 1885-1887)':'liberal',\n",
    "    'Första kammarens konservativa grupp':'right',\n",
    "    'Frisinnade landsföreningen':'liberal',\n",
    "    'Frisinnade försvarsvänner':'liberal',\n",
    "    'Friesenska diskussionsklubben':'liberal',\n",
    "    'Liberala riksdagspartiet':'liberal',\n",
    "    'Liberala samlingspartiet':'liberal',\n",
    "    'Liberalerna':'liberal',\n",
    "    'Nya centern (partigrupp 1883-1887)':'liberal',\n",
    "    'Nyliberala partiet':'liberal',\n",
    "    'Medborgerlig samling (1964–1968)':'liberal',\n",
    "    'Lantmanna- och borgarepartiet inom andrakammaren':'agro'\n",
    "}\n",
    "\n",
    "def determine_alignment(example):\n",
    "    party = example['party']\n",
    "    alignment = party_alignment.get(party, 'Non classifié')\n",
    "    example['alignment'] = alignment\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset = party_dataset.map(determine_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(parties['train']['party']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Text', 'Note/seg', '__index_level_0__'],\n",
      "        num_rows: 11286588\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Text', 'Note/seg', '__index_level_0__'],\n",
      "        num_rows: 2821648\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"/home/laurinemeier/swerick/evaluation/swerick_data_seg_train.pkl\", \"test\": \"/home/laurinemeier/swerick/evaluation/swerick_data_seg_test.pkl\"}\n",
    "seg_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(seg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'protocole': 'data/1867/prot-1867--ak--0119.xml', 'Note': 'För min del kan jag ej se något hinder för företagande af dessa val ännu något tidigare än Herr Talmannen föreslagit. Jag har föreställt mig, att vi så mycket som möjligt borde taga vara på den dyrbara tiden, och att nu ifrågakomna val kunde förrättas t. ex. på Tisdagen, och valen till suppleanter i Utskotten på Thorsdagen.', 'id': 'i-PyY1Vo1W6WaajphhpnKHN8', 'party': None, 'gender': 'man', 'date': 1867}\n",
      "{'protocole': 'data/1867/prot-1867--ak--0118.xml', 'Note': 'Mine Herrar! Sannolikt är det för många bland Eder oväntadt att se en prest intaga talmansstolen i Riksdagens Andra Kammare. Det sker också icke efter min egen önskan. Jag har lika litet eftersträfvat detta höga och ansvarsfulla förtroende, som jag sökt sjelfva riksdagsmannakallets hedrande uppdrag. Men då vahnännen inom den krets jag tillhör lemnat mig det sednare, och Kongl. Maj:t sedermera behagat förläna mig det förra, har jag ej haft giltiga skäl att undandraga mig någotdera. Det är nemligen min öfvertygelse, att hvarje Svensk medborgare, i hvilken samhällsställning han än må befinna sig, är skyldig att gå dit han kallas, så snart kallelsen för honom innebär en uppmaning att, på den plats honom anvisas, efter bästa förstånd bidraga till fäderneslandets gagn. Då I, Mine Herrar, utan tvitvel alla hafven samma öfvertygelse, vågar jag hoppas, att af Eder blifva emottagen med den välvilja, som en ärlig afsigt förtjenar, och att derjemte få påräkna det öfverseende, som jag säkert vid många tillfällen nödgas taga i anspråk. Jag behöfver icke här beskrifva de svårigheter, som åtfölja talmansbefattningen och förorsaka bryderi äfven för en långt större förmåga än den, som blifvit mig beskärd, ej heller erinra, att dessa svårigheter ingalunda blifvit förminskade utan snarare förökade genom den nya Riksdags-ordningen, så länge hon ännu saknar den gamlas traditioner. En antydning härom torde vara tillräcklig för att framkalla den endrägt, det trofasta inbördes tillstånd, hvarförutan vår gemensamma verksamhet omöjligt kan vinna erforderlig reda och önskad framgång.', 'id': 'i-QW5W2LNiETVn7C6pxUPTUU', 'party': None, 'gender': 'man', 'date': 1867}\n"
     ]
    }
   ],
   "source": [
    "dates = [extract_date_from_filename(row['protocole']) for row in party_dataset['train']]\n",
    "dates_test = [extract_date_from_filename(row['protocole']) for row in party_dataset['test']]\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date', dates)\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date', dates_test)\n",
    "\n",
    "print(party_dataset[\"train\"][0])\n",
    "print(party_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_reform(example):\n",
    "    example['reform'] = 'pre' if example['date'] <= 1912 else 'post'\n",
    "    return example\n",
    "\n",
    "# Appliquer la fonction à chaque ligne du dataset\n",
    "party_dataset = party_dataset.map(determine_reform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['protocole', 'Note', 'id', 'party', 'gender', 'date'],\n",
       "        num_rows: 1322959\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['protocole', 'Note', 'id', 'party', 'gender', 'date'],\n",
       "        num_rows: 281978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def year_filter(example, year):\n",
    "    print(\"hey\")\n",
    "    return example['date'] >= year\n",
    "\n",
    "party_dataset = party_dataset.filter(lambda x: year_filter(x, 2000))\n",
    "party_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021}\n"
     ]
    }
   ],
   "source": [
    "print(set(list(party_dataset[\"train\"][\"date\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"valid\": \"swerick_data_party_valid.pkl\"}\n",
    "party_valid_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"valid\": \"swerick_data_intro_valid.csv\"}\n",
    "party_valid_dataset = load_dataset(\"pandas\",data_files=data_files)\n",
    "print(party_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_valid = [extract_date_from_filename(row['protocole']) for row in party_valid_dataset['valid']]\n",
    "party_valid_dataset['valid'] = party_valid_dataset['valid'].add_column('date', dates_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.57723263]\n",
      " [-2.57723263]\n",
      " [-2.57723263]\n",
      " ...\n",
      " [ 1.05391207]\n",
      " [ 1.05391207]\n",
      " [ 1.05391207]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "date_scaler = StandardScaler()\n",
    "dates_train_2d = [[date] for date in party_dataset['train'][\"date\"]]\n",
    "dates_test_2d=[[date] for date in party_dataset['test'][\"date\"]]\n",
    "date_scaler.fit(dates_train_2d)\n",
    "dates_train =date_scaler.transform(dates_train_2d)\n",
    "dates_test =date_scaler.transform(dates_test_2d)\n",
    "print(dates_train)\n",
    "party_dataset['train'] = party_dataset['train'].add_column('date_scaled', dates_train.squeeze())\n",
    "party_dataset['test'] = party_dataset['test'].add_column('date_scaled', dates_test.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].filter(lambda x : filter_NaN(\"gender\",x))\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].filter(lambda x : filter_NaN(\"gender\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e7443fea24a2397a0e248682c9309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1322822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e354671d52d34512912bf383f659cf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/281935 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['protocole', 'Note', 'id', 'party', 'gender', 'date'],\n",
      "    num_rows: 1322822\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def filter_notes(example):\n",
    "    note = example['Note']\n",
    "    return ('Herr' in note or\n",
    "            'Fru'  in note)\n",
    "    \n",
    "    \n",
    "party_dataset_note_train=party_dataset['train'].filter(filter_notes)\n",
    "party_dataset_note_test=party_dataset['test'].filter(filter_notes)\n",
    "print(party_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fru talman! Vi har anledning att vara stolta över', 'Fru talman! Sverige har ett rikt kulturliv. Det ska vi värna om och vara stolta över.', 'Fru talman! Sverige får aldrig någonsin mer hamna i den tvångströja som växande underskott sätter på politiken.', 'Fru talman! Ärade ledamöter och åhörare! Det är med stor stolthet som jag i dag kan presentera en budget som kännetecknas av reformer och ett fortsatt ansvar för en balanserad ekonomisk utveckling. Inflationen är låg. Räntorna är låga. De offentliga finanserna uppvisar betydande överskott. Redan nästa år har nettoskulden försvunnit. Vår ekonomi växer i år med nära 4 % och förväntas öka nästan lika mycket nästa år. Målet om 4 % arbetslöshet, som sattes upp för fyra år sedan, är så gott som nått. I dagens situation har vi utrymme både för att öka utgifterna och att sänka skatterna, och ändå kan vi betala av mer på statsskulden än vi tidigare hade planerat.', 'Fru talman! Den första meningen i budgeten säger att detta är en budget för framtiden. Jag vill gärna säga att det brukar budgetar vara - annars är de inga budgetar.', 'Fru talman! Det som blir avgörande för välfärden och människors trygghet i framtiden kommer inte längre att främst vara de svenska företagens konkurrenskraft utan Sveriges konkurrenskraft om företagande och kunskap. Det kräver reformer. Vi ser redan i dag, alltifrån företag som flyttar ut till alkohol som köps i utlandet och svenska åkerier som flaggar ut, att reformer krävs.', 'Fru talman! Jag kan börja med att hålla med Gunnar Hökmark om en sak, och det är att den här budgeten naturligtvis kunde vara bättre. Det är kanske udda att någon som står bakom budgeten håller med om det. Men jag tror att alla inser att precis så är det.', 'Fru talman! Ibland är det faktiskt glädjande att bli omkörd till vänster. Det strider inte ens mot gängse trafikregler. Jag vill inte kalla s-politikerna överbudspolitiker på grund av detta. Det var okej för oss att socialdemokraterna vågade släppa sin stela hållning i denna del av budgetprocessen så att vi inte tvingades vänta till nästa vår. Det är bra att ha mera flexibla socialdemokrater, som vi nu har.', 'Fru talman! Jag ska slutligen kommentera arbetstidsfrågan. Vi i Vänstern har inte nått någonstans när det gäller formella riksdagsbeslut. Det kanske kan ses som ett nederlag. Det är nog ett nederlag för oss i', 'Fru talman! Vi är tre partier som ännu en gång har visat att vi förmår att samarbeta trots att vi kan ha skilda åsikter i vissa frågor. Det är styrkan i vårt samarbete. Vi har visat i Vänstern att vi kan ta ansvar, att vi vill gå vidare och att vi törs ta strid när det behövs, och vi står för solidaritet, jämställdhet och rättvisa.']\n"
     ]
    }
   ],
   "source": [
    "print(party_dataset_note_train[:10]['Note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55791bc892424db9b6f71107b3cbf382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/267159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b98635c1e943fc82db005cb0257bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/57457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['protocole', 'Note', 'id', 'party', 'gender', 'date'],\n",
      "    num_rows: 199990\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def filter_size(example):\n",
    "    note = example['Note']\n",
    "    word_count = len(note.split())\n",
    "    return word_count > 30\n",
    "\n",
    "party_dataset_note_train=party_dataset_note_train.filter(filter_size)\n",
    "party_dataset_note_test=party_dataset_note_test.filter(filter_size)\n",
    "print(party_dataset_note_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].filter(lambda x : filter_NaN(\"party\",x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(party_dataset[\"train\"][\"gender\"])\n",
    "label_names = label_encoder.classes_\n",
    "label_dict={ i : label_names[i] for i in  range(len(label_names))}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels_gender.pkl\", \"wb\") as fp:   \n",
    "   pickle.dump(label_names, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.pkl\",\"rb\") as f :\n",
    "    label_names=pickle.load(f)\n",
    "\n",
    "print(label_names.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset[\"train\"]=party_dataset[\"train\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})\n",
    "party_dataset[\"test\"]=party_dataset[\"test\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_valid_dataset[\"valid\"]=party_valid_dataset[\"valid\"].map(lambda example :{\"party_labels\" : label_encoder.transform([example[\"party\"]])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(list(party_dataset_year['train']['party'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_train_datasets = subset_label(party_dataset_year[\"train\"],10000,\"party\")\n",
    "party_test_datasets = subset_label(party_dataset[\"test\"],5000,\"party\")\n",
    "#party_valid_datasets = subset_label(party_valid_dataset[\"valid\"],5000,\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1 = subset_random(party_dataset[\"train\"],100)\n",
    "train_set2 = subset_random(party_dataset[\"train\"],200)\n",
    "train_set3 = subset_random(party_dataset[\"train\"],500)\n",
    "train_set4= subset_random(party_dataset[\"train\"],1000)\n",
    "test_set = subset_random(party_dataset[\"test\"],10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = subset_random(party_dataset['test'],5000)\n",
    "train_set = subset_random(party_dataset[\"train\"],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(party_train_datasets)\n",
    "print(party_test_datasets)\n",
    "print(party_valid_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dataset = concatenate_datasets([train_set,test_set,valid_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets = train_set.map(tokenize_function,batched=True )\n",
    "tokenized_test_datasets = test_set.map(tokenize_function,batched=True )\n",
    "tokenized_valid_datasets = valid_set.map(tokenize_function,batched=True )\n",
    "tokenized_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_test_datasets=tokenized_test_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.remove_columns([\"protocole\",\"id\",\"party\",\"gender\",\"Note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets=tokenized_train_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_test_datasets=tokenized_test_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "tokenized_valid_datasets=tokenized_valid_datasets.rename_column(\"party_labels\",\"labels\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_test_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])\n",
    "tokenized_valid_datasets.set_format(type=\"torch\",columns=[\"input_ids\",\"labels\",\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers=4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        tokenized_train_datasets,\n",
    "        shuffle=True,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "        tokenized_valid_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "# Not used atm\n",
    "test_loader = DataLoader(\n",
    "        tokenized_test_datasets,\n",
    "        shuffle=False,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs =10\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(label_dict),\n",
    "        id2label=label_dict).to(\"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "num_training_steps = len(train_loader) * n_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "\n",
    "# Linear warmup and step decay\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_pickle(\"swerick_data_party_train.pkl\")\n",
    "df = df.rename(columns={\"Note\":\"content\",\"party\" : \"tag\"})\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_party(example):\n",
    "    if example =='left':\n",
    "        return 0\n",
    "    elif example ==\"center\":\n",
    "        return 1\n",
    "    elif example ==\"right\":\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"swerick_subsetdata_alignment_test_withoutcenter.csv\")\n",
    "df[\"tag\"][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.dropna(subset=\"tag\")\n",
    "df.to_csv(\"swerick_data_party_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words in the \"Note\" column\n",
    "df =party_dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Rename the columns\n",
    "\\\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "df.to_csv(\"swerick_subsetdata_date_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "8\n",
      "{0: 'Centerpartiet', 1: 'Kristdemokraterna', 2: 'Liberalerna', 3: 'Miljöpartiet', 4: 'Moderaterna', 5: 'Socialdemokraterna', 6: 'Sverigedemokraterna', 7: 'Vänsterpartiet'}\n",
      "  0%|                                                  | 0/1875 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████| 1875/1875 [03:12<00:00,  9.73it/s]\n",
      "Training Loss 1: 24.575\n",
      "\n",
      "Accuracy model 1: 0.4381\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.28      0.32      2897\n",
      "           1       0.45      0.24      0.31      2674\n",
      "           2       0.40      0.27      0.33      2924\n",
      "           3       0.54      0.18      0.27      2426\n",
      "           4       0.41      0.47      0.44      6276\n",
      "           5       0.46      0.73      0.56      8249\n",
      "           6       0.56      0.31      0.40      1500\n",
      "           7       0.42      0.32      0.36      3054\n",
      "\n",
      "    accuracy                           0.44     30000\n",
      "   macro avg       0.45      0.35      0.37     30000\n",
      "weighted avg       0.44      0.44      0.42     30000\n",
      "\n",
      "  0%|                                                  | 0/1875 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████| 1875/1875 [03:12<00:00,  9.74it/s]\n",
      "Training Loss 1: 24.178\n",
      "\n",
      "Accuracy model 1: 0.46203333333333335\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.20      0.30      2897\n",
      "           1       0.58      0.21      0.30      2674\n",
      "           2       0.39      0.29      0.33      2924\n",
      "           3       0.46      0.22      0.30      2426\n",
      "           4       0.44      0.51      0.47      6276\n",
      "           5       0.46      0.81      0.58      8249\n",
      "           6       0.62      0.30      0.41      1500\n",
      "           7       0.48      0.34      0.40      3054\n",
      "\n",
      "    accuracy                           0.46     30000\n",
      "   macro avg       0.50      0.36      0.39     30000\n",
      "weighted avg       0.48      0.46      0.43     30000\n",
      "\n",
      "Test McNemar\n",
      "statistic=95.061, p-value=0.000\n",
      "Differences between models are significant\n"
     ]
    }
   ],
   "source": [
    "!python3 compare_models.py --model_filename1 \"trained_party_classification-\" --model_filename2 \"trained_party_classification+061000\" --data_path \"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_party_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:20:27 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m14:20:28 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m14:21:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([4, 2, 5,  ..., 4, 5, 4])\u001b[0m\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m14:21:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████| 3750/3750 [18:28<00:00,  3.38it/s]\n",
      "100%|███████████████████████████████████████| 1250/1250 [02:08<00:00,  9.74it/s]\n",
      "\u001b[34m14:41:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 26.608\u001b[0m\n",
      "\u001b[34m14:41:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 23.982\u001b[0m\n",
      "\u001b[34m14:41:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4438999891281128\u001b[0m\n",
      "\u001b[32m14:41:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m14:41:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████| 3750/3750 [18:29<00:00,  3.38it/s]\n",
      "100%|███████████████████████████████████████| 1250/1250 [02:08<00:00,  9.73it/s]\n",
      "\u001b[34m15:02:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 21.223\u001b[0m\n",
      "\u001b[34m15:02:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 23.795\u001b[0m\n",
      "\u001b[34m15:02:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4650999903678894\u001b[0m\n",
      "\u001b[32m15:02:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m15:02:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████| 3750/3750 [18:30<00:00,  3.38it/s]\n",
      "100%|███████████████████████████████████████| 1250/1250 [02:08<00:00,  9.71it/s]\n",
      "\u001b[34m15:23:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 14.911\u001b[0m\n",
      "\u001b[34m15:23:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 25.576\u001b[0m\n",
      "\u001b[34m15:23:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4672999978065491\u001b[0m\n",
      "\u001b[32m15:23:16 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m15:23:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "100%|███████████████████████████████████████| 3750/3750 [18:30<00:00,  3.38it/s]\n",
      "100%|███████████████████████████████████████| 1250/1250 [02:08<00:00,  9.71it/s]\n",
      "\u001b[34m15:43:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.102\u001b[0m\n",
      "\u001b[34m15:43:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 30.388\u001b[0m\n",
      "\u001b[34m15:43:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.45605000853538513\u001b[0m\n",
      "\u001b[32m15:43:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 train_binary_bert_base.py --model_filename \"trained_party_classification\"+\"061000\" --base_model \"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2061000\" --data_path \"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_party_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m10:57:36 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m10:57:37 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m10:58:27 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([4, 2, 5,  ..., 4, 5, 4])\u001b[0m\n",
      "\u001b[34m10:58:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m11:19:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 28.224\u001b[0m\n",
      "\u001b[34m11:19:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 25.669\u001b[0m\n",
      "\u001b[34m11:19:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.40685001015663147\u001b[0m\n",
      "\u001b[32m11:19:08 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m11:19:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m11:39:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 23.514\u001b[0m\n",
      "\u001b[34m11:39:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 24.249\u001b[0m\n",
      "\u001b[34m11:39:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.44475001096725464\u001b[0m\n",
      "\u001b[32m11:39:50 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m11:39:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m12:00:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 18.707\u001b[0m\n",
      "\u001b[34m12:00:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 25.496\u001b[0m\n",
      "\u001b[34m12:00:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.43959999084472656\u001b[0m\n",
      "\u001b[32m12:00:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m12:00:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m12:21:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 13.999\u001b[0m\n",
      "\u001b[34m12:21:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 28.049\u001b[0m\n",
      "\u001b[34m12:21:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4336499869823456\u001b[0m\n",
      "\u001b[32m12:21:20 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 3750/3750 [18:30<00:00,  3.38it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.74it/s]\n",
      "100%|██████████| 3750/3750 [18:32<00:00,  3.37it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.72it/s]\n",
      "100%|██████████| 3750/3750 [18:34<00:00,  3.37it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.71it/s]\n",
      "100%|██████████| 3750/3750 [18:37<00:00,  3.36it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.71it/s]\n",
      "\n",
      "stdout: \u001b[32m12:21:24 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m12:21:24 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m12:22:18 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([4, 2, 5,  ..., 4, 5, 4])\u001b[0m\n",
      "\u001b[34m12:22:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m12:43:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 26.642\u001b[0m\n",
      "\u001b[34m12:43:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 23.956\u001b[0m\n",
      "\u001b[34m12:43:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.44279998540878296\u001b[0m\n",
      "\u001b[32m12:43:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m12:43:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m13:03:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 21.413\u001b[0m\n",
      "\u001b[34m13:03:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 23.838\u001b[0m\n",
      "\u001b[34m13:03:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4691999852657318\u001b[0m\n",
      "\u001b[32m13:03:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m13:03:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m13:24:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 15.200\u001b[0m\n",
      "\u001b[34m13:24:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 25.637\u001b[0m\n",
      "\u001b[34m13:24:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4675999879837036\u001b[0m\n",
      "\u001b[32m13:24:33 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m13:24:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m13:45:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.440\u001b[0m\n",
      "\u001b[34m13:45:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 30.248\u001b[0m\n",
      "\u001b[34m13:45:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.4556500017642975\u001b[0m\n",
      "\u001b[32m13:45:20 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 3750/3750 [18:35<00:00,  3.36it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.72it/s]\n",
      "100%|██████████| 3750/3750 [18:35<00:00,  3.36it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.71it/s]\n",
      "100%|██████████| 3750/3750 [18:34<00:00,  3.36it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.70it/s]\n",
      "100%|██████████| 3750/3750 [18:37<00:00,  3.36it/s]\n",
      "100%|██████████| 1250/1250 [02:08<00:00,  9.70it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "8\n",
      "{0: 'Centerpartiet', 1: 'Kristdemokraterna', 2: 'Liberalerna', 3: 'Miljöpartiet', 4: 'Moderaterna', 5: 'Socialdemokraterna', 6: 'Sverigedemokraterna', 7: 'Vänsterpartiet'}\n",
      "Training Loss 1: 24.575\n",
      "\n",
      "Accuracy model 1: 0.4381\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.28      0.32      2897\n",
      "           1       0.45      0.24      0.31      2674\n",
      "           2       0.40      0.27      0.33      2924\n",
      "           3       0.54      0.18      0.27      2426\n",
      "           4       0.41      0.47      0.44      6276\n",
      "           5       0.46      0.73      0.56      8249\n",
      "           6       0.56      0.31      0.40      1500\n",
      "           7       0.42      0.32      0.36      3054\n",
      "\n",
      "    accuracy                           0.44     30000\n",
      "   macro avg       0.45      0.35      0.37     30000\n",
      "weighted avg       0.44      0.44      0.42     30000\n",
      "\n",
      "Training Loss 1: 24.317\n",
      "\n",
      "Accuracy model 1: 0.4570666666666667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.18      0.27      2897\n",
      "           1       0.56      0.21      0.30      2674\n",
      "           2       0.39      0.28      0.33      2924\n",
      "           3       0.49      0.22      0.30      2426\n",
      "           4       0.44      0.51      0.47      6276\n",
      "           5       0.45      0.80      0.58      8249\n",
      "           6       0.61      0.31      0.41      1500\n",
      "           7       0.46      0.33      0.39      3054\n",
      "\n",
      "    accuracy                           0.46     30000\n",
      "   macro avg       0.50      0.35      0.38     30000\n",
      "weighted avg       0.48      0.46      0.43     30000\n",
      "\n",
      "Test McNemar\n",
      "statistic=59.339, p-value=0.000\n",
      "Differences between models are significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1875/1875 [03:12<00:00,  9.72it/s]\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1875/1875 [03:12<00:00,  9.73it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import preprocessing\n",
    "from evaluation import party_classifaction_scratch,reform_classification,reform_scratch_classfication,party_classifaction_bis\n",
    "from evaluation import regression_year\n",
    "from evaluation import party_gender_detection\n",
    "from evaluation import train_binary_bert,compare_model\n",
    "reform_scratch_classfication(\"trained_party_classification\",\"KBLab/bert-base-swedish-cased\",\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_party_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_party_test.csv\",tokenizer2=\"/home/laurinemeier/swerick/exbert_tokenizer\")\n",
    " #party_gender_detection(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\")\n",
    "#regression_year(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_date_train100.csv\")\n",
    "#reform_classification(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_bis-finetuned-imdb/checkpoint-2175500\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_test.csv\")\n",
    "#reform_scratch_classfication(\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-3511320\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_test.csv\",\"/home/laurinemeier/swerick/exbert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 compare_models.py --model_filename1 \"trained_alignment_classification_good\" --model_filename2 \"trained_alignment_party_good_118250\" --data_path \"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_alignment_test_withoutcenter.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 compare_models.py --model_filename1 \"trained_alignment_classification_good\" --model_filename2 \"trained_alignment_party_good_118250\" --data_path \"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_alignment_test_withoutcenter.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --model_filename1 \"trained_alignment_classification\"  --model_filename2 \"trained_alignment_party_good_118250\" --data_path \"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_alignment_train_withoutcenter.csv\" --learning_rate 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"comparison_results.txt\", \"a\") as file:\n",
    "        file.write(f\"{best_settings },{best_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m10:47:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m10:47:33 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m10:47:33 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1])\u001b[0m\n",
      "\u001b[34m10:47:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m10:47:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 5.012\u001b[0m\n",
      "\u001b[34m10:47:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.969\u001b[0m\n",
      "\u001b[34m10:47:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9567307829856873\u001b[0m\n",
      "\u001b[32m10:47:47 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m10:47:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m10:48:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.929\u001b[0m\n",
      "\u001b[34m10:48:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 0.825\u001b[0m\n",
      "\u001b[34m10:48:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9807692170143127\u001b[0m\n",
      "\u001b[32m10:48:00 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m10:48:02 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m10:48:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.376\u001b[0m\n",
      "\u001b[34m10:48:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.297\u001b[0m\n",
      "\u001b[34m10:48:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9759615659713745\u001b[0m\n",
      "\u001b[32m10:48:14 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m10:48:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m10:48:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.217\u001b[0m\n",
      "\u001b[34m10:48:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 1.269\u001b[0m\n",
      "\u001b[34m10:48:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9807692170143127\u001b[0m\n",
      "\u001b[32m10:48:27 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.39it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.22it/s]\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.43it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.22it/s]\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.41it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.38it/s]\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.40it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.41it/s]\n",
      "\n",
      "stdout: \u001b[32m10:48:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m10:48:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m10:48:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1])\u001b[0m\n",
      "\u001b[34m10:48:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m10:48:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 4.548\u001b[0m\n",
      "\u001b[34m10:48:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 0.796\u001b[0m\n",
      "\u001b[34m10:48:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9903846383094788\u001b[0m\n",
      "\u001b[32m10:48:43 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m10:48:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m10:48:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.809\u001b[0m\n",
      "\u001b[34m10:48:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 0.632\u001b[0m\n",
      "\u001b[34m10:48:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9903846383094788\u001b[0m\n",
      "\u001b[32m10:48:57 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m10:48:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m10:49:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.426\u001b[0m\n",
      "\u001b[34m10:49:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 0.505\u001b[0m\n",
      "\u001b[34m10:49:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9903846383094788\u001b[0m\n",
      "\u001b[32m10:49:11 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m10:49:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m10:49:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 0.366\u001b[0m\n",
      "\u001b[34m10:49:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 0.555\u001b[0m\n",
      "\u001b[34m10:49:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.9903846383094788\u001b[0m\n",
      "\u001b[32m10:49:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.36it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.25it/s]\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.40it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.09it/s]\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.40it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.23it/s]\n",
      "100%|██████████| 38/38 [00:11<00:00,  3.39it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.27it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'post', 1: 'pre'}\n",
      "Training Loss 1: 1.280\n",
      "\n",
      "Accuracy model 1: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8532\n",
      "           1       0.86      0.95      0.90      1468\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.92      0.96      0.94     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Training Loss 1: 1.062\n",
      "\n",
      "Accuracy model 1: 0.9841\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      8532\n",
      "           1       0.96      0.93      0.95      1468\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.97      0.96      0.97     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "Test McNemar\n",
      "statistic=70.751, p-value=0.000\n",
      "Differences between models are significant\n",
      "\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 625/625 [01:04<00:00,  9.75it/s]\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 625/625 [01:04<00:00,  9.76it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import regression_year,reform_scratch_classfication\n",
    "\n",
    "reform_scratch_classfication(\"trained_reform_classification\",\"KBLab/bert-base-swedish-cased\",\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_train.csv\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_reform_test.csv\",tokenizer2=\"/home/laurinemeier/swerick/exbert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIMExplainer:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = \"cpu\"\n",
    "        \n",
    "\n",
    "    def predict(self, data):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        ref = self.tokenizer.tokenize(data[0])\n",
    "        data_temp = [ref]\n",
    "        for x in data[1:]:\n",
    "            ref_temp = ref.copy()\n",
    "            new = [\"\" for _ in range(len(ref))]\n",
    "            x = self.tokenizer.tokenize(x)\n",
    "            #if \".   .\" in x:\n",
    "            #    x = [xx if xx != \".   .\" else [xxx for xxx in xx.split(\" \") if xxx] for xx in x]\n",
    "            #    x = [xx for xxx in x for xx in xxx]\n",
    "            for w in x:\n",
    "                id = ref_temp.index(w)\n",
    "                new[id] = w\n",
    "                ref_temp[id] = \"\"\n",
    "            data_temp.append(new)\n",
    "\n",
    "        data_temp = [[\"[PAD]\" if xx == \"\" else xx for xx in x] for x in data_temp]\n",
    "        data_temp = [\" \".join(x) for x in data_temp]\n",
    "\n",
    "        tokenized_nopad = [self.tokenizer.tokenize(text) for text in data_temp]\n",
    "        MAX_SEQ_LEN = max(len(x) for x in tokenized_nopad)\n",
    "        tokenized_text = [[\"[PAD]\", ] * MAX_SEQ_LEN for _ in range(len(data))]\n",
    "        for i in range(len(data)):\n",
    "            tokenized_text[i][0:len(tokenized_nopad[i])] = tokenized_nopad[i][0:MAX_SEQ_LEN]\n",
    "        indexed_tokens = [self.tokenizer.convert_tokens_to_ids(tt) for tt in tokenized_text]\n",
    "        tokens_tensor = torch.tensor(indexed_tokens)\n",
    "        tokens_tensor = tokens_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=tokens_tensor)\n",
    "            print(outputs)\n",
    "            # logits = outputs[0]\n",
    "            predictions = outputs.logits.detach().cpu().numpy()\n",
    "        final = [self.softmax(x) for x in predictions]\n",
    "        return np.array(final)\n",
    "\n",
    "    def softmax(self, it):\n",
    "        exps = np.exp(np.array(it))\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def split_string(self, string):\n",
    "        data_raw = self.tokenizer.tokenize(string)\n",
    "        data_raw = [x for x in data_raw if x not in \".,:;'\"]\n",
    "        return data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "def contains_old_spelling(text):\n",
    "    old_spelling_patterns = [r'\\bdt\\b', r'\\bfv\\b', r'\\bf\\b', r'\\bw\\b', r'\\bdh\\b', r'\\bgh\\b']\n",
    "    for pattern in old_spelling_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "model_exbert_classification = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/evaluation/trained_reform_classification054000\")\n",
    "df=pd.read_csv(\"/home/laurinemeier/swerick/evaluation/trained_reform_classification054000/missclassifed_exbert_KBBERT.csv\")\n",
    "sample_texts = df['content'].tolist()\n",
    "predictor = LIMExplainer(model_exbert_classification, exbert_tokenizer)\n",
    "label_names = [\"0\", \"1\"]\n",
    "explainer = LimeTextExplainer(class_names=label_names, split_expression=predictor.split_string)\n",
    "temp = predictor.split_string(sample_texts[0])\n",
    "xp = explainer.explain_instance(text_instance=sample_texts[0], classifier_fn=predictor.predict, num_features=len(temp))\n",
    "words = exp.as_list()\n",
    "exp.show_in_notebook(text=True, labels=(exp.available_labels()[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456\n",
      "456\n",
      "Rapport de classification pour le modèle de base:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "post-réforme       0.00      0.00      0.00       228\n",
      " pré-réforme       0.50      1.00      0.67       228\n",
      "\n",
      "    accuracy                           0.50       456\n",
      "   macro avg       0.25      0.50      0.33       456\n",
      "weighted avg       0.25      0.50      0.33       456\n",
      "\n",
      "\n",
      "Rapport de classification pour le modèle finetuné:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "post-réforme       1.00      0.36      0.53       228\n",
      " pré-réforme       0.61      1.00      0.76       228\n",
      "\n",
      "    accuracy                           0.68       456\n",
      "   macro avg       0.80      0.68      0.64       456\n",
      "weighted avg       0.80      0.68      0.64       456\n",
      "\n",
      "\n",
      "Rapport de classification pour le modèle exbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "post-réforme       1.00      0.27      0.43       228\n",
      " pré-réforme       0.58      1.00      0.73       228\n",
      "\n",
      "    accuracy                           0.64       456\n",
      "   macro avg       0.79      0.64      0.58       456\n",
      "weighted avg       0.79      0.64      0.58       456\n",
      "\n",
      "\n",
      "Rapport de classification pour le modèle scratch:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "post-réforme       0.00      0.00      0.00       228\n",
      " pré-réforme       0.50      1.00      0.67       228\n",
      "\n",
      "    accuracy                           0.50       456\n",
      "   macro avg       0.25      0.50      0.33       456\n",
      "weighted avg       0.25      0.50      0.33       456\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data = {\n",
    "    'content': [\n",
    "        'hvad', 'hvar', 'hvarje', 'hvem', 'hvilka',\n",
    "        'vad', 'var', 'varje', 'vem', 'vilka',\n",
    "        'hvardag', 'vardag', 'hvarifrån', 'varifrån', 'hvarhelst',\n",
    "        'varhelst', 'hvarken', 'varken', 'hvarmed', 'varmed',\n",
    "        'hvarom', 'varom', 'hvarpå', 'varpå', 'hvarifrån',\n",
    "        'varifrån', 'hvarvid', 'varvid', 'hvarest', 'varest',\n",
    "        'hvarjehanda', 'varjehanda', 'hvart', 'vart', 'hvartannat',\n",
    "        'vartannat', 'hvarannan', 'varannan', 'hvarvidlag', 'varvidlag',\n",
    "        'hvarenda', 'varenda', 'hvarken', 'varken', 'hvarifrån',\n",
    "        'varifrån', 'hvarpå', 'varpå', 'hvarvid', 'varvid',\n",
    "        'hvarom', 'varom', 'hvarhelst', 'varhelst', 'hvarifrån',\n",
    "        'varifrån', 'hvarmed', 'varmed', 'hvarest', 'varest',\n",
    "        'hvarjehanda', 'varjehanda', 'hvart', 'vart', 'hvartannat',\n",
    "        'vartannat', 'hvarannan', 'varannan', 'hvarvidlag', 'varvidlag',\n",
    "        'hvarenda', 'varenda', 'hvarken', 'varken', 'hvad',\n",
    "        'vad', 'hvar', 'var', 'hvarje', 'varje',\n",
    "        'hvem', 'vem', 'hvilka', 'vilka', 'hvardag',\n",
    "        'vardag', 'hvarifrån', 'varifrån', 'hvarhelst', 'varhelst',\n",
    "        'hvarken', 'varken', 'hvarmed', 'varmed', 'hvarom',\n",
    "        'varom', 'hvarpå', 'varpå', 'hvarifrån', 'varifrån',\n",
    "        'hvarvid', 'varvid', 'hvarest', 'varest', 'hvarjehanda',\n",
    "        'varjehanda', 'hvart', 'vart', 'hvartannat', 'vartannat',\n",
    "        'hvarannan', 'varannan', 'hvarvidlag', 'varvidlag', 'hvarenda',\n",
    "        'varenda', 'hvarken', 'varken', 'hvarifrån', 'varifrån',\n",
    "        'hvarpå', 'varpå', 'hvarvid', 'varvid', 'hvarom',\n",
    "        'varom', 'hvarhelst', 'varhelst', 'hvarifrån', 'varifrån',\n",
    "        'hvarmed', 'varmed', 'hvarest', 'varest', 'hvarjehanda',\n",
    "        'varjehanda', 'hvart', 'vart', 'hvartannat', 'vartannat',\n",
    "        'hvarannan', 'varannan', 'hvarvidlag', 'varvidlag', 'hvarenda',\n",
    "        'varenda', 'hvarken', 'varken', 'hvad', 'vad',\n",
    "        'hvar', 'var', 'hvarje', 'varje', 'hvem',\n",
    "        'vem', 'hvilka', 'vilka', 'hvardag', 'vardag',\n",
    "        'hvarifrån', 'varifrån', 'hvarhelst', 'varhelst', 'hvarken',\n",
    "        'varken', 'hvarmed', 'varmed', 'hvarom', 'varom',\n",
    "        'hvarpå', 'varpå', 'hvarifrån', 'varifrån', 'hvarvid',\n",
    "        'varvid', 'hvarest', 'varest', 'hvarjehanda', 'varjehanda',\n",
    "        'hvart', 'vart', 'hvartannat', 'vartannat', 'hvarannan',\n",
    "        'varannan', 'hvarvidlag', 'varvidlag', 'hvarenda', 'varenda',\n",
    "        'hvarken', 'varken', 'hvad', 'vad', 'hvar',\n",
    "        'var', 'hvarje', 'varje', 'hvem', 'vem',\n",
    "        'hvilka', 'vilka', 'hvardag', 'vardag', 'hvarifrån',\n",
    "        'varifrån', 'hvarhelst', 'varhelst', 'hvarken', 'varken',\n",
    "        'hvarmed', 'varmed', 'hvarom', 'varom', 'hvarpå',\n",
    "        'varpå', 'hvarifrån', 'varifrån', 'hvarvid', 'varvid',\n",
    "        'hvarest', 'varest', 'hvarjehanda', 'varjehanda', 'hvart',\n",
    "        'vart', 'hvartannat', 'vartannat', 'hvarannan', 'varannan',\n",
    "        'hvarvidlag', 'varvidlag', 'hvarenda', 'varenda', 'hvarken',\n",
    "        'varken', 'hvad', 'vad', 'hvar', 'var',\n",
    "        'hvarje', 'varje', 'hvem', 'vem', 'hvilka',\n",
    "        'vilka', 'hvardag', 'vardag', 'hvarifrån', 'varifrån',\n",
    "        'hvarhelst', 'varhelst', 'hvarken', 'varken', 'hvarmed',\n",
    "        'varmed', 'hvarom', 'varom', 'hvarpå', 'varpå',\n",
    "        'hvarifrån', 'varifrån', 'hvarvid', 'varvid', 'hvarest',\n",
    "        'varest', 'hvarjehanda', 'varjehanda', 'hvart', 'vart',\n",
    "        'hvartannat', 'vartannat', 'hvarannan', 'varannan', 'hvarvidlag',\n",
    "        'varvidlag', 'hvarenda', 'varenda', 'hvarken', 'varken',\n",
    "        'hvad', 'vad', 'hvar', 'var', 'hvarje',\n",
    "        'varje', 'hvem', 'vem', 'hvilka', 'vilka',\n",
    "        'hvardag', 'vardag', 'hvarifrån', 'varifrån', 'hvarhelst',\n",
    "        'varhelst', 'hvarken', 'varken', 'hvarmed', 'varmed',\n",
    "        'hvarom', 'varom', 'hvarpå', 'varpå', 'hvarifrån',\n",
    "        'varifrån', 'hvarvid', 'varvid', 'hvarest', 'varest',\n",
    "        'hvarjehanda', 'varjehanda', 'hvart', 'vart', 'hvartannat',\n",
    "        'vartannat', 'hvarannan', 'varannan', 'hvarvidlag', 'varvidlag',\n",
    "        'hvarenda', 'varenda', 'hvarken', 'varken', 'hvad',\n",
    "        'vad', 'hvar', 'var', 'hvarje', 'varje',\n",
    "        'hvem', 'vem', 'hvilka', 'vilka', 'hvardag',\n",
    "        'vardag', 'hvarifrån', 'varifrån', 'hvarhelst', 'varhelst',\n",
    "        'hvarken', 'varken', 'hvarmed', 'varmed', 'hvarom',\n",
    "        'varom', 'hvarpå', 'varpå', 'hvarifrån', 'varifrån',\n",
    "        'hvarvid', 'varvid', 'hvarest', 'varest', 'hvarjehanda',\n",
    "        'varjehanda', 'hvart', 'vart', 'hvartannat', 'vartannat',\n",
    "        'hvarannan', 'varannan', 'hvarvidlag', 'varvidlag', 'hvarenda',\n",
    "        'varenda', 'hvarken', 'varken', 'hvad', 'vad',\n",
    "        'hvar', 'var', 'hvarje', 'varje', 'hvem',\n",
    "        'vem', 'hvilka', 'vilka', 'hvardag', 'vardag',\n",
    "        'hvarifrån', 'varifrån', 'hvarhelst', 'varhelst', 'hvarken',\n",
    "        'varken', 'hvarmed', 'varmed', 'hvarom', 'varom',\n",
    "        'hvarpå', 'varpå', 'hvarifrån', 'varifrån', 'hvarvid',\n",
    "        'varvid', 'hvarest', 'varest', 'hvarjehanda', 'varjehanda',\n",
    "        'hvart', 'vart', 'hvartannat', 'vartannat', 'hvarannan',\n",
    "        'varannan', 'hvarvidlag', 'varvidlag', 'hvarenda', 'varenda',\n",
    "        'hvarken', 'varken', 'hvad', 'vad', 'hvar',\n",
    "        'var', 'hvarje', 'varje', 'hvem', 'vem',\n",
    "        'hvilka', 'vilka', 'hvardag', 'vardag', 'hvarifrån',\n",
    "        'varifrån', 'hvarhelst', 'varhelst', 'hvarken', 'varken',\n",
    "        'hvarmed', 'varmed', 'hvarom', 'varom', 'hvarpå',\n",
    "        'varpå', 'hvarifrån', 'varifrån', 'hvarvid', 'varvid',\n",
    "        'hvarest', 'varest', 'hvarjehanda', 'varjehanda', 'hvart',\n",
    "        'vart', 'hvartannat', 'vartannat', 'hvarannan', 'varannan',\n",
    "        'hvarvidlag', 'varvidlag', 'hvarenda', 'varenda', 'hvarken',\n",
    "        'varken'],\n",
    "    \n",
    "    'label': [\n",
    "        1, 1, 1, 1, 1,\n",
    "        0, 0, 0, 0, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0\n",
    "       \n",
    "        \n",
    "       ]}\n",
    "\n",
    "print(len(data['label']))\n",
    "print(len(data['content']))\n",
    "df = pd.DataFrame(data)\n",
    "model_kb_classification = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/evaluation/trained_reform_classification-\").to(device)\n",
    "model_cpt_classification = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/evaluation/trained_reform_classification061000\").to(device)\n",
    "model_eb_classification = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/evaluation/trained_reform_classification054000\").to(device)\n",
    "model_spa_classification = AutoModelForSequenceClassification.from_pretrained(\"/home/laurinemeier/swerick/evaluation/trained_reform_classification258900\").to(device)\n",
    "\n",
    "def evaluate_model(model, tokenizer, df, device):\n",
    "    inputs = tokenizer(df['content'].tolist(), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    labels = torch.tensor(df['label'].tolist()).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, axis=1)\n",
    "    report = classification_report(labels.cpu().numpy(), preds.cpu().numpy(), target_names=['post-réforme', 'pré-réforme'])\n",
    "    return report\n",
    "\n",
    "# Exemple d'évaluation (model_base et model_finetuned sont déjà chargés)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "report_base = evaluate_model(model_kb_classification, tokenizer, df, device)\n",
    "report_cpt = evaluate_model(model_cpt_classification, tokenizer, df, device)\n",
    "report_eb = evaluate_model(model_eb_classification, exbert_tokenizer, df, device)\n",
    "report_spa = evaluate_model(model_spa_classification, swerick_tokenizer, df, device)\n",
    "\n",
    "print(\"Rapport de classification pour le modèle de base:\")\n",
    "print(report_base)\n",
    "print(\"\\nRapport de classification pour le modèle finetuné:\")\n",
    "print(report_cpt)\n",
    "print(\"\\nRapport de classification pour le modèle exbert:\")\n",
    "print(report_eb)\n",
    "print(\"\\nRapport de classification pour le modèle scratch:\")\n",
    "print(report_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'protocole': ['data/1867/prot-1867--ak--0119.xml',\n",
       "  'data/1867/prot-1867--ak--0119.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml',\n",
       "  'data/1867/prot-1867--ak--0121.xml'],\n",
       " 'Note': ['För min del kan jag ej se något hinder för företagande af dessa val ännu något tidigare än Herr Talmannen föreslagit. Jag har föreställt mig, att vi så mycket som möjligt borde taga vara på den dyrbara tiden, och att nu ifrågakomna val kunde förrättas t. ex. på Tisdagen, och valen till suppleanter i Utskotten på Thorsdagen.',\n",
       "  'Då tiden snart nalkas det timslag, vid hvilket Kamrarne äro kallade att sig infinna i stadens Storkyrka, hemställer jag huruvida det kan vara skäl att nu utspinna en diskussion, som möjligen kunde blifva alltför vidlyftig att här nu hinna afslutas. Jag anser derför, att vi böra fatta oss helt kort, och att Herr Talmannens framställning bör af Kammaren godkännas.',\n",
       "  'Må det tillåtas mig, att, då begge motionerna omfatta samma ämne och den ena af dem är föredragen, nu endast tillägga att Herr Sjöbergs förslag äfven innehåller åtskilligt, som jag anser mindre lämpligt, t. ex. den föreskrift, att, ifall valen förrättas genom elektorer, desses antal skall vara en femtedel af närvarande ledamöter, då beslutet fattas. Härom innehåller grundlagen alldeles icke något stadgande, och såsom jag förut anmärkt böra vi använda yttersta försigtighet, då vi antaga en arbetsordning, att deruti icke något må inflyta, som icke eger stöd i gällande grundlag.',\n",
       "  'Jag kan icke annat än instämma med Herr \"Talmannen i afseende på hans åsigt om föredragningen, ty sannt är att Herr Sjöbergs motion ännu icke är föredragen, och derför vill jag ock endast vända mina anmärkningar mot det förslag tillarbetsordning, som den högt aktade representanten för Malmö stad nyss uppläst. Deruti yrkar han till en början, så vidt jag vid uppläsningen kunde fatta, att protokollen skola i möjligaste korthet upptaga, blånd annat, namnen på de ledamöter, som i öfverläggningen deltagit och i hvad syfte desse yttrat sig. Jag, för min del, önskar dock bibehållandet af det gamla sättet att föra protokollet, med intagande af fullständiga yttranden, ty, ehuru jag har mycken aktning för de värde herrarne här på läktaren midt emot mig, så tror jag ändock att ej så få exempel finnas derpå, att de någon gång grundligt misstagit sig, då de icke lyckats att alltid uppfatta en talares ord och mening. Om sådant händer vid protokollet, så hjelpes detta lätt vid justeringen. Vidare föreslår motionären att skriftliga anföranden, med undantag af motioner, ej må i Kammaren uppläsas. Jag vågar dock tro att äfven den skickligaste talare mången gång kan behöfva det skrifna ordet till hjelp, då sifferuppgifter, m. m. dylikt, förekomma hvilket man med minnet icke förmår någon längre tid qvarhålla. Ytterligare hemställer motionären att i hvarje fråga, som utgör föremål stör särskild proposition, ingen talare må yttra sig mer än tvänne gånger. Detta förefaller mig såsom ett betänkligt ingrepp uti den, hvarje riksdagsman tillförsäkrade fria yttranderätten, och ehuru man genom ett obegränsadt bruk af densamma kan, såsom i det Stånd jag fordom tillhörde, få höra samma sak mångfaldiga gånger sägas om igen med föga förändring i ordalagen, så utsätter jag mig heldre för dessa, vanligen kallade omtuggningar, än jag äfventyrar det minsta band på en hvars fria yttranderätt. Vidare föreslås att, vid val, som inom Kammaren verkställas, valsedlarne skola inför Talmannen afgifvas, men att deras öppnande och rösternas hopräknande skall verkställas i något till sessionssalen gränsande rum, allt under iakttagande af vissa härför gifna föreskrifter. Visserligen begagnades detta sätt att tillvägagå ej så sällan i det fordna Bonde- Ståndet, och användes der utan all olägenhet, men jag är ej rätt säker, om jag vågar, under så väsendtligen förändrade omständigheter, tillstyrka användandet af detsamma härstädes. Motionären har äfvenledes, såsom jag tyckte mig märka, rigtat en beskyllning mot de fordna Utskottens kanslier, gående derpå ut, att personalen inom dessa, genom alltför vidlyftiga och omständliga betänkanden, vållat långsamheten af våra riksdagar och af arbetena derstädes. Jag vågar dock tillbakakasta en dylik anspel-',\n",
       "  'ning, och tager derpå till bevis förhållandet vid sednaste riksdag, då arbetet, af allmänt kända och erkända orsaker, till en början visserligen gick ganska långsamt, hvartill Riksdagens kanslipersonal dock ingalunda hade någon skuld, men derefter påskyndades arbetet så, att intet rimligt skäl till klagan förefinnes, och med den påföljd att riksdagen ock kunde afslutas långt före den tid, man annars varit van att härför beräkna. Hvad vidare beträffar motionärens yrkande, om Utskotts-betänkandenas uppställande på ett mer kortfattadt och knapphändigt sätt, än hittills varit brukligt, så vill jag ej bestrida att detta sätt må medföra vissa fördelar för dem, som ega högre bildning och vidsträckta kunskaper, men för dem, som lika med mig befinna sig på en annan och lägre ståndpuänkt i detta hänseende, för oss, som behötva en fullständig utredning af ärendena, för att erhålla en noggrann och på rigtiga grunder fotad k ännedom om desamma, för oss, säger jag, skulle en dylik för? ändring blifva ganska ovälkommen. Vidare hemställes uti motionen, att förslag till ny, samt till ändring af gällande lag eller förordning, som af någon Kammarens ledamot väckes, bör \"at motionären fullständigt formuleras, men då vi ofta varit vittnen till huruledes författningar, uppsatta af en samling så lagkunnige män, som väl Lag-Utskottet bör anses kunna räkna inom antalet at sina ledamöter, ofta nog slopas och kasseras utaf Högsta Domstolen, huru vill man då rimligtvis begära att en stackars motionär ensam skall kunna förmå uppställa sima förslag 1 fullt författningsenlig form? — Efter atslutandet af dessa anmärkningar vill jag endast tillägg ja, att så få reglementariska föreskrifter, som möjligt, äro de bästa, och, anslutande mig till en viss känd personlighet, som yttrade de allbekanta orden: \"ju simplare, ju enklare\", anser jag att ju enklare arbetsordningen blir, desto bättre skall den uppfylla sitt ändamål.',\n",
       "  'Jag vill vid detta tillfälle inskränka mitt yrkande derhän, att ett tillfälligt Utskott må af Kammaren tillsättas, till hvars handläggning dessa båda motioner må hänvisas. Den sednast väckta motionen erhåller här, enligt min öfvertygelse, ganska ringa sympati, ty dertill är densamma alltför mycket genomgripande och alltför litet ställd i öfverensstämmelse med grundlagen, och jag må för egen del bekänna det jag, i likhet med min ärade vän Sven Nilsson, tror, att ju färre de reglementariska föreskrifterna blifva, desto bättre och friare bedrifves riksdagsarbetet. Med stigande erfarenhet af detta arbetes beskaffenhet utbildar sig så småningom sjelfmant en arbetsordning, som troligen kommer att erbjuda långt bättre föreskrifter än dem vi nu på förhand kunna utarbeta. Emellertid hemställer jag, att det blifvande Utskottet måtte komma att bestå af sju personer, och att de båda väckta motionerna måtte till behandling af detta Utskott hänvisas.',\n",
       "  'För min del har jag tyckt att båda de väckta förslagen till reglementariska föreskrifter för Kammarens arbeten äro alltför vidlyftiga, och att särdeles det sista, ifall det konseqvent genomföres, skulle komma att orsaka högst betydliga och ytterst betänkliga inskränkningar uti den en hvar af Riksdagens medlemmar tillförsäkrade fria yttranderätten, och då det är af vigt att härvid gå till väga med mycken betänksamhet, på det vi icke må tillåta oss något, som är mot grundlagen stridande, yrkar jag förslagets bordläggande, på det vi må blifva i tillfälle att deraf taga närmare kännedom, samt att det derefter måtte behandlas på samma gång med Herr Sjöbergs förslag.',\n",
       "  'Anseende en förberedande diskussion ej vara Oogagnelig, då mer än en återremiss möjligen kan derigenom förekommas, vill Jag här uttala min mening vara, det inga inskränkningar i yttranderätten må af Kammaren medgifvas. Mot långa och tråkiga anföranden, vare sig skriftliga eller muntliga, har Kammaren mer än en utväg att visa sin otålighet, utan att något behöfver deront stadgas.',\n",
       "  'Redan vid åhörandet af uppläsningen utaf Herr Sjöbergs ifrågavarande motion yttrade jag mitt förmenande, att den lemnade rum för vigtiga anmärkningar med afseende å både hvad deri blifvit föreslaget och hvad deri saknades. Sedan motionen blifvit tryckt och Jag derigenom fått tillfälle närmare granska densamma, beder jag nu att få framställa mina anmärkningar, de der tillkommit endast i afsigt att i min ringa mån söka bidraga till åstadkommande af en så god arbets-',\n",
       "  'ordning som möjligt, och hvilka Kammaren derföre anmodas att med tålamod åhöra.'],\n",
       " 'id': ['i-PyY1Vo1W6WaajphhpnKHN8',\n",
       "  'i-ApHmSnfxK7X5tAzqwJM3Ve',\n",
       "  'i-9XuHqsad9Y4ZuW7LJvmnDf',\n",
       "  'i-BqTEGFHCkiyw2BHdEDbheb',\n",
       "  'i-BqTEGFHCkiyw2BHdEDbheb',\n",
       "  'i-TF2nMgsgs9UxhzkCePtBJo',\n",
       "  'i-HvB43EUnLaSUKmFYYm2t2U',\n",
       "  'i-YFzzFKh2NjM54cFjad2qWQ',\n",
       "  'i-3DKytmgpnLCJYb8hSjHGUm',\n",
       "  'i-3DKytmgpnLCJYb8hSjHGUm'],\n",
       " 'party': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  'Gamla lantmannapartiet',\n",
       "  'ministeriella partiet',\n",
       "  'ministeriella partiet',\n",
       "  'ministeriella partiet',\n",
       "  'ministeriella partiet'],\n",
       " 'gender': ['man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man',\n",
       "  'man']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party_dataset[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:04:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:04:36 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:04:36 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0])\u001b[0m\n",
      "\u001b[34m16:04:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:04:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.332\u001b[0m\n",
      "\u001b[34m16:04:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.389\u001b[0m\n",
      "\u001b[34m16:04:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:39 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:04:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:04:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.917\u001b[0m\n",
      "\u001b[34m16:04:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 11.869\u001b[0m\n",
      "\u001b[34m16:04:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:41 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:04:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:04:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.503\u001b[0m\n",
      "\u001b[34m16:04:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 11.976\u001b[0m\n",
      "\u001b[34m16:04:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:42 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:04:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:04:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.474\u001b[0m\n",
      "\u001b[34m16:04:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 11.859\u001b[0m\n",
      "\u001b[34m16:04:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:44 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.83it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.94it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.44it/s]\n",
      "\n",
      "stdout: \u001b[32m16:04:46 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:04:46 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:04:46 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0])\u001b[0m\n",
      "\u001b[34m16:04:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:04:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.038\u001b[0m\n",
      "\u001b[34m16:04:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.730\u001b[0m\n",
      "\u001b[34m16:04:49 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:49 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:04:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:04:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.934\u001b[0m\n",
      "\u001b[34m16:04:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.783\u001b[0m\n",
      "\u001b[34m16:04:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:51 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:04:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:04:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.617\u001b[0m\n",
      "\u001b[34m16:04:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 13.181\u001b[0m\n",
      "\u001b[34m16:04:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:53 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:04:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:04:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.419\u001b[0m\n",
      "\u001b[34m16:04:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.807\u001b[0m\n",
      "\u001b[34m16:04:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.65625\u001b[0m\n",
      "\u001b[32m16:04:54 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.67it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.47it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.96it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 9.109\n",
      "\n",
      "Accuracy model 1: 0.7262\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 10.204\n",
      "\n",
      "Accuracy model 1: 0.7254\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84      3645\n",
      "           1       0.09      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.41      0.50      0.42      5000\n",
      "weighted avg       0.56      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=0.250, p-value=0.617\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:31<00:00,  9.82it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:31<00:00,  9.81it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:06:10 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:06:10 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:06:10 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:06:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:06:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.165\u001b[0m\n",
      "\u001b[34m16:06:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.029\u001b[0m\n",
      "\u001b[34m16:06:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:06:13 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:06:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:06:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.041\u001b[0m\n",
      "\u001b[34m16:06:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.899\u001b[0m\n",
      "\u001b[34m16:06:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:16 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:06:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:06:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.826\u001b[0m\n",
      "\u001b[34m16:06:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.858\u001b[0m\n",
      "\u001b[34m16:06:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:19 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:06:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:06:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.378\u001b[0m\n",
      "\u001b[34m16:06:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.766\u001b[0m\n",
      "\u001b[34m16:06:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:21 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.88it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.54it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.54it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.98it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.95it/s]\n",
      "\n",
      "stdout: \u001b[32m16:06:24 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:06:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:06:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:06:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.005\u001b[0m\n",
      "\u001b[34m16:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.660\u001b[0m\n",
      "\u001b[34m16:06:27 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:27 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:06:29 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.560\u001b[0m\n",
      "\u001b[34m16:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.697\u001b[0m\n",
      "\u001b[34m16:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:30 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:06:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:06:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.648\u001b[0m\n",
      "\u001b[34m16:06:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.514\u001b[0m\n",
      "\u001b[34m16:06:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:06:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.215\u001b[0m\n",
      "\u001b[34m16:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.425\u001b[0m\n",
      "\u001b[34m16:06:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.75\u001b[0m\n",
      "\u001b[32m16:06:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.98it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.29it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.12it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 9.238\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 9.865\n",
      "\n",
      "Accuracy model 1: 0.7284\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=1.333, p-value=0.248\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.77it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.77it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:07:52 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:07:52 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:07:52 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:07:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:07:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.430\u001b[0m\n",
      "\u001b[34m16:07:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.337\u001b[0m\n",
      "\u001b[34m16:07:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:07:55 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:07:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:07:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.318\u001b[0m\n",
      "\u001b[34m16:07:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.259\u001b[0m\n",
      "\u001b[34m16:07:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:07:57 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:07:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:08:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.653\u001b[0m\n",
      "\u001b[34m16:08:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.896\u001b[0m\n",
      "\u001b[34m16:08:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:08:00 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:08:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:08:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.828\u001b[0m\n",
      "\u001b[34m16:08:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.733\u001b[0m\n",
      "\u001b[34m16:08:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:08:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.57it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.53it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.58it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.76it/s]\n",
      "\n",
      "stdout: \u001b[32m16:08:05 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:08:06 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:08:06 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:08:07 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:08:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.372\u001b[0m\n",
      "\u001b[34m16:08:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.429\u001b[0m\n",
      "\u001b[34m16:08:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:08:09 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:08:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:08:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.871\u001b[0m\n",
      "\u001b[34m16:08:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.448\u001b[0m\n",
      "\u001b[34m16:08:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:08:12 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:08:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:08:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.968\u001b[0m\n",
      "\u001b[34m16:08:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.433\u001b[0m\n",
      "\u001b[34m16:08:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:08:13 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:08:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:08:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.029\u001b[0m\n",
      "\u001b[34m16:08:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.243\u001b[0m\n",
      "\u001b[34m16:08:14 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:08:14 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.28it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.704\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.50      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.61      0.50      0.42      5000\n",
      "weighted avg       0.67      0.73      0.62      5000\n",
      "\n",
      "Training Loss 1: 10.836\n",
      "\n",
      "Accuracy model 1: 0.7284\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.20      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.46      0.50      0.42      5000\n",
      "weighted avg       0.59      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=0.571, p-value=0.450\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.73it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:31<00:00,  9.78it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:09:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:09:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:09:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:09:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:09:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 10.796\u001b[0m\n",
      "\u001b[34m16:09:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.307\u001b[0m\n",
      "\u001b[34m16:09:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:09:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:09:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.808\u001b[0m\n",
      "\u001b[34m16:09:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 11.030\u001b[0m\n",
      "\u001b[34m16:09:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:38 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:09:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:09:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.998\u001b[0m\n",
      "\u001b[34m16:09:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.431\u001b[0m\n",
      "\u001b[34m16:09:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:39 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:09:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:09:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.580\u001b[0m\n",
      "\u001b[34m16:09:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.007\u001b[0m\n",
      "\u001b[34m16:09:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:41 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "\n",
      "stdout: \u001b[32m16:09:44 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:09:44 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:09:44 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:09:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:09:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.039\u001b[0m\n",
      "\u001b[34m16:09:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.877\u001b[0m\n",
      "\u001b[34m16:09:47 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:47 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:09:48 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:09:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.262\u001b[0m\n",
      "\u001b[34m16:09:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.992\u001b[0m\n",
      "\u001b[34m16:09:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:50 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:09:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:09:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.750\u001b[0m\n",
      "\u001b[34m16:09:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.893\u001b[0m\n",
      "\u001b[34m16:09:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:51 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:09:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:09:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.245\u001b[0m\n",
      "\u001b[34m16:09:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.015\u001b[0m\n",
      "\u001b[34m16:09:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.6875\u001b[0m\n",
      "\u001b[32m16:09:53 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.48it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.90it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 9.157\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 10.542\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.76it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.75it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/laurinemeier/swerick/evaluation/compare_models.py\", line 188, in <module>\n",
      "    main(args)\n",
      "  File \"/home/laurinemeier/swerick/evaluation/compare_models.py\", line 162, in main\n",
      "    x = (abs(misclassified_by_1_not_by_2-misclassified_by_2_not_by_1)-1)**2/(misclassified_by_1_not_by_2+misclassified_by_2_not_by_1)\n",
      "        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:11:10 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:11:11 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:11:11 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1])\u001b[0m\n",
      "\u001b[34m16:11:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:11:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.323\u001b[0m\n",
      "\u001b[34m16:11:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.240\u001b[0m\n",
      "\u001b[34m16:11:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:13 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:11:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:11:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.101\u001b[0m\n",
      "\u001b[34m16:11:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.827\u001b[0m\n",
      "\u001b[34m16:11:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:16 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:11:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:11:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.217\u001b[0m\n",
      "\u001b[34m16:11:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.777\u001b[0m\n",
      "\u001b[34m16:11:19 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:19 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:11:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:11:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.600\u001b[0m\n",
      "\u001b[34m16:11:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.710\u001b[0m\n",
      "\u001b[34m16:11:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:21 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.26it/s]\n",
      "\n",
      "stdout: \u001b[32m16:11:24 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:11:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:11:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1])\u001b[0m\n",
      "\u001b[34m16:11:26 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:11:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.039\u001b[0m\n",
      "\u001b[34m16:11:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.608\u001b[0m\n",
      "\u001b[34m16:11:28 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:28 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:11:29 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:11:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.488\u001b[0m\n",
      "\u001b[34m16:11:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.465\u001b[0m\n",
      "\u001b[34m16:11:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:30 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:11:31 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:11:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.987\u001b[0m\n",
      "\u001b[34m16:11:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.441\u001b[0m\n",
      "\u001b[34m16:11:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:33 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:11:34 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:11:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.616\u001b[0m\n",
      "\u001b[34m16:11:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.414\u001b[0m\n",
      "\u001b[34m16:11:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:11:36 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.62it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.988\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 10.233\n",
      "\n",
      "Accuracy model 1: 0.7282\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=2.250, p-value=0.134\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.75it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.78it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:12:53 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:12:53 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:12:54 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1])\u001b[0m\n",
      "\u001b[34m16:12:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:12:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.065\u001b[0m\n",
      "\u001b[34m16:12:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.228\u001b[0m\n",
      "\u001b[34m16:12:56 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:12:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:12:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:12:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.883\u001b[0m\n",
      "\u001b[34m16:12:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.653\u001b[0m\n",
      "\u001b[34m16:12:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:12:59 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:13:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:13:02 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.074\u001b[0m\n",
      "\u001b[34m16:13:02 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.652\u001b[0m\n",
      "\u001b[34m16:13:02 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:13:02 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:13:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:13:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.247\u001b[0m\n",
      "\u001b[34m16:13:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.721\u001b[0m\n",
      "\u001b[34m16:13:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:13:04 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.12it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "\n",
      "stdout: \u001b[32m16:13:06 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:13:07 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:13:07 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1])\u001b[0m\n",
      "\u001b[34m16:13:08 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:13:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.672\u001b[0m\n",
      "\u001b[34m16:13:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.150\u001b[0m\n",
      "\u001b[34m16:13:09 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:13:09 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:13:11 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:13:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.820\u001b[0m\n",
      "\u001b[34m16:13:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.503\u001b[0m\n",
      "\u001b[34m16:13:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:13:12 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:13:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:13:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.012\u001b[0m\n",
      "\u001b[34m16:13:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.316\u001b[0m\n",
      "\u001b[34m16:13:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:13:15 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:13:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:13:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.293\u001b[0m\n",
      "\u001b[34m16:13:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.292\u001b[0m\n",
      "\u001b[34m16:13:17 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:13:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.17it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.11it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.292\n",
      "\n",
      "Accuracy model 1: 0.7306\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.63      0.01      0.03      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.68      0.51      0.44      5000\n",
      "weighted avg       0.70      0.73      0.62      5000\n",
      "\n",
      "Training Loss 1: 8.888\n",
      "\n",
      "Accuracy model 1: 0.7242\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.98      0.84      3645\n",
      "           1       0.42      0.05      0.08      1355\n",
      "\n",
      "    accuracy                           0.72      5000\n",
      "   macro avg       0.58      0.51      0.46      5000\n",
      "weighted avg       0.65      0.72      0.63      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=5.460, p-value=0.019\n",
      "Differences between models are significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.75it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.74it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:14:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:14:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:14:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0])\u001b[0m\n",
      "\u001b[34m16:14:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:14:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.060\u001b[0m\n",
      "\u001b[34m16:14:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.520\u001b[0m\n",
      "\u001b[34m16:14:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:38 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:14:39 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:14:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.265\u001b[0m\n",
      "\u001b[34m16:14:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.647\u001b[0m\n",
      "\u001b[34m16:14:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:41 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:14:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:14:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.688\u001b[0m\n",
      "\u001b[34m16:14:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.616\u001b[0m\n",
      "\u001b[34m16:14:43 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:43 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:14:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:14:46 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.108\u001b[0m\n",
      "\u001b[34m16:14:46 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.664\u001b[0m\n",
      "\u001b[34m16:14:46 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:46 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.09it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.58it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.57it/s]\n",
      "\n",
      "stdout: \u001b[32m16:14:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:14:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:14:49 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0])\u001b[0m\n",
      "\u001b[34m16:14:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:14:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 12.055\u001b[0m\n",
      "\u001b[34m16:14:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.967\u001b[0m\n",
      "\u001b[34m16:14:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:51 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:14:52 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:14:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.473\u001b[0m\n",
      "\u001b[34m16:14:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.578\u001b[0m\n",
      "\u001b[34m16:14:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:54 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:14:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:14:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.963\u001b[0m\n",
      "\u001b[34m16:14:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.530\u001b[0m\n",
      "\u001b[34m16:14:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:57 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:14:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:14:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 6.490\u001b[0m\n",
      "\u001b[34m16:14:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 6.508\u001b[0m\n",
      "\u001b[34m16:14:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.875\u001b[0m\n",
      "\u001b[32m16:14:59 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.993\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 8.591\n",
      "\n",
      "Accuracy model 1: 0.7292\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.60      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.66      0.50      0.42      5000\n",
      "weighted avg       0.69      0.73      0.62      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=0.000, p-value=1.000\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.74it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.75it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:16:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:16:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:16:17 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:16:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:16:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.752\u001b[0m\n",
      "\u001b[34m16:16:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.988\u001b[0m\n",
      "\u001b[34m16:16:20 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:20 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:16:21 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:16:23 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.056\u001b[0m\n",
      "\u001b[34m16:16:23 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.093\u001b[0m\n",
      "\u001b[34m16:16:23 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:23 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:16:23 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:16:24 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.447\u001b[0m\n",
      "\u001b[34m16:16:24 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.067\u001b[0m\n",
      "\u001b[34m16:16:24 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:24 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:16:24 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:16:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.215\u001b[0m\n",
      "\u001b[34m16:16:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.981\u001b[0m\n",
      "\u001b[34m16:16:25 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:25 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.57it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.15it/s]\n",
      "\n",
      "stdout: \u001b[32m16:16:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:16:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:16:29 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:16:30 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:16:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.811\u001b[0m\n",
      "\u001b[34m16:16:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.496\u001b[0m\n",
      "\u001b[34m16:16:32 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:32 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:16:33 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:16:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.122\u001b[0m\n",
      "\u001b[34m16:16:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.776\u001b[0m\n",
      "\u001b[34m16:16:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:35 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:16:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:16:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.901\u001b[0m\n",
      "\u001b[34m16:16:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.731\u001b[0m\n",
      "\u001b[34m16:16:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:36 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:16:36 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:16:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.267\u001b[0m\n",
      "\u001b[34m16:16:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 10.225\u001b[0m\n",
      "\u001b[34m16:16:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.71875\u001b[0m\n",
      "\u001b[32m16:16:38 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.33it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.838\n",
      "\n",
      "Accuracy model 1: 0.7288\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 9.708\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=0.000, p-value=1.000\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.73it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.76it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:17:55 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:17:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:17:56 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:17:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:17:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 10.214\u001b[0m\n",
      "\u001b[34m16:17:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.720\u001b[0m\n",
      "\u001b[34m16:17:59 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.46875\u001b[0m\n",
      "\u001b[32m16:17:59 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:18:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:18:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.177\u001b[0m\n",
      "\u001b[34m16:18:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 13.683\u001b[0m\n",
      "\u001b[34m16:18:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5\u001b[0m\n",
      "\u001b[32m16:18:01 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:18:01 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:18:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.676\u001b[0m\n",
      "\u001b[34m16:18:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.162\u001b[0m\n",
      "\u001b[34m16:18:03 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.625\u001b[0m\n",
      "\u001b[32m16:18:03 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:18:04 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:18:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.074\u001b[0m\n",
      "\u001b[34m16:18:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.012\u001b[0m\n",
      "\u001b[34m16:18:06 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5\u001b[0m\n",
      "\u001b[32m16:18:06 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.83it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.66it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "\n",
      "stdout: \u001b[32m16:18:08 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:18:09 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:18:09 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:18:10 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:18:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.629\u001b[0m\n",
      "\u001b[34m16:18:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 11.799\u001b[0m\n",
      "\u001b[34m16:18:12 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.34375\u001b[0m\n",
      "\u001b[32m16:18:12 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:18:13 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:18:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.658\u001b[0m\n",
      "\u001b[34m16:18:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.683\u001b[0m\n",
      "\u001b[34m16:18:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.46875\u001b[0m\n",
      "\u001b[32m16:18:15 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:18:15 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:18:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.600\u001b[0m\n",
      "\u001b[34m16:18:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.698\u001b[0m\n",
      "\u001b[34m16:18:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5625\u001b[0m\n",
      "\u001b[32m16:18:16 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\u001b[34m16:18:16 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:18:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.857\u001b[0m\n",
      "\u001b[34m16:18:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 12.707\u001b[0m\n",
      "\u001b[34m16:18:18 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.5625\u001b[0m\n",
      "\u001b[32m16:18:18 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.96it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.524\n",
      "\n",
      "Accuracy model 1: 0.7182\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82      3645\n",
      "           1       0.47      0.29      0.36      1355\n",
      "\n",
      "    accuracy                           0.72      5000\n",
      "   macro avg       0.62      0.58      0.59      5000\n",
      "weighted avg       0.69      0.72      0.69      5000\n",
      "\n",
      "Training Loss 1: 10.006\n",
      "\n",
      "Accuracy model 1: 0.7172\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.97      0.83      3645\n",
      "           1       0.33      0.04      0.07      1355\n",
      "\n",
      "    accuracy                           0.72      5000\n",
      "   macro avg       0.53      0.50      0.45      5000\n",
      "weighted avg       0.62      0.72      0.63      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=0.017, p-value=0.896\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.72it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.73it/s]\n",
      "\n",
      "Party alignement classification\n",
      "training\n",
      "stdout: \u001b[32m16:19:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:19:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:19:34 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:19:35 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:19:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.241\u001b[0m\n",
      "\u001b[34m16:19:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 9.048\u001b[0m\n",
      "\u001b[34m16:19:37 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:37 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:38 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:19:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.044\u001b[0m\n",
      "\u001b[34m16:19:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.848\u001b[0m\n",
      "\u001b[34m16:19:40 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:40 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:41 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:19:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.344\u001b[0m\n",
      "\u001b[34m16:19:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.695\u001b[0m\n",
      "\u001b[34m16:19:42 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:42 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:44 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:19:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.034\u001b[0m\n",
      "\u001b[34m16:19:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.601\u001b[0m\n",
      "\u001b[34m16:19:45 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:45 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KBLab/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.23it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "\n",
      "stdout: \u001b[32m16:19:48 [INFO] \u001b[37m(train-bert)\u001b[0m: Load and save tokenizer...\u001b[0m\n",
      "\u001b[32m16:19:49 [INFO] \u001b[37m(train-bert)\u001b[0m: Preprocess datasets...\u001b[0m\n",
      "\u001b[32m16:19:49 [INFO] \u001b[37m(train-bert)\u001b[0m: Labels: tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\u001b[0m\n",
      "\u001b[34m16:19:50 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 0 starts!\u001b[0m\n",
      "\u001b[34m16:19:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 11.153\u001b[0m\n",
      "\u001b[34m16:19:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 8.013\u001b[0m\n",
      "\u001b[34m16:19:51 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:51 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:53 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 1 starts!\u001b[0m\n",
      "\u001b[34m16:19:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 9.342\u001b[0m\n",
      "\u001b[34m16:19:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.091\u001b[0m\n",
      "\u001b[34m16:19:54 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:54 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:55 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 2 starts!\u001b[0m\n",
      "\u001b[34m16:19:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 8.631\u001b[0m\n",
      "\u001b[34m16:19:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.039\u001b[0m\n",
      "\u001b[34m16:19:57 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:19:57 [INFO] \u001b[37m(train-bert)\u001b[0m: Best validation loss so far\u001b[0m\n",
      "\u001b[34m16:19:58 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Epoch 3 starts!\u001b[0m\n",
      "\u001b[34m16:20:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Training Loss: 7.577\u001b[0m\n",
      "\u001b[34m16:20:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation Loss: 7.094\u001b[0m\n",
      "\u001b[34m16:20:00 [TRAIN] \u001b[37m(train-bert)\u001b[0m: Validation accuracy: 0.78125\u001b[0m\n",
      "\u001b[32m16:20:00 [INFO] \u001b[37m(train-bert)\u001b[0m: Not the best validation loss so far\u001b[0m\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.53it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.15it/s]\n",
      "\n",
      "comparing\n",
      "stdout: None\n",
      "2\n",
      "{0: 'man', 1: 'woman'}\n",
      "Training Loss 1: 8.675\n",
      "\n",
      "Accuracy model 1: 0.7288\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Training Loss 1: 8.719\n",
      "\n",
      "Accuracy model 1: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84      3645\n",
      "           1       0.00      0.00      0.00      1355\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.36      0.50      0.42      5000\n",
      "weighted avg       0.53      0.73      0.61      5000\n",
      "\n",
      "Test McNemar\n",
      "statistic=0.000, p-value=1.000\n",
      "Differences between models are not significant\n",
      "\n",
      "stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.74it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [00:32<00:00,  9.75it/s]\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/laurinemeier/anaconda3/envs/swerick/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import party_gender_detection,reform_classification,regression_year,party_classifaction_scratch,reform_scratch_classfication\n",
    "#party_dataset[\"train\"]=load_dataset(\"csv\",\"swerick_subsetdata_date_train.csv\")\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "        train_set = subset_random(party_dataset[\"train\"],100)\n",
    "        df=train_set.to_pandas()\n",
    "        df=df.rename(columns={\"Note\":\"content\", \"gender\" : \"tag\"})\n",
    "\n",
    "        df.to_csv(\"swerick_subsetdata_gender_train100.csv\")\n",
    "        reform_scratch_classfication(\"trained_gender_classification\",\"KBLab/bert-base-swedish-cased\",\"/home/laurinemeier/swerick/exbert-finetuned-imdb/checkpoint-6054000\",\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_train100.csv\",data_path_test=\"/home/laurinemeier/swerick/evaluation/swerick_subsetdata_gender_test.csv\",tokenizer2=\"/home/laurinemeier/swerick/exbert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import party_gender_detection\n",
    "with open(\"comparison_results_gender.txt\", \"w\") :\n",
    "        pass\n",
    "for i in [4,6]:\n",
    "    train_set = subset_random(party_dataset[\"train\"],5000)\n",
    "    df=train_set.to_pandas()\n",
    "    df=df.rename(columns={\"Note\":\"content\", \"gender\" : \"tag\"})\n",
    "    print()\n",
    "    df.to_csv(\"swerick_subsetdata_gender_train.csv\")\n",
    "    party_gender_detection(\"/home/laurinemeier/swerick/finetuning_hugging_whitespace_lr-finetuned-imdb/checkpoint-858750\",\"swerick_subsetdata_gender_train.csv\")\n",
    "\n",
    "\n",
    "losses_model2, r2_model2 = [], []\n",
    "losses_model1, r2_model1 = [], []\n",
    "with open(\"comparison_results.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        loss_model1, r2_model1_val, loss_model2, r2_model2_val = line.strip().split(',')\n",
    "        losses_model1.append(float(loss_model1))\n",
    "        r2_model1.append(float(r2_model1_val))\n",
    "\n",
    "        losses_model2.append(float(loss_model2))\n",
    "        r2_model2.append(float(r2_model2_val))\n",
    "\n",
    "print(sum(losses_model1)/len(losses_model1))\n",
    "print(sum(losses_model2)/len(losses_model2))\n",
    "print(sum(r2_model1)/len(r2_model1))\n",
    "print(sum(r2_model2)/len(r2_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Perplexity and performance on regression task\n",
    "train_set = subset_random(party_dataset[\"train\"],1000)\n",
    "df=train_set.to_pandas()\n",
    "df=df.rename(columns={\"Note\":\"content\", \"gender\" : \"tag\"})\n",
    "df.to_csv(\"swerick_subsetdata_gender_train.csv\")\n",
    "from evaluation import regression_year, party_gender_detection\n",
    "with open(\"comparison_results_gender.txt\", \"w\") :\n",
    "        pass\n",
    "checkpoint_directory = \"/home/laurinemeier/swerick/finetuning/finetuning_hugging_whitespace-finetuned-imdb\"\n",
    "checkpoint_files = os.listdir(checkpoint_directory)\n",
    "checkpoint_files.sort(key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)))\n",
    "selected_checkpoints = [checkpoint_files[i] for i in range(0, len(checkpoint_files), 10)]\n",
    "for name in selected_checkpoints:\n",
    "        print(checkpoint_directory + '/' +name)\n",
    "        party_gender_detection(checkpoint_directory + '/' + name,\"swerick_subsetdata_gender_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity=[3.03,2.89,2.807,2.748,2.708,2.674,2.638,2.615,2.591,2.567]\n",
    "accuracy =[0.7308,0.716,0.7248,0.7012,0.7044,0.7242,0.7104,0.7096,0.7258,0.7064]\n",
    "f1=[0.71,0.68,0.72,0.71,0.71,0.69,0.71,0.71,0.72,0.71]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(perplexity,accuracy, marker='o')\n",
    "#plt.plot(perplexity, r2, label='r2', marker='o')\n",
    "plt.title(' Accuracy over perplexity')\n",
    "plt.xlabel('perplexity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def r2_score(outputs, labels):\n",
    "    predictions = outputs.logits.squeeze()\n",
    "    labels_mean = torch.mean(labels.float())\n",
    "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
    "    ss_res = torch.sum((labels - predictions) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for ix, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['content'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['tag'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss, valid_r2 = 0.0, []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].float().to(device)\n",
    "        output = model(input_ids,token_type_ids=None,attention_mask=input_mask,labels=labels)\n",
    "        loss +=output.loss.item()\n",
    "        r2 = r2_score(output, labels)\n",
    "        valid_r2.append(r2.item())\n",
    "        \n",
    "    r2 = torch.mean(torch.tensor(valid_r2))\n",
    "    return loss, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd \n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"trained/regression_date\",\n",
    "    num_labels=1,\n",
    ").to(device)\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"trained/regression_date_hugging_face\",\n",
    "    num_labels=1).to(device)\n",
    "\n",
    "df = pd.read_csv(\"swerick_subsetdata_date_test.csv\")\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "input_ids, attention_masks, labels = encode(df, tokenizer)\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "test_loader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=16,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "loss1,r2=evaluate(model1,test_loader)\n",
    "loss2,r22=evaluate(model2,test_loader)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"\\nLoss model 1:\", loss1 * 16/ len(test_loader))\n",
    "print(\"\\nR2 model1:\",torch.mean(torch.tensor(r2)))\n",
    "\n",
    "\n",
    "print(\"\\nLoss model 2:\", loss2* 16 / len(test_loader))\n",
    "print(\"\\nR2 model2:\",torch.mean(torch.tensor(r22)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --data_path \"swerick_subsetdata_party_train.csv\" --label_names $label_names_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_binary_bert.py --model_filename \"trained_hugging_face_party_classification\" --base_model \"finetuning_hugging_whitespace-finetuned-imdb/checkpoint-343500\" --data_path \"swerick_subsetdata_party_train.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swerick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
